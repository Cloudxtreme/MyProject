!include L2Workloads.yaml
!include L3Workloads.yaml
!include MPCommonWorkloads.yaml
!include MPVerticalWorkloads.yaml
!include TestbedSpec.yaml
!include DeploymentWorkloads.yaml

MPClusterForm3MPNodeCluster:
  Product: 'NSXTransformers'
  Component: 'Clustering'
  Category: 'ManagementPlatform'
  TestName: 'MPClusterForm3MPNodeCluster'
  Version: "2"
  TCMSId: ''
  Priority: 'P0'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  TestbedSpec: *3MP_1ESX
  Summary: 'cluster with 3 mp nodes'
  Procedure: '1. Login to the NSX managers
              2. Using REST call - /cluster/nodes - join second node to cluster
              3. Using REST call - /cluster/nodes - join third node to cluster
              4. Create IP Pool from node 1
              5. Read and Update IP Pool from node 2
              6. Delete IP Pool from node 3
              4. Cleanup'
  ExpectedResult: 'After step 2 - node 2 is successfully got registered to MP cluster
                   After step 3 - node 3 is successfully got registered to MP cluster'
  Duration: '300'
  Tags: 'nsx,management,clustering,cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    <<: *IPPoolWorkloads
    Sequence:
      - ["InitialVerificationOf3MPNodeCluster"]
      - ["CreateIPPool1From_Node1"]
      - ["ReadIPPoolFrom_Node2"]
      - ["UpdateIPPoolFrom_Node2"]
    ExitSequence:
      - ["DeleteAllIPPoolsFrom_Node3"]

MPClusterFormationTwoMPNodesSingleControlPlaneNode:
  Product: 'NSXTransformers'
  Component: 'Clustering'
  Category: 'ManagementPlatform'
  TestName: 'MPClusterFormationTwoMPNodesSingleControlPlaneNode'
  Version: "2"
  TCMSId: ''
  Priority: 'P2'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'MP Cluster with 2 mp node and 1 ccp node'
  Procedure: ' 1. Form a cluster of 2 MP nodes
               2. Register controller with MP cluster
               3. Create transport zone on node 1
               4. Create logical switch from node 1
               5. Attach vnics to logical switch
               6. Verify logical switch
               7. remove node 2 from cluster
               8. Verify cluster membership
               9. Verify logical switch
               10. Cleanup'
  ExpectedResult: '1. After step 4 - Logical switch creation is successful on ESX host
                   2. After step 7 - 1 MP and 1 CCP should be the cluster membership
                   3. After step 9 - Logical switch present on ESX host'
  Duration: '300'
  Tags: 'nsx,management,clustering,3mp_3ccp_4esx_without_cluster_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  TestbedSpec: *3MP_3CCP_4ESX_WITHOUT_CLUSTER
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    <<: *CCPClusteringConfigurationWorkloads
    Sequence:
      - ["GetMPNode1Id"]
      - ["GetMPNode2Id"]
      - ["MapNSXManager1ToCluster"]
      - ["MapNSXManager2ToCluster"]
      - ["ReadClusterNode1"]
      - ["ReadClusterNode2"]
      - ["VerifyClusterStatusFromNode1"]
      - ["VerifyClusterStatusFromNode2"]
      - ["AddMPNode2ToCluster"]
      - ["RegisterController1"]
      - ['SetSecurityOnController1']
      - ['InitializeController1']
      - ["JoinController1ToCCPCluster"]
      - ["VerifyClusterMembers2MP1CCPFromNode1"]
      - ["VerifyTwoNSXManagersOnController1"]
      - ["CreateLogicalSwitch01FromNode1"]
      - ["VerifyLogicalSwitch01InfoOnController1"]
      - ["VifAttachment"]
      - ["VerifyLSReplicationModeOnHostForLS1"]
      - ["DeleteNode2FromClusterNode1"]
      - ["VerifyClusterStatusFromNode1"]
      - ["VerifyClusterMembers1MP1CCPFromNode1"]
      - ["VerifyOneNSXManagersOnController1"]
      - ["VerifyLogicalSwitch01InfoOnController1"]
      - ["VerifyLSReplicationModeOnHostForLS1"]
    ExitSequence:
      - ["VifDetachment"]
      - ["DeleteVnic1FromVM1"]
      - ["DeleteVnic1FromVM2"]
      - ["DeleteLogicalSwitchFromNode1"]
      - ["CleanupCCPNode1ForReuse"]
      - ["SetProtonServiceIdFor_Node2"]
      - ["StopProtonServiceOn_Node2"]
      - ["VerifyStopProtonServiceStatusFor_Node2"]
      - ["RemoveMP_Node2Ignore"]
      - ["CleanupMPNode2ForReuse"]
      - ["VerifyClusterMembers_1MP_From_Node1"]

    VerifyClusterMembers2MPFromNode1: *VERIFY_CLUSTER_MEMBERS_2MP

    ReadClusterNode1: *READ_CLUSTER_NODE_1

    ReadClusterNode2: *READ_CLUSTER_NODE_2

    VerifyClusterStatusFromNode1: *VERIFY_CLUSTER_STATUS_FROM_NODE_1

    VerifyClusterStatusFromNode2: *VERIFY_CLUSTER_STATUS_FROM_NODE_2

    VerifyClusterMembers2MP1CCPFromNode1: *VERIFY_CLUSTER_MEMBERS_2MP_1CCP_FROM_NODE_1

    VerifyClusterMembers1MP1CCPFromNode1: *VERIFY_CLUSTER_MEMBERS_1MP_1CCP_FROM_NODE_1

    CreateTransportNodesFromNode1: *CREATE_TRANSPORT_NODE_01--ESX

    CreateLogicalSwitch01FromNode1: *CREATE_LOGICAL_SWITCH_01

    DeleteLogicalSwitchFromNode1: *DELETE_LOGICAL_SWITCH_01

    DeleteTransportNodesFromNode1: *DELETE_TRANSPORT_NODE_01

    VifAttachment: *VIF_ATTACHMENT_01--ESX

    VifDetachment: *VIF_DETACHMENT_01--ESX

    RemoveMP_Node2Ignore:
      <<: *REMOVE_MP_NODE_2
      expectedResult: ignore

MPClusterFormationThreeMPNodesSingleControlPlaneNode:
  Product: 'NSXTransformers'
  Component: 'Clustering'
  Category: 'ManagementPlatform'
  TestName: 'MPClusterFormationThreeMPNodesSingleControlPlaneNode'
  Version: "2"
  TCMSId: ''
  Priority: 'P0'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'MP Cluster with 3 mp node and 1 ccp node'
  Procedure: ' 1. Form a cluster of 3 MP nodes
               2. Register controller with MP cluster
               3. Create transport zone on node 1
               4. Create logical switch from node 1
               5. Attach vnics to logical switch
               6. Verify logical switch
               7. remove node 2 from cluster
               8. Verify cluster membership
               9. Verify logical switch
               10. remove node 3 from cluster
               11. Verify cluster membership
               12. Verify logical switch
               13. Cleanup'
  ExpectedResult: '1. After step 4 - Logical switch creation is successful on ESX host
                   2. After step 7 - 2 MP and 1 CCP should be the cluster membership
                   3. After step 9 - Logical switch present on ESX host
                   4. After step 10 - 1 MP and 1 CCP should be the cluster membership
                   5. After step 12 - Logical switch present on ESX host'
  Duration: '300'
  Tags: 'nsx,management,clustering,3mp_3ccp_4esx_without_cluster_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  TestbedSpec: *3MP_3CCP_4ESX_WITHOUT_CLUSTER
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    <<: *CCPClusteringConfigurationWorkloads
    Sequence:
      - ["GetMPNodeIdForAllNodes"]
      - ["MapNSXManagerForAllNodes"]
      - ["ReadClusterForAllNodes"]
      - ["VerifyClusterStatusFromAllNodesForMPAndCCP"]
      - ["Add2MPNodesToCluster"]
      - ["VerifyClusterMembers_3MP"]
      - ["RegisterController1"]
      - ['SetSecurityOnController1']
      - ['InitializeController1']
      - ["JoinController1ToCCPCluster"]
      - ["VerifyClusterMembers_3MP_1CCP_From_Node1"]
      - ["VerifyAllNSXManagersOnController1"]
      - ["CreateLogicalSwitch01FromNode1"]
      - ["VerifyLogicalSwitch01InfoOnController1"]
      - ["VifAttachment1"]
      - ["VerifyLSReplicationModeOnHostForLS1"]
      - ["DeleteNode3FromClusterNode1"]
      - ["VerifyClusterStatusFromNode1"]
      - ["VerifyClusterStatusFromNode2"]
      - ["VerifyClusterMembers2MP1CCPFromNode1"]
      - ["CreateLogicalSwitch02FromNode1"]
      - ["VerifyLogicalSwitch02InfoOnController1"]
      - ["VerifyLogicalSwitch01InfoOnController1"]
      - ["VifAttachment2"]
      - ["VerifyLSReplicationModeOnHostForLS2"]
      - ["VerifyLSReplicationModeOnHostForLS1"]
      - ["DeleteNode2FromClusterNode1"]
      - ["VerifyClusterStatusFromNode1"]
      - ["VerifyClusterMembers1MP1CCPFromNode1"]
      - ["VifDetachment1"]
      - ["DeleteVnic1FromVM1"]
      - ["DeleteVnic1FromVM2"]
      - ["Wait_For_Cluster_Status_Stable_On_Node1"]
      - ["CreateLogicalSwitch03FromNode1WithSleep"]
      - ["VerifyLogicalSwitch03InfoOnController1UsingNode1"]
      - ["VerifyLogicalSwitch02InfoOnController1UsingNode1"]
      - ["VerifyLogicalSwitch01InfoOnController1"]
      - ["VifAttachment3"]
      - ["VerifyLS03ReplicationModeOnHostFromNode1"]
      - ["VerifyLSReplicationModeOnHostForLS2"]
    ExitSequence:
#      - ["VifDetachmentForVM1Ignore"] # This and below workload is failing. Refer PR 1434035
#      - ["VifDetachmentForVM2Ignore"]
      - ["VifDetachment2"]
      - ["VifDetachment3"]
      - ["DeleteVnic1FromVM1Ignore"]
      - ["DeleteVnic1FromVM2Ignore"]
      - ["DeleteVnic1FromVM3"]
      - ["DeleteVnic1FromVM4"]
      - ["DeleteVnic2FromVM1"]
      - ["DeleteVnic2FromVM2"]
      - ["DeleteLogicalSwitchFromNode1"]
      - ["CleanupCCPNode1ForReuse"]
      - ["SetProtonServiceIdFor_Node2"]
      - ["StopProtonServiceOn_Node2"]
      - ["VerifyStopProtonServiceStatusFor_Node2"]
      - ["RemoveMP_Node2Ignore"]
      - ["SetProtonServiceIdFor_Node3"]
      - ["StopProtonServiceOn_Node3"]
      - ["VerifyStopProtonServiceStatusFor_Node3"]
      - ["RemoveMP_Node3Ignore"]
      - ["CleanupMPNode2ForReuse"]
      - ["CleanupMPNode3ForReuse"]
      - ["SetProtonServiceIdFor_Node1"]
      - ["RestartProtonServiceOn_Node1"]
      - ["VerifyStartProtonServiceStatusFor_Node1"]
      - ["Wait_For_Cluster_Status_Stable_On_Node1"]
      - ["VerifyClusterMembers_1MP_From_Node1"]

    VerifyClusterMembers1MPFromNode3: *VERIFY_CLUSTER_MEMBERS_1MP_FROM_NODE_3

    StartProtonServiceOnNode2: *START_PROTON_SERVICE_ON_NODE_2

    VerifyStartProtonServiceStatusForNode2: *VERFIY_START_PROTON_SERVICE_STATUS_FOR_NODE_2

    VerifyClusterMembers1MPFromNode2: *VERIFY_CLUSTER_MEMBERS_1MP_FROM_NODE_3

    WaitForClusterStatusStableOnNode3: *WAIT_STABLE_NODE_3

    WaitForClusterStatusStableOnNode2: *WAIT_STABLE_NODE_3

    VerifyStartProtonServiceStatusForNode3: *VERFIY_START_PROTON_SERVICE_STATUS_FOR_NODE_3

    StartProtonServiceOnNode3: *START_PROTON_SERVICE_ON_NODE_3

    VerifyClusterMembers3MP: *VERIFY_CLUSTER_MEMBERS_3MP

    VerifyClusterMembers3MP3CCP: *VERIFY_CLUSTER_MEMBERS_3MP_3CCP_FROM_NODE_1

    VifAttachment3:
      Type: VM
      TestVM: 'vm.[1-2]'
      vnic:
        '[2]':
          driver: "e1000"
          portgroup: "nsxmanager.[1].logicalswitch.[3]"
          connected: 1
          startconnected: 1

    VifDetachment3:
      Type: NetAdapter
      TestAdapter: 'vm.[1-2].vnic.[2]'
      reconfigure: true
      connected: 0
      startconnected: 0

    CreateLogicalSwitch03FromNode1WithSleep:
      <<: *CREATE_LOGICAL_SWITCH_03_FROM_NODE1
      noofretries: 10
      sleepbetweenretry: 60

    VerifyClusterMembers2MP1CCPFromNode1: *VERIFY_CLUSTER_MEMBERS_2MP_1CCP_FROM_NODE_1

    VerifyClusterStatusFromNode1: *VERIFY_CLUSTER_STATUS_FROM_NODE_1

    VerifyClusterStatusFromNode2: *VERIFY_CLUSTER_STATUS_FROM_NODE_2

    VifAttachment2: *VIF_ATTACHMENT_02--ESX

    VifDetachment2: *VIF_DETACHMENT_02--ESX

    CreateLogicalSwitch02FromNode1: *CREATE_LOGICAL_SWITCH_02

    VerifyClusterMembers1MP1CCPFromNode1: *VERIFY_CLUSTER_MEMBERS_1MP_1CCP_FROM_NODE_1

    CreateTransportNodesFromNode1: *CREATE_TRANSPORT_NODE_01--ESX

    CreateLogicalSwitch01FromNode1: *CREATE_LOGICAL_SWITCH_01

    DeleteLogicalSwitchFromNode1: *DELETE_LOGICAL_SWITCH_01

    DeleteTransportNodesFromNode1: *DELETE_TRANSPORT_NODE_01

    VifAttachment1: *VIF_ATTACHMENT_01--ESX

    VifDetachment1: *VIF_DETACHMENT_01--ESX

    VifDetachmentForVM1Ignore:
      Type: NetAdapter
      TestAdapter: 'vm.[1].vnic.[1]'
      reconfigure: true
      connected: 0
      startconnected: 0
      expectedResult: ignore

    VifDetachmentForVM2Ignore:
      Type: NetAdapter
      TestAdapter: 'vm.[2].vnic.[1]'
      reconfigure: true
      connected: 0
      startconnected: 0
      expectedResult: ignore

    DeleteVnic1FromVM1Ignore:
      <<: *DELETE_VNIC_1_FROM_VM1
      expectedResult: ignore

    DeleteVnic1FromVM2Ignore:
      <<: *DELETE_VNIC_1_FROM_VM2
      expectedResult: ignore

    RemoveMP_Node2Ignore:
      <<: *REMOVE_MP_NODE_2
      expectedResult: ignore

    RemoveMP_Node3Ignore:
      <<: *REMOVE_MP_NODE_3
      expectedResult: ignore

MPClusterFormationSingleMPNodeSingleControlPlaneNode:
  Product: 'NSXTransformers'
  Component: 'Clustering'
  Category: 'ManagementPlatform'
  TestName: 'MPClusterFormationSingleMPNodeSingleControlPlaneNode'
  Version: "2"
  TCMSId: ''
  Priority: 'P2'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'MP Cluster with 2 mp node and 1 ccp node'
  Procedure: ' 1. install node nsx manager n1
               2. Register controller with MP cluster
               3. Verify cluster membership
               4. Create transport zone on node 1
               5. Create logical switch from node 1
               6. Attach vnics to logical switch
               7. Verify logical switch
               8. Cleanup'
  ExpectedResult: '1. After step 5 - Logical switch creation is successful on ESX host'
  Duration: '300'
  Tags: 'nsx,management,clustering,3mp_3ccp_4esx_without_cluster_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  TestbedSpec: *3MP_3CCP_4ESX_WITHOUT_CLUSTER
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    <<: *CCPClusteringConfigurationWorkloads
    Sequence:
      - ["GetMPNode1Id"]
      - ["MapNSXManager1ToCluster"]
      - ["ReadCluster_Node1"]
      - ["VerifyClusterStatusFrom_Node1"]
      - ["RegisterController1"]
      - ['SetSecurityOnController1']
      - ['InitializeController1']
      - ["JoinController1ToCCPCluster"]
      - ["VerifyClusterMembers_1MP_1CCP_From_Node1"]
      - ["VerifyOneNSXManagersOnController1"]
      - ["CreateLogicalSwitch01FromNode1"]
      - ["VerifyLogicalSwitch01InfoOnController1"]
      - ["VifAttachment1"]
      - ["VerifyLSReplicationModeOnHostForLS1"]
    ExitSequence:
      - ["VifDetachment1"]
      - ["DeleteVnic1FromVM1"]
      - ["DeleteVnic1FromVM2"]
      - ["DeleteLogicalSwitchFromNode1"]
      - ["CleanupCCPNode1ForReuse"]
      - ["VerifyClusterMembers_1MP_From_Node1"]

    CreateTransportNodesFromNode1: *CREATE_TRANSPORT_NODE_01--ESX

    CreateLogicalSwitch01FromNode1: *CREATE_LOGICAL_SWITCH_01

    DeleteLogicalSwitchFromNode1: *DELETE_LOGICAL_SWITCH_01

    DeleteTransportNodesFromNode1: *DELETE_TRANSPORT_NODE_01

    VifAttachment1: *VIF_ATTACHMENT_01--ESX

    VifDetachment1: *VIF_DETACHMENT_01--ESX

MPClusterReboot_inorder:
  Product: 'NSXTransformers'
  Component: 'Clustering'
  Category: 'ManagementPlatform'
  TestName: 'MPClusterReboot_inorder'
  Version: "2"
  TCMSId: ''
  Priority: 'P0'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'To verify cluster node reboot in successive makes the cluster status stable.'
  Procedure: ' 1. Form a 3 node cluster
               2. create a IP Pool-1 from node 1
               3. reboot node 1
               4. wait till node 1 comes UP and then check the cluster status from all nodes
               5. verify IP Pool-1 from all nodes
               6. create a IP Pool-2 from node 1
               7. verify IP Pool-2 from all nodes
               8. reboot node 2
               9. wait till node 2 comes UP and then check the cluster status from all nodes
               10. verify IP Pool-2 from all nodes
               11. create a IP Pool-3 from node 1
               12. verify IP Pool-3 from all nodes
               13. reboot node 3
               14. wait till node 3 comes UP and then check the cluster status from all nodes
               15. verify IP Pool-3 from all nodes
               16. create a IP Pool-4 from node 1
               17. verify IP Pool-4 from all nodes'
  ExpectedResult: 'Step-5 IP Pool-1 present on all nodes
                   Step-6 IP Pool-2  created successfully
                   Step-7 IP Pool-2  present on all nodes
                   Step-10 IP Pool-2 present on all nodes
                   Step-11 IP Pool-3  created successfully
                   Step-12 IP Pool-3  present on all nodes
                   Step-15 IP Pool-3 present on all nodes
                   Step-16 IP Pool-4  created successfully
                   Step-17 IP Pool-4  present on all nodes'
  Duration: '300'
  Tags: 'nsx,management,clustering,cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  TestbedSpec: *3MP_1ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringVerificationWorkloads
    <<: *IPPoolWorkloads
    Sequence:
      - ["InitialVerificationOf3MPNodeCluster"]
      - ["CreateIPPool1FromNode1"]
      - ["ReadIPPool1FromNode1"]
      - ["RestartNode1"]
      - ["SetProtonServiceIdForNode1"]
      - ["VerifyStartProtonServiceStatusForNode1AfterRestart"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["CreateIPPool2FromNode2"]
      - ["ReadIPPool2FromNode2"]
      - ["RestartNode2"]
      - ["VerifyStartProtonServiceStatusForNode2AfterRestart"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["ReadIPPool1FromNode1"]
      - ["ReadIPPool2FromNode2"]
      - ["CreateIPPool3FromNode3"]
      - ["ReadIPPool3FromNode3"]
      - ["RestartNode3"]
      - ["VerifyStartProtonServiceStatusForNode3AfterRestart"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["ReadIPPoolsFromAllNodes"]
    ExitSequence:
      - ["ClusterStabilityFor3MPClusterAfterTestCaseExecution"]
      - ["DeleteAllIPPoolsFromNode1"]

    CreateIPPool1FromNode1: *CREATE_IPPOOL_1

    ReadIPPool1FromNode1: *READ_IP_POOL_1_FROM_NODE_1

    RestartNode1: *RESTART_NODE_1

    SetProtonServiceIdForNode1: *SET_PROTON_SERVICE_ID_FOR_NODE_1

    CreateIPPool2FromNode2: *CREATE_IPPOOL_2_FROM_NODE2

    CreateIPPool3FromNode3: *CREATE_IPPOOL_3_FROM_NODE3

    ReadIPPool2FromNode2: *READ_IP_POOL_2_FROM_NODE_2

    ReadIPPool3FromNode3: *READ_IP_POOL_3_FROM_NODE_3

    RestartNode2: *RESTART_NODE_2

    RestartNode3: *RESTART_NODE_3

    DeleteAllIPPoolsFromNode1: *DELETE_ALL_IPPOOLS

    VerifyStartProtonServiceStatusForNode1AfterRestart:
      <<: *VERFIY_START_PROTON_SERVICE_STATUS_FOR_NODE_1
      noofretries: 10
      sleepbetweenretry: 60

    VerifyStartProtonServiceStatusForNode2AfterRestart:
      <<: *VERFIY_START_PROTON_SERVICE_STATUS_FOR_NODE_2
      noofretries: 10
      sleepbetweenretry: 60

    VerifyStartProtonServiceStatusForNode3AfterRestart:
      <<: *VERFIY_START_PROTON_SERVICE_STATUS_FOR_NODE_3
      noofretries: 10
      sleepbetweenretry: 60

MPClusterReboot_Successive:
  Product: 'NSXTransformers'
  Component: 'Clustering'
  Category: 'ManagementPlatform'
  TestName: 'MPClusterReboot_Successive'
  Version: "2"
  TCMSId: ''
  Priority: 'P0'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'To verify cluster node reboot in successive makes the cluster status stable'
  Procedure: ' 1. Form a 3 node cluster
               2. create a IP Pool-1 from node 1
               3. reboot node 1,node 2,node 3
               4. wait till all nodes comes UP and then check the cluster status from all nodes
               5. verify IP Pool-1 from all nodes
               6. create a IP Pool-2 from node 1
               7. verify IP Pool-2 from all nodes'
  ExpectedResult: 'Step-5 IP Pool-1 present on all nodes
                   Step-6 IP Pool-2  created successfully
                   Step-7 IP Pool-2  present on all nodes'
  Duration: '300'
  Tags: 'nsx,management,clustering,cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  TestbedSpec: *3MP_1ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringVerificationWorkloads
    <<: *IPPoolWorkloads
    Sequence:
      - ["InitialVerificationOf3MPNodeCluster"]
      - ["CreateIPPool1FromNode1"]
      - ["ReadIPPool1FromNode1"]
      - ["RestartNode1","RestartNode2","RestartNode3"]
      - ["SetProtonServiceIdForNode1"]
      - ["VerifyStartProtonServiceStatusForNode1AfterRestart",
        "VerifyStartProtonServiceStatusForNode2AfterRestart",
        "VerifyStartProtonServiceStatusForNode3AfterRestart"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["ReadIPPool1FromNode1"]
      - ["CreateIPPool2FromNode2"]
      - ["ReadIPPool1FromNode1"]
      - ["ReadIPPool2FromNode2"]
    ExitSequence:
      - ["ClusterStabilityFor3MPClusterAfterTestCaseExecution"]
      - ["DeleteAllIPPoolsFromNode1"]

    CreateIPPool1FromNode1: *CREATE_IPPOOL_1

    ReadIPPool1FromNode1:
      <<: *READ_IP_POOL_1_FROM_NODE_1
      sleepbetweenworkloads: 300

    RestartNode1: *RESTART_NODE_1

    RestartNode2: *RESTART_NODE_2

    RestartNode3: *RESTART_NODE_3

    SetProtonServiceIdForNode1: *SET_PROTON_SERVICE_ID_FOR_NODE_1

    CreateIPPool2FromNode2: *CREATE_IPPOOL_2_FROM_NODE2

    ReadIPPool2FromNode2: *READ_IP_POOL_2_FROM_NODE_2

    DeleteAllIPPoolsFromNode1: *DELETE_ALL_IPPOOLS

    VerifyStartProtonServiceStatusForNode1AfterRestart:
      <<: *VERFIY_START_PROTON_SERVICE_STATUS_FOR_NODE_1
      noofretries: 10
      sleepbetweenretry: 30

    VerifyStartProtonServiceStatusForNode2AfterRestart:
      <<: *VERFIY_START_PROTON_SERVICE_STATUS_FOR_NODE_2
      noofretries: 10
      sleepbetweenretry: 30

    VerifyStartProtonServiceStatusForNode3AfterRestart:
      <<: *VERFIY_START_PROTON_SERVICE_STATUS_FOR_NODE_3
      noofretries: 10
      sleepbetweenretry: 30

AddMPNodeWithOtherAPIOperationInParallel:
  Product: 'NSXTransformers'
  Component: 'Clustering'
  Category: 'ManagementPlatform'
  TestName: 'AddMPNodeWithOtherAPIOperationInParallel'
  Version: "2"
  TCMSId: ''
  Priority: 'P1'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'To verify a node can be added to a 2 node MP Cluster while creating IP Pool.'
  Procedure: ' 1) now form a cluster of 2 nodes (n1 and n2)
               2) now add n3  to node n1
               3) At the same time, create ip pool-1 from node 1 also'
  ExpectedResult: 'After Step 3 - n3 should get added to cluster
                   IP Pool-1 should also get created and verify the same from all nodes'
  Duration: '300'
  Tags: 'nsx,management,clustering,corner_cases_for_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  TestbedSpec: *4MP_2CCP_1AuthServer_2ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringVerificationWorkloads
    <<: *IPPoolWorkloads
    Sequence:
      - ["GetMPNodeIdForAllNodes"]
      - ["MapNSXManagerForAllNodes"]
      - ["ReadClusterForAllNodes"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["VerifyClusterMembersFromAllNodes"]
      - ["AddMPNode2ToCluster"]
      - ["VerifyClusterStatusFromNodes1And2"]
      - ["VerifyClusterMembers2MPFromNode1"]
      - ["CreateIPPoolFromNode11to1000","AddMPNode3ToCluster"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["VerifyClusterMembers3MP"]
      - ["VerifyIPPoolList"]
    ExitSequence:
      - ["DeleteAllIPPoolsFromNode1"]
      - ["Cleanup3NodesMPCluster"]

    CreateIPPoolFromNode11to1000:
      Type: "NSX"
      TestNSX: "nsxmanager.[1]"
      ippool:
        '[1-1000]':
          name: 'autogenerate'
          summary: "IPPool created through automation"
          subnets:
            - allocation_ranges:
              - end: 192.168.1.10
                begin: 192.168.1.2
              - end: 192.168.1.20
                begin: 192.168.1.11
              cidr: 192.168.1.0/24
              gateway_ip: 192.168.1.255
              servers:
                - 192.1.1.1

    VerifyIPPoolList:
        Type: "GroupingObject"
        TestGroupingObject: "nsxmanager.[1].ippool.[1]"
        get_pool_list:
           'result_count[?]equal_to': 1000

    VerifyClusterMembers1MPFromNode1: *VERIFY_CLUSTER_MEMBERS_1MP_FROM_NODE_1

    VerifyClusterMembers2MPFromNode1: *VERIFY_CLUSTER_MEMBERS_2MP_FROM_NODE_1

    CreateIPPool1FromNode1: *CREATE_IPPOOL_1

    VerifyClusterMembers3MP: *VERIFY_CLUSTER_MEMBERS_3MP

    ReadIPPool1FromNode1: *READ_IP_POOL_1_FROM_NODE_1

    DeleteAllIPPoolsFromNode1: *DELETE_ALL_IPPOOLS

Add2MPNodeClusterTo1MPNodeCluster:
  Product: 'NSXTransformers'
  Category: 'ManagementPlatform'
  Component: 'Clustering'
  TestName: 'Add2MPNodeClusterTo1MPNodeCluster'
  Version: "2"
  TCMSId: ''
  Priority: 'P2'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'To Verify 2 MP node Cluster cant be added to 1 MP node cluster'
  Procedure: '1) install  3 nsx manager n1,n2 and n3
              2) now form a cluster of 2 nodes (n1 and n2)
              3)  now add n2 to node n3'
  ExpectedResult: 'Add node should fail'
  Duration: '300'
  Tags: 'nsx,management,clustering,corner_cases_for_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  TestbedSpec: *4MP_2CCP_1AuthServer_2ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringVerificationWorkloads
    Sequence:
      - ["GetMPNodeIdForAllNodes"]
      - ["MapNSXManagerForAllNodes"]
      - ["ReadClusterForAllNodes"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["VerifyClusterMembersFromAllNodes"]
      - ["AddMPNode2ToCluster"]
      - ["VerifyClusterStatusFromNodes1And2"]
      - ["VerifyClusterMembers2MPFromNode1"]
      - ["AddMPNode2ToClusterNode3VerifyError"]
    ExitSequence:
      - ["Cleanup3NodesMPCluster"]

    AddMPNode2ToClusterNode3VerifyError:
      <<: *ADD_MP_NODE_2_TO_CLUSTER_MP_NODE_3
      ExpectedResult:
        status_code: BAD_REQUEST

    VerifyClusterMembers2MPFromNode1: *VERIFY_CLUSTER_MEMBERS_2MP_FROM_NODE_1

MPCluster4NodeCluster:
  Product: 'NSXTransformers'
  Category: 'ManagementPlatform'
  Component: 'Clustering'
  TestName: 'MPCluster4NodeCluster'
  Version: "2"
  TCMSId: ''
  Priority: 'P2'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'Verify 4 node cluster can be formed and the cluster is not broken.
            NOTE: We dont claim for 4 node support. This was tested to make sure the existing 3 node cluster is not
            broken after adding 4th node to it.'
  Procedure: '1. create a 4 node cluster.
              2. Then remvoe the 4th node from cluster.verify it is now 3 node cluster'
  ExpectedResult: '1. 4 node cluster is formed.
                   2. node 4 is removed successfully
                   3. verify cluster status from node 1,2 and 3'
  Duration: '300'
  Tags: 'nsx,management,clustering,corner_cases_for_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  TestbedSpec: *4MP_2CCP_1AuthServer_2ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringVerificationWorkloads
    Sequence:
      - ["GetMPNodeIdForAllNodes"]
      - ["MapNSXManagerForAllNodes"]
      - ["ReadClusterForAllNodes"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["VerifyClusterMembersFromAllNodes"]
      - ["Add2MPNodesToCluster"]
      - ["VerifyClusterMembers_3MP"]
      - ["GetMPNode4Id"]
      - ["MapNSXManager4ToCluster"]
      - ["VerifyClusterMembers_1MP_From_Node4"]
      - ["AddMPNode4ToClusterNode1"]
      - ["VerifyClusterMembers_4MP_From_Node1_Having_Nodes_1_2_3_4"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["Wait_For_Cluster_Status_Stable_On_Node4"]
      - ["SetProtonServiceIdFor_Node4"]
      - ["StopProtonServiceOn_Node4"]
      - ["VerifyStopProtonServiceStatusFor_Node4"]
      - ["RemoveMP_Node4"]
      - ["VerifyClusterMembers_3MP"]
      - ["VerifyClusterStatusFromAllNodes"]
    ExitSequence:
      - ["Cleanup3NodesMPCluster"]
      - ["CleanupMPNode4ForReuse"]

MPClusterNegativeShutdownNodeDuringRemoval_1:
  Product: 'NSXTransformers'
  Category: 'ManagementPlatform'
  Component: 'Clustering'
  TestName: 'MPClusterNegativeShutdownNodeDuringRemoval_1'
  Version: "2"
  TCMSId: ''
  Priority: 'P2'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'While removal - shutdown management plane node'
  Procedure: '1. Using REST call - /cluster/nodes - join node to form 3 node cluster
              2. While removal operation is in progress, shutdown the node that is getting removed'
  ExpectedResult: 'Node should get removed'
  Duration: '300'
  Tags: 'nsx,management,clustering,corner_cases_for_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  TestbedSpec: *4MP_2CCP_1AuthServer_2ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringVerificationWorkloads
    Sequence:
      - ["GetMPNodeIdForAllNodes"]
      - ["MapNSXManagerForAllNodes"]
      - ["ReadClusterForAllNodes"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["VerifyClusterMembersFromAllNodes"]
      - ["Add2MPNodesToCluster"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["VerifyClusterMembers3MP"]
      - ["SetProtonServiceIdFor_Node3"]
      - ["StopProtonServiceOn_Node3"]
      - ["VerifyStopProtonServiceStatusFor_Node3"]
      - ["RemoveMP_Node3","Shutdown_Node3"]
      - ["Wait_For_Cluster_Status_STABLE_On_Node1_With_Sleep"]
      - ["Wait_For_Cluster_Status_Stable_On_Node2"]
      - ["VerifyClusterMembers_2MP"]
    ExitSequence:
      - ["PowerOnMP_Node3"]
      - ["Cleanup3NodesMPCluster"]

    VerifyClusterMembers3MP: *VERIFY_CLUSTER_MEMBERS_3MP

    Wait_For_Cluster_Status_STABLE_On_Node1_With_Sleep:
      <<: *WAIT_STABLE_NODE_1
      sleepbetweenworkloads: 300

AddNode4AfterNode3Shutdown:
  Product: 'NSXTransformers'
  Category: 'ManagementPlatform'
  Component: 'Clustering'
  TestName: 'AddNode4AfterNode3Shutdown'
  Version: "2"
  TCMSId: ''
  Priority: 'P1'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'To verify user can add node 4 after node 3 is shudown in a 3 node cluster.'
  Procedure: '1) form a cluster of 3 nodes n1,n2,n3
              2) verify cluster status and member list
              3) shutdown node n3
              4) verify cluster status and member list
              5) add node n4
              6) verify cluster status and member list'
  ExpectedResult: '1.After step 6, node n4 is added to the cluster successfully'
  Duration: '300'
  Tags: 'nsx,management,clustering,corner_cases_for_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  TestbedSpec: *4MP_2CCP_1AuthServer_2ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringVerificationWorkloads
    Sequence:
      - ["GetMPNodeIdForAllNodes"]
      - ["MapNSXManagerForAllNodes"]
      - ["ReadClusterForAllNodes"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["VerifyClusterMembersFromAllNodes"]
      - ["Add2MPNodesToCluster"]
      - ["VerifyClusterMembers_3MP"]
      - ["Shutdown_Node3"]
      - ["GetMPNode4Id"]
      - ["MapNSXManager4ToCluster"]
      - ["VerifyClusterMembers_1MP_From_Node4"]
      - ["AddMPNode4ToClusterNode1"]
      - ["VerifyClusterMembers_4MP_From_Node1_Having_Nodes_1_2_3_4"]
      - ["Wait_For_Cluster_Status_Stable_On_Node1"]
      - ["Wait_For_Cluster_Status_Stable_On_Node2"]
      - ["Wait_For_Cluster_Status_Stable_On_Node4"]
    ExitSequence:
      - ["PowerOnMP_Node3"]
      - ["Cleanup3NodesMPCluster"]
      - ["CleanupMPNode4ForReuse"]

    Wait_For_Cluster_Status_Stable_On_Node3_With_Sleep:
      <<: *WAIT_STABLE_NODE_3
      sleepbetweenworkloads: 60

DuringRebootAddNode:
  Product: 'NSXTransformers'
  Category: 'ManagementPlatform'
  Component: 'Clustering'
  TestName: 'DuringRebootAddNode'
  Version: "2"
  TCMSId: ''
  Priority: 'P2'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'To Verify when in a 2 node cluster, n2 reboots try to add node 3'
  Procedure: '1. Form a cluster of 2 nodes
              2. reboot node 2 and at the same time add node 3 from node 1'
  ExpectedResult: 'node 3 should get added'
  Duration: '300'
  Tags: 'nsx,management,clustering,corner_cases_for_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  TestbedSpec: *4MP_2CCP_1AuthServer_2ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringVerificationWorkloads
    Sequence:
      - ["GetMPNodeIdForAllNodes"]
      - ["MapNSXManagerForAllNodes"]
      - ["ReadClusterForAllNodes"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["VerifyClusterMembersFromAllNodes"]
      - ["AddMPNode2ToCluster"]
      - ["VerifyClusterStatusFromNodes1And2"]
      - ["VerifyClusterMembers2MPFromNode1"]
      - ["AddMPNode3ToCluster","RestartNode2"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["VerifyClusterMembers3MP"]
    ExitSequence:
      - ["Cleanup3NodesMPCluster"]

    VerifyClusterMembers2MPFromNode1: *VERIFY_CLUSTER_MEMBERS_2MP_FROM_NODE_1

    VerifyClusterMembers3MP: *VERIFY_CLUSTER_MEMBERS_3MP

    RestartNode2: *RESTART_NODE_2

DuringRebootRemoveNode:
  Product: 'NSXTransformers'
  Category: 'ManagementPlatform'
  Component: 'Clustering'
  TestName: 'DuringRebootRemoveNode'
  Version: "2"
  TCMSId: ''
  Priority: 'P2'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'To Verify when in a 3 node cluster, when n3 reboots try to remove node 2'
  Procedure: '1. Form a cluster of 3 nodes
              2. reboot node 3 and at the same time delete node 2'
  ExpectedResult: 'node 2 should get deleted'
  Duration: '300'
  Tags: 'nsx,management,clustering,corner_cases_for_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  TestbedSpec: *4MP_2CCP_1AuthServer_2ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringVerificationWorkloads
    Sequence:
      - ["GetMPNodeIdForAllNodes"]
      - ["MapNSXManagerForAllNodes"]
      - ["ReadClusterForAllNodes"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["VerifyClusterMembersFromAllNodes"]
      - ["Add2MPNodesToCluster"]
      - ["VerifyClusterMembers3MP"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["SetProtonServiceIdForNode2"]
      - ["StopProtonServiceOnNode2"]
      - ["VerifyStopProtonServiceStatusForNode2"]
      - ["RemoveMPNode2","RestartNode3"]
      - ["SetProtonServiceIdForNode3WithSleep"]
      - ["Wait_For_Cluster_Status_Stable_On_Node1"]
      - ["Wait_For_Cluster_Status_Stable_On_Node3"]
      - ["VerifyClusterMembers2MPFromNode1HavingNodes1And3"]
    ExitSequence:
      - ["Cleanup3NodesMPCluster"]

    VerifyStopProtonServiceStatusForNode2: *VERFIY_STOP_PROTON_SERVICE_STATUS_FOR_NODE_2

    SetProtonServiceIdForNode3WithSleep: *SET_PROTON_SERVICE_ID_FOR_NODE_3_WITH_SLEEP

    SetProtonServiceIdForNode2: *SET_PROTON_SERVICE_ID_FOR_NODE_2

    StopProtonServiceOnNode2: *STOP_PROTON_SERVICE_ON_NODE_2

    RemoveMPNode2: *REMOVE_MP_NODE_2

    RestartNode3: *RESTART_NODE_3

    VerifyClusterMembers3MP: *VERIFY_CLUSTER_MEMBERS_3MP

    VerifyClusterMembers2MPFromNode1HavingNodes1And3:
      Type: "Cluster"
      TestCluster: "nsxmanager.[1].clusternode.[1]"
      get_cluster_members:
        'result_count[?]equal_to': 2
        'results[?]contain_once':
           - 'id_': 'nsxmanager.[1].clusternode.[1]'
             'manager_role':
               'node_type': 'ManagementClusterRoleConfig'
           - 'id_': 'nsxmanager.[3].clusternode.[3]'
             'manager_role':
               'node_type': 'ManagementClusterRoleConfig'

MPClusterPartialPartition_2nodes:
  Product: 'NSXTransformers'
  Category: 'ManagementPlatform'
  Component: 'Clustering'
  TestName: 'MPClusterPartialPartition_2nodes'
  Version: "2"
  TCMSId: ''
  Priority: 'P1'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'to verify write quorum with 2 node cluster using iptable rule'
  Procedure: '1. create a cluster of 2 node n1 and n2
              2. create ip pool-1 from node 1
              3. read all ip pools from all nodes
              4. add iptable rule to block communciation between node n1 and n2
              5. check cluster status from all nodes
              6. update ip pool
              7. now reomve the iptable rules
              8. restart the proton service on node whose status was "UNKNOWN"
              9. check cluster status from all nodes
              10. create/update ip pool'
  ExpectedResult: '1. After step 5 - node 1 status as "UNSTABLE" and node 2 status as "UNKNOCWN"
                   2. After step 6 - Create/Update should fail as there is no wite quorum available
                   3. After step 9 - node 1 and node 2 should have status as "STABLE"
                   4. After step 10 - Create/Update should pass'
  Duration: '300'
  Tags: 'nsx,management,clustering,corner_cases_for_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  TestbedSpec: *4MP_2CCP_1AuthServer_2ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringVerificationWorkloads
    <<: *IPPoolWorkloads
    Sequence:
      - ["GetMPNode1Id"]
      - ["GetMPNode2Id"]
      - ["ReadCluster_Node1"]
      - ["ReadCluster_Node2"]
      - ["MapNSXManager1ToCluster"]
      - ["MapNSXManager2ToCluster"]
      - ["VerifyClusterStatusFrom_Node1"]
      - ["VerifyClusterStatusFrom_Node2"]
      - ["AddMPNode2ToCluster"]
      - ["VerifyClusterStatusFrom_Node1"]
      - ["VerifyClusterStatusFrom_Node2"]
      - ["VerifyClusterMembers_2MP"]
      - ["CreateIPPool1From_Node1"]
      - ["ReadIPPool1From_Node1"]
      - ["ReadIPPoolFrom_Node2"]
      - ["BlockMPNode2Traffic_On_Node1"]
      - ["Wait_For_Cluster_Status_Unstable_On_Node1_With_Sleep"]
      - ["Wait_For_Cluster_Status_Unknown_On_Node2_With_Timeout"]
      - ["UpdateIPPoolFrom_Node1_Verify_Fail"]
      - ["UpdateIPPoolFrom_Node2_Verify_Fail"]
      - ["UnBlockMPNode2Traffic_On_Node1"]
      - ["SetProtonServiceIdFor_Node2"]
      - ["RestartProtonServiceOn_Node2"]
      - ["VerifyStartProtonServiceStatusFor_Node2_With_Sleep"]
      - ["Wait_For_Cluster_Status_Stable_On_Node1"]
      - ["Wait_For_Cluster_Status_Stable_On_Node2"]
      - ["ReadIPPool1From_Node1"]
      - ["ReadIPPoolFrom_Node2"]
      - ["UpdateIPPoolFrom_Node1"]
      - ["UpdateIPPoolFrom_Node2"]
      - ["ReadIPPoolFrom_Node1_After_Update"]
      - ["ReadIPPoolFrom_Node2_After_Update"]
      - ["CreateIPPool2From_Node1"]
    ExitSequence:
      - ["UnBlockMPNode2Traffic_On_Node1"]
      - ["SetProtonServiceIdFor_Node2"]
      - ["RestartProtonServiceOn_Node2"]
      - ["VerifyStartProtonServiceStatusFor_Node2_With_Sleep"]
      - ["Wait_For_Cluster_Status_Stable_On_Node1"]
      - ["Wait_For_Cluster_Status_Stable_On_Node2"]
      - ["DeleteAllIPPoolsFrom_Node1"]
      - ["Cleanup2NodesMPCluster"]

    UpdateIPPoolFrom_Node1_Verify_Fail:
      <<: *UPDATE_IP_POOL_FROM_NODE_1
      ExpectedResult:
        status_code: SERVICE_UNAVAILABLE

    UpdateIPPoolFrom_Node2_Verify_Fail:
      <<: *UPDATE_IP_POOL_FROM_NODE_2
      ExpectedResult:
        status_code: SERVICE_UNAVAILABLE

    VerifyStartProtonServiceStatusFor_Node2_With_Sleep:
      <<: *VERFIY_START_PROTON_SERVICE_STATUS_FOR_NODE_2
      sleepbetweenworkloads: 90

    Wait_For_Cluster_Status_Unknown_On_Node2_With_Timeout:
      <<: *WAIT_UNKNOWN_NODE_2
      sleepbetweenworkloads: 120

    Wait_For_Cluster_Status_Unstable_On_Node1_With_Sleep:
      <<: *WAIT_UNSTABLE_NODE_1
      sleepbetweenworkloads: 120

MPClusterRemoveThenAddNewNodesInCluster:
  Product: 'NSXTransformers'
  Category: 'ManagementPlatform'
  Component: 'Clustering'
  TestName: 'MPClusterRemoveThenAddNewNodesInCluster'
  Version: "2"
  TCMSId: ''
  Priority: 'P1'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'To verify the add/remove node of a cluster and in between create/update/delete ip pool objects'
  Procedure: '1. Login to the NSX managers
              2. Using REST call - /cluster/nodes - join second node to cluster
              3. Verify cluster status and membership
              4. Create 10 ip pools on node 1 and verify from other nodes
              5. Now delete node 2
              6. Using REST call - /cluster/nodes - join another node to cluster
              7. Verify that all the nodes should have 10 ip pools
              9. create another 10 ip pools from newly added node'
  ExpectedResult: 'Verify that all the nodes should have 20 ip pools'
  Duration: '300'
  Tags: 'nsx,management,clustering,corner_cases_for_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  TestbedSpec: *4MP_2CCP_1AuthServer_2ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringVerificationWorkloads
    <<: *IPPoolWorkloads
    Sequence:
      - ["GetMPNodeIdForAllNodes"]
      - ["MapNSXManagerForAllNodes"]
      - ["ReadClusterForAllNodes"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["AddMPNode2ToCluster"]
      - ["VerifyClusterStatusFromNodes1And2"]
      - ["VerifyClusterMembers2MP"]
      - ["CreateIPPoolFromNode11to10"]
      - ["ReadIPPoolFromNode1For1To10"]
      - ["ReadIPPoolFromNode2For1To10"]
      - ["DeleteNode2FromClusterNode1"]
      - ["Wait_For_Cluster_Status_Stable_On_Node1"]
      - ["VerifyClusterMembers1MPFromNode1"]
      - ["ReadIPPoolFromNode1For1To10"]
      - ["AddMPNode3ToCluster"]
      - ["Wait_For_Cluster_Status_Stable_On_Node1"]
      - ["Wait_For_Cluster_Status_Stable_On_Node3"]
      - ["VerifyClusterMembers2MPFromNode3"]
      - ["ReadIPPoolFromNode1For1To10"]
      - ["ReadIPPoolFromNode3For1To10"]
      - ["CreateIPPoolFromNode311to20"]
      - ["ReadIPPoolFromNode1For1To10"]
      - ["ReadIPPoolFromNode3For1To10"]
      - ["ReadIPPoolFromNode1For11To20"]
      - ["ReadIPPoolFromNode3For11To20"]
    ExitSequence:
      - ["DeleteAllIPPoolsFromNode1"]
      - ["Cleanup3NodesMPCluster"]

    VerifyClusterMembers2MP: *VERIFY_CLUSTER_MEMBERS_2MP_FROM_NODE_1

    VerifyClusterMembers1MPFromNode1: *VERIFY_CLUSTER_MEMBERS_1MP_FROM_NODE_1

    VerifyClusterMembers2MPFromNode3: *VERIFY_CLUSTER_MEMBERS_2MP_FROM_NODE_3

    CreateIPPoolFromNode11to10: *CREATE_1_TO_10_IPPOOL_FROM_NODE1

    ReadIPPoolFromNode1For1To10: *READ_IP_POOL_FROM_NODE_1_FOR_1_TO_10

    ReadIPPoolFromNode2For1To10: *READ_IP_POOL_FROM_NODE_2_FOR_1_TO_10

    ReadIPPoolFromNode3For1To10: *READ_IP_POOL_FROM_NODE_3_FOR_1_TO_10

    CreateIPPoolFromNode311to20: *CREATE_11_TO_20_IPPOOL_FROM_NODE3

    ReadIPPoolFromNode1For11To20: *READ_IP_POOL_FROM_NODE_1_FOR_11_TO_20

    ReadIPPoolFromNode3For11To20: *READ_IP_POOL_FROM_NODE_3_FOR_11_TO_20

    DeleteAllIPPoolsFromNode1: *DELETE_ALL_IPPOOLS

    StartProtonServiceOnNode2: *START_PROTON_SERVICE_ON_NODE_2

    VerifyStartProtonServiceStatusForNode2: *VERFIY_START_PROTON_SERVICE_STATUS_FOR_NODE_2

    VerifyClusterMembers1MPFromNode2: *VERIFY_CLUSTER_MEMBERS_1MP_FROM_NODE_2

MPClusterRemoveSelfFromCluster:
  Product: 'NSXTransformers'
  Category: 'ManagementPlatform'
  Component: 'Clustering'
  TestName: 'MPClusterRemoveSelfFromCluster'
  Version: "2"
  TCMSId: ''
  Priority: 'P1'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'To verify the self node of a cluster cant be deleted by itself'
  Procedure: '1. Login to the NSX managers
              2. Verify cluster status and membership
              3. Now delete self node-1 from the cluster(node-1 itself)
              4. It should not allow to delete as the proton service is still running
              5. now stop the proton service on node-1 and then delete it.
              4. Verify it should not allow to do so'
  ExpectedResult: ''
  Duration: '300'
  Tags: 'nsx,management,clustering,corner_cases_for_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  TestbedSpec: *4MP_2CCP_1AuthServer_2ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringVerificationWorkloads
    <<: *IPPoolWorkloads
    Sequence:
      - ["GetMPNode4Id"]
      - ["ReadCluster_Node4"]
      - ["MapNSXManager4ToCluster"]
      - ["Wait_For_Cluster_Status_Stable_On_Node4"]
      - ["VerifyClusterMembers_1MP_From_Node4"]
      - ["RemoveMPNode4FromSelfNode4_Before_Proton_Service_Down"]
      - ["SetProtonServiceIdFor_Node4"]
      - ["StopProtonServiceOn_Node4"]
      - ["VerifyStopProtonServiceStatusFor_Node4"]
      - ["RemoveMPNode4FromSelfNode4_After_Proton_Service_Down"]
    ExitSequence:
      - ["StartProtonServiceOn_Node4"]
      - ["VerifyStartProtonServiceStatusFor_Node4"]
      - ["Wait_For_Cluster_Status_Stable_On_Node4"]

    RemoveMPNode4FromSelfNode4_Before_Proton_Service_Down:
      Type : "NSX"
      TestNSX : "nsxmanager.[4]"
      ExpectedResult:
        status_code: BAD_REQUEST
      deleteclusternode:  "nsxmanager.[4].clusternode.[4]"

    RemoveMPNode4FromSelfNode4_After_Proton_Service_Down:
      Type : "NSX"
      TestNSX : "nsxmanager.[4]"
      ExpectedResult:
        status_code: INTERNAL_SERVER_ERROR
      deleteclusternode:  "nsxmanager.[4].clusternode.[4]"

MPClusterNodesInDifferentTimeZones:
  Product: 'NSXTransformers'
  Category: 'ManagementPlatform'
  Component: 'Clustering'
  TestName: 'MPClusterNodesInDifferentTimeZones'
  Version: "2"
  TCMSId: ''
  Priority: 'P0'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'To verify the node of a cluster in different time zones is working properly'
  Procedure:  ' 1. Install NSX Manager node n1 and set timezone t1
                2. Install NSX Manager node n2 and and set timezone t2
                3. Now add node n2 to node n1
                4. Verify node n2 should show congifurations like timezone
                   as t2 and node n1 as t1
                5. Install NSX Manager node n3 and and set timezone t3
                6. Now add node n3 to node n1'
  ExpectedResult: 'Verify node n3 should show congifurations like
                   timezone as t3,n1 as t1 and n2 as t2'
  Duration: '300'
  Tags: 'nsx,management,clustering,corner_cases_for_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  TestbedSpec: *4MP_2CCP_1AuthServer_2ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringVerificationWorkloads
    Sequence:
      - ["GetMPNodeIdForAllNodes"]
      - ["MapNSXManagerForAllNodes"]
      - ["ReadClusterForAllNodes"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["VerifyClusterMembersFromAllNodes"]
      - ["UpdateNSXManager1TimezoneToEST"]
      - ["VerifyChangedNSXManager1TimezoneToEST"]
      - ["UpdateNSXManager2TimezoneToHST"]
      - ["VerifyChangedNSXManager2TimezoneToHST"]
      - ["UpdateNSXManager3TimezoneToMST"]
      - ["VerifyChangedNSXManager3TimezoneToMST"]
      - ["AddMPNode2ToCluster"]
      - ["VerifyClusterStatusFromNodes1And2"]
      - ["VerifyClusterMembers2MPFromNode1"]
      - ["VerifyChangedNSXManager1TimezoneToEST"]
      - ["VerifyChangedNSXManager2TimezoneToHST"]
      - ["AddMPNode3ToCluster"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["VerifyClusterMembers3MPFromNode1"]
      - ["VerifyChangedNSXManager1TimezoneToEST"]
      - ["VerifyChangedNSXManager2TimezoneToHST"]
      - ["VerifyChangedNSXManager3TimezoneToMST"]
    ExitSequence:
      - ["Cleanup3NodesMPCluster"]

    VerifyClusterMembers2MPFromNode1: *VERIFY_CLUSTER_MEMBERS_2MP_FROM_NODE_1

    VerifyClusterMembers3MPFromNode1: *VERIFY_CLUSTER_MEMBERS_3MP_FROM_NODE_1

    UpdateNSXManager1TimezoneToEST:
      Type: NSX
      TestNSX: "nsxmanager.[1]"
      reconfigure_nsx:
        timezone: 'EST'

    VerifyChangedNSXManager1TimezoneToEST:
      Type: NSX
      TestNSX: "nsxmanager.[1]"
      get_setting:
        'timezone[?]equal_to': 'EST'

    UpdateNSXManager2TimezoneToHST:
      Type: NSX
      TestNSX: "nsxmanager.[2]"
      reconfigure_nsx:
        timezone: 'HST'

    VerifyChangedNSXManager2TimezoneToHST:
      Type: NSX
      TestNSX: "nsxmanager.[2]"
      get_setting:
        'timezone[?]equal_to': 'HST'

    UpdateNSXManager3TimezoneToMST:
      Type: NSX
      TestNSX: "nsxmanager.[3]"
      reconfigure_nsx:
        timezone: 'MST'

    VerifyChangedNSXManager3TimezoneToMST:
      Type: NSX
      TestNSX: "nsxmanager.[3]"
      get_setting:
        'timezone[?]equal_to': 'MST'

MPClusterConcurrentCreateOn3ClusterNode:
  Product: 'NSXTransformers'
  Component: 'Clustering'
  Category: 'ManagementPlatform'
  TestName: 'MPClusterConcurrentCreateOn3ClusterNode'
  Version: "2"
  TCMSId: ''
  Priority: 'P0'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  TestbedSpec: *3MP_1ESX
  Summary: 'cluster with 3 mp nodes and concurrently create 3 different ip pools from node 1,2 and node 3'
  Procedure: '1. Form cluster of 3 MP nodes
              2. Create 3 different ip pools on each nodes 1,2 and 3 concurrently
              3. Verify 3 ip pools on each nodes 1,2 and 3'
  ExpectedResult: 'Verify all configurations are availabe on other nodes by doing get operation'
  Duration: '300'
  Tags: 'nsx,management,clustering,cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    <<: *IPPoolWorkloads
    Sequence:
      - ["InitialVerificationOf3MPNodeCluster"]
      - ["CreateIPPool1FromNode1","CreateIPPool2FromNode2","CreateIPPool3FromNode3"]
      - ["ReadIPPoolsFromAllNodes"]
    ExitSequence:
      - ["ClusterStabilityFor3MPClusterAfterTestCaseExecution"]
      - ["DeleteAllIPPoolsFrom_Node1"]

    CreateIPPool1FromNode1: *CREATE_IPPOOL_1

    CreateIPPool2FromNode2: *CREATE_IPPOOL_2_FROM_NODE2

    CreateIPPool3FromNode3: *CREATE_IPPOOL_3_FROM_NODE3

MPCluster3Nodes_BringUpOrder_1:
  Product: 'NSXTransformers'
  Category: 'ManagementPlatform'
  Component: 'Clustering'
  TestName: 'MPCluster3Nodes_BringUpOrder_1'
  Version: "2"
  TCMSId: ''
  Priority: 'P0'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  TestbedSpec: *3MP_1ESX
  Summary: 'Join nodes to form a cluster of three nodes, check for bring up sequence'
  Procedure: '1. Login to the NSX managers
              2. Form cluster of three nodes
              3. Create IPPool 1
              4. shutdown node 1
              5. Configure ippool 2
              6. shutdown node 2
              7. shutdown node 3
              8. bring up node 2
              9. bring up node 1
              10. Configure ipppool 3 - configuration will fail
              11. bring up node 3'
  ExpectedResult: 'Nodes 2 and 1 need to wait till node 3 is up'
  Duration: '300'
  Tags: 'nsx,management,clustering,cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Draft'
  PartnerFacing: 'N'
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    <<: *IPPoolWorkloads
    Sequence:
      - ["InitialVerificationOf3MPNodeCluster"]
      - ["CreateIPPool1FromNode1"]
      - ["ReadIPPool1FromNode1"]
      - ["Shutdown_Node1"]
      - ["CreateIPPool2FromNode2"]
      - ["ReadIPPool2FromNode2"]
      - ["ShutdownNode2WithSleep"]
      - ["CreateIPPool3Node3VerifyErrorWithSleep"]
      - ["ShutdownNode3WithSleep"]
      - ["PowerOnMP_Node2"]
      - ["PowerOnMP_Node1"]
      - ["CreateIPPool3FromNode1VerifyErrorWithSleep"]
      - ["PowerOnMP_Node3"]
      - ["Wait_For_Cluster_Status_Stable_On_Node1"]
      - ["Wait_For_Cluster_Status_Stable_On_Node2"]
      - ["Wait_For_Cluster_Status_Stable_On_Node3_With_Sleep"]
      - ["ReadIPPool1FromNode1"]
      - ["ReadIPPool2FromNode2"]
    ExitSequence:
      - ["ClusterStabilityFor3MPClusterAfterTestCaseExecution"]
      - ["DeleteAllIPPoolsFrom_Node2"]

    CreateIPPool1FromNode1: *CREATE_IPPOOL_1

    ReadIPPool1FromNode1: *READ_IP_POOL_1_FROM_NODE_1

    CreateIPPool2FromNode2: *CREATE_IPPOOL_2_FROM_NODE2

    ReadIPPool2FromNode2: *READ_IP_POOL_2_FROM_NODE_2

    CreateIPPool3Node3VerifyErrorWithSleep:
      <<: *CREATE_IPPOOL_3_FROM_NODE3
      ExpectedResult:
        status_code: SERVICE_UNAVAILABLE
      sleepbetweenworkloads: 180

    CreateIPPool3FromNode1VerifyErrorWithSleep:
      <<: *CREATE_IPPOOL_3
      ExpectedResult:
        status_code: SERVICE_UNAVAILABLE
      sleepbetweenworkloads: 180

    Wait_For_Cluster_Status_Stable_On_Node3_With_Sleep:
      <<: *WAIT_STABLE_NODE_3
      sleepbetweenworkloads: 300

    ShutdownNode2WithSleep:
      <<: *SHUTDOWN_NODE_2
      sleepbetweenworkloads: 180

    ShutdownNode3WithSleep:
      <<: *SHUTDOWN_NODE_3
      sleepbetweenworkloads: 180

MPClusterFormationThreeMPNodesThreeControlPlaneNodes:
  Product: 'NSXTransformers'
  Component: 'Clustering'
  Category: 'ManagementPlatform'
  TestName: 'MPClusterFormationThreeMPNodesThreeControlPlaneNodes'
  Version: "2"
  TCMSId: ''
  Priority: 'P0'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'MP Cluster with 3 mp node and 3 ccp node'
  Procedure: ' 1. Form a cluster of 3 MP nodes
               2. Register controller with MP cluster
               3. Create transport zone on node 1
               4. Create logical switch from node 1
               5. Attach vnics to logical switch
               6. Verify logical switch
               7. remove node 2 from cluster
               8. Verify cluster membership
               9. Verify logical switch
               10. remove node 3 from cluster
               11. Verify cluster membership
               12. Verify logical switch
               13. Cleanup'
  ExpectedResult: '1. After step 4 - Logical switch creation is successful on ESX host
                   2. After step 7 - 2 MP and 3 CCP should be the cluster membership
                   3. After step 9 - Logical switch present on ESX host
                   4. After step 10 - 1 MP and 3 CCP should be the cluster membership
                   5. After step 12 - Logical switch present on ESX host'
  Duration: '300'
  Tags: 'nsx,management,clustering,3mp_3ccp_4esx_without_cluster_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'N'
  TestbedSpec: *3MP_3CCP_4ESX_WITHOUT_CLUSTER
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    <<: *CCPClusteringConfigurationWorkloads
    Sequence:
      - ["GetMPNodeIdForAllNodes"]
      - ["MapNSXManagerForAllNodes"]
      - ["ReadClusterForAllNodes"]
      - ["VerifyClusterStatusFromAllNodesForMPAndCCP"]
      - ["Add2MPNodesToCluster"]
      - ["VerifyClusterMembers_3MP"]
      - ["RegisterController1"]
      - ["RegisterController2"]
      - ["RegisterController3"]
      - ["SetSecurityOnController1"]
      - ["InitializeController1"]
      - ["JoinController1ToCCPCluster"]
      - ["SetSecurityOnController2"]
      - ["JoinController2ToCCPCluster"]
      - ["SetSecurityOnController3"]
      - ["JoinController3ToCCPCluster"]
      - ["ActivateController2"]
      - ["ActivateController3"]
      - ["VerifyClusterMembers_3MP_3CCP"]
      - ["VerifyAllNSXManagersOnController1"]
      - ["CreateLogicalSwitch01FromNode1"]
      - ["VerifyLogicalSwitch01InfoOnController1"]
      - ["VifAttachment1"]
      - ["VerifyLSReplicationModeOnHostForLS1"]
      - ["DeleteNode3FromClusterNode1"]
      - ["VerifyClusterStatusFromNode1"]
      - ["VerifyClusterStatusFromNode2"]
      - ["VerifyClusterMembers2MP3CCPFromNode1"]
      - ["CreateLogicalSwitch02FromNode1"]
      - ["VerifyLogicalSwitch02InfoOnController1"]
      - ["VerifyLogicalSwitch01InfoOnController1"]
      - ["VifAttachment2"]
      - ["VerifyLSReplicationModeOnHostForLS2"]
      - ["VerifyLSReplicationModeOnHostForLS1"]
      - ["DeleteNode2FromClusterNode1"]
      - ["VerifyClusterStatusFromNode1"]
      - ["VerifyClusterMembers1MP3CCPFromNode1"]
      - ["VifDetachment1"]
      - ["DeleteVnic1FromVM1"]
      - ["DeleteVnic1FromVM2"]
      - ["CreateLogicalSwitch03FromNode1"]
      - ["VerifyLogicalSwitch03InfoOnController1UsingNode1"]
      - ["VerifyLogicalSwitch02InfoOnController1UsingNode1"]
      - ["VerifyLogicalSwitch01InfoOnController1"]
      - ["VifAttachment3"]
      - ["VerifyLS03ReplicationModeOnHostFromNode1"]
      - ["VerifyLSReplicationModeOnHostForLS2"]
    ExitSequence:
#      - ["VifDetachmentForVM1Ignore"] # This and below workload is failing. Refer PR 1434035
#      - ["VifDetachmentForVM2Ignore"]
      - ["VifDetachment2"]
      - ["VifDetachment3"]
      - ["DeleteVnic1FromVM1Ignore"]
      - ["DeleteVnic1FromVM2Ignore"]
      - ["DeleteVnic1FromVM3"]
      - ["DeleteVnic1FromVM4"]
      - ["DeleteVnic2FromVM1"]
      - ["DeleteVnic2FromVM2"]
      - ["DeleteLogicalSwitchFromNode1"]
      - ["Cleanup3NodesCCPCluster"]
      - ["SetProtonServiceIdFor_Node2"]
      - ["StopProtonServiceOn_Node2"]
      - ["VerifyStopProtonServiceStatusFor_Node2"]
      - ["RemoveMP_Node2Ignore"]
      - ["SetProtonServiceIdFor_Node3"]
      - ["StopProtonServiceOn_Node3"]
      - ["VerifyStopProtonServiceStatusFor_Node3"]
      - ["RemoveMP_Node3Ignore"]
      - ["CleanupMPNode2ForReuse"]
      - ["CleanupMPNode3ForReuse"]
      - ["VerifyClusterMembers_1MP_From_Node1"]

    VerifyClusterMembers1MPFromNode3: *VERIFY_CLUSTER_MEMBERS_1MP_FROM_NODE_3

    StartProtonServiceOnNode2: *START_PROTON_SERVICE_ON_NODE_2

    VerifyStartProtonServiceStatusForNode2: *VERFIY_START_PROTON_SERVICE_STATUS_FOR_NODE_2

    VerifyClusterMembers1MPFromNode2: *VERIFY_CLUSTER_MEMBERS_1MP_FROM_NODE_3

    WaitForClusterStatusStableOnNode3: *WAIT_STABLE_NODE_3

    WaitForClusterStatusStableOnNode2: *WAIT_STABLE_NODE_3

    VerifyStartProtonServiceStatusForNode3: *VERFIY_START_PROTON_SERVICE_STATUS_FOR_NODE_3

    StartProtonServiceOnNode3: *START_PROTON_SERVICE_ON_NODE_3

    VerifyClusterMembers3MP: *VERIFY_CLUSTER_MEMBERS_3MP

    VerifyClusterMembers3MP3CCP: *VERIFY_CLUSTER_MEMBERS_3MP_3CCP_FROM_NODE_1

    VifAttachment3:
      Type: VM
      TestVM: 'vm.[1-2]'
      vnic:
        '[2]':
          driver: "e1000"
          portgroup: "nsxmanager.[1].logicalswitch.[3]"
          connected: 1
          startconnected: 1

    VifDetachment3:
      Type: NetAdapter
      TestAdapter: 'vm.[1-2].vnic.[2]'
      reconfigure: true
      connected: 0
      startconnected: 0

    CreateLogicalSwitch03FromNode1: *CREATE_LOGICAL_SWITCH_03_FROM_NODE1

    VerifyClusterMembers1MP3CCPFromNode1: *VERIFY_CLUSTER_MEMBERS_1MP_3CCP_FROM_NODE_1

    VerifyClusterStatusFromNode1: *VERIFY_CLUSTER_STATUS_FROM_NODE_1

    VerifyClusterStatusFromNode2: *VERIFY_CLUSTER_STATUS_FROM_NODE_2

    VifAttachment2: *VIF_ATTACHMENT_02--ESX

    VifDetachment2: *VIF_DETACHMENT_02--ESX

    CreateLogicalSwitch02FromNode1: *CREATE_LOGICAL_SWITCH_02

    VerifyClusterMembers2MP3CCPFromNode1: *VERIFY_CLUSTER_MEMBERS_2MP_3CCP_FROM_NODE_1

    CreateTransportNodesFromNode1: *CREATE_TRANSPORT_NODE_01--ESX

    CreateLogicalSwitch01FromNode1: *CREATE_LOGICAL_SWITCH_01

    DeleteLogicalSwitchFromNode1: *DELETE_LOGICAL_SWITCH_01

    DeleteTransportNodesFromNode1: *DELETE_TRANSPORT_NODE_01

    VifAttachment1: *VIF_ATTACHMENT_01--ESX

    VifDetachment1: *VIF_DETACHMENT_01--ESX

    VifDetachmentForVM1Ignore:
      Type: NetAdapter
      TestAdapter: 'vm.[1].vnic.[1]'
      reconfigure: true
      connected: 0
      startconnected: 0
      expectedResult: ignore

    VifDetachmentForVM2Ignore:
      Type: NetAdapter
      TestAdapter: 'vm.[2].vnic.[1]'
      reconfigure: true
      connected: 0
      startconnected: 0
      expectedResult: ignore

    DeleteVnic1FromVM1Ignore:
      <<: *DELETE_VNIC_1_FROM_VM1
      expectedResult: ignore

    DeleteVnic1FromVM2Ignore:
      <<: *DELETE_VNIC_1_FROM_VM2
      expectedResult: ignore

    RemoveMP_Node2Ignore:
      <<: *REMOVE_MP_NODE_2
      expectedResult: ignore

    RemoveMP_Node3Ignore:
      <<: *REMOVE_MP_NODE_3
      expectedResult: ignore

MPClusterFormationThreeMPNodesTwoControlPlaneNodes:
  Product: 'NSXTransformers'
  Component: 'Clustering'
  Category: 'ManagementPlatform'
  TestName: 'MPClusterFormationThreeMPNodesTwoControlPlaneNodes'
  Version: "2"
  TCMSId: ''
  Priority: 'P1'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'Form MP cluster of 3 nodes and CCP cluster of 2 nodes and execute L2 apis'
  Procedure: ' 1. Form a cluster of 3 MP nodes
               2. Register controller with MP cluster
               3. Create transport zone on node 1
               4. Create logical switch from node 1
               5. Attach vnics to logical switch
               6. Verify logical switch
               7. remove node 2 from cluster
               8. Verify cluster membership
               9. Verify logical switch
               10. remove node 3 from cluster
               11. Verify cluster membership
               12. Verify logical switch
               13. Cleanup'
  ExpectedResult: '1. After step 4 - Logical switch creation is successful on ESX host
                   2. After step 7 - 2 MP and 2 CCP should be the cluster membership
                   3. After step 9 - Logical switch present on ESX host
                   4. After step 10 - 1 MP and 2 CCP should be the cluster membership
                   5. After step 12 - Logical switch present on ESX host'
  Duration: '300'
  Tags: 'nsx,management,clustering,3mp_3ccp_4esx_without_cluster_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  TestbedSpec: *3MP_3CCP_4ESX_WITHOUT_CLUSTER
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    <<: *CCPClusteringConfigurationWorkloads
    Sequence:
      - ["GetMPNodeIdForAllNodes"]
      - ["MapNSXManagerForAllNodes"]
      - ["ReadClusterForAllNodes"]
      - ["VerifyClusterStatusFromAllNodesForMPAndCCP"]
      - ["Add2MPNodesToCluster"]
      - ["VerifyClusterMembers_3MP"]
      - ["RegisterController1"]
      - ["RegisterController2"]
      - ["SetSecurityOnController1"]
      - ["InitializeController1"]
      - ["JoinController1ToCCPCluster"]
      - ["SetSecurityOnController2"]
      - ["JoinController2ToCCPCluster"]
      - ["ActivateController2"]
      - ["VerifyClusterMembers_3MP_2CCP_From_Node1"]
      - ["VerifyAllNSXManagersOnController1"]
      - ["CreateLogicalSwitch01FromNode1"]
      - ["VerifyLogicalSwitch01InfoOnController1"]
      - ["VifAttachment1"]
      - ["VerifyLSReplicationModeOnHostForLS1"]
      - ["DeleteNode3FromClusterNode1"]
      - ["VerifyClusterStatusFromNode1"]
      - ["VerifyClusterStatusFromNode2"]
      - ["VerifyClusterMembers2MP2CCPFromNode1"]
      - ["CreateLogicalSwitch02FromNode1"]
      - ["VerifyLogicalSwitch02InfoOnController1"]
      - ["VerifyLogicalSwitch01InfoOnController1"]
      - ["VifAttachment2"]
      - ["VerifyLSReplicationModeOnHostForLS2"]
      - ["VerifyLSReplicationModeOnHostForLS1"]
      - ["DeleteNode2FromClusterNode1"]
      - ["VerifyClusterStatusFromNode1"]
      - ["VerifyClusterMembers1MP2CCPFromNode1"]
      - ["VifDetachment1"]
      - ["DeleteVnic1FromVM1"]
      - ["DeleteVnic1FromVM2"]
      - ["CreateLogicalSwitch03FromNode1"]
      - ["VerifyLogicalSwitch03InfoOnController1UsingNode1"]
      - ["VerifyLogicalSwitch02InfoOnController1UsingNode1"]
      - ["VerifyLogicalSwitch01InfoOnController1"]
      - ["VifAttachment3"]
      - ["VerifyLS03ReplicationModeOnHostFromNode1"]
      - ["VerifyLSReplicationModeOnHostForLS2"]
    ExitSequence:
#      - ["VifDetachmentForVM1Ignore"] # This and below workload is failing. Refer PR 1434035
#      - ["VifDetachmentForVM2Ignore"]
      - ["VifDetachment2"]
      - ["VifDetachment3"]
      - ["DeleteVnic1FromVM1Ignore"]
      - ["DeleteVnic1FromVM2Ignore"]
      - ["DeleteVnic1FromVM3"]
      - ["DeleteVnic1FromVM4"]
      - ["DeleteVnic2FromVM1"]
      - ["DeleteVnic2FromVM2"]
      - ["DeleteLogicalSwitchFromNode1"]
      - ["Cleanup2NodesCCPCluster"]
      - ["SetProtonServiceIdFor_Node2"]
      - ["StopProtonServiceOn_Node2"]
      - ["VerifyStopProtonServiceStatusFor_Node2"]
      - ["RemoveMP_Node2Ignore"]
      - ["SetProtonServiceIdFor_Node3"]
      - ["StopProtonServiceOn_Node3"]
      - ["VerifyStopProtonServiceStatusFor_Node3"]
      - ["RemoveMP_Node3Ignore"]
      - ["CleanupMPNode2ForReuse"]
      - ["CleanupMPNode3ForReuse"]
      - ["VerifyClusterMembers_1MP_From_Node1"]

    VerifyClusterMembers1MPFromNode3: *VERIFY_CLUSTER_MEMBERS_1MP_FROM_NODE_3

    StartProtonServiceOnNode2: *START_PROTON_SERVICE_ON_NODE_2

    VerifyStartProtonServiceStatusForNode2: *VERFIY_START_PROTON_SERVICE_STATUS_FOR_NODE_2

    VerifyClusterMembers1MPFromNode2: *VERIFY_CLUSTER_MEMBERS_1MP_FROM_NODE_3

    WaitForClusterStatusStableOnNode3: *WAIT_STABLE_NODE_3

    WaitForClusterStatusStableOnNode2: *WAIT_STABLE_NODE_3

    VerifyStartProtonServiceStatusForNode3: *VERFIY_START_PROTON_SERVICE_STATUS_FOR_NODE_3

    StartProtonServiceOnNode3: *START_PROTON_SERVICE_ON_NODE_3

    VerifyClusterMembers3MP: *VERIFY_CLUSTER_MEMBERS_3MP

    VerifyClusterMembers3MP3CCP: *VERIFY_CLUSTER_MEMBERS_3MP_3CCP_FROM_NODE_1

    VifAttachment3:
      Type: VM
      TestVM: 'vm.[1-2]'
      vnic:
        '[2]':
          driver: "e1000"
          portgroup: "nsxmanager.[1].logicalswitch.[3]"
          connected: 1
          startconnected: 1

    VifDetachment3:
      Type: NetAdapter
      TestAdapter: 'vm.[1-2].vnic.[2]'
      reconfigure: true
      connected: 0
      startconnected: 0

    CreateLogicalSwitch03FromNode1: *CREATE_LOGICAL_SWITCH_03_FROM_NODE1

    VerifyClusterMembers1MP2CCPFromNode1: *VERIFY_CLUSTER_MEMBERS_1MP_2CCP_FROM_NODE_1

    VerifyClusterStatusFromNode1: *VERIFY_CLUSTER_STATUS_FROM_NODE_1

    VerifyClusterStatusFromNode2: *VERIFY_CLUSTER_STATUS_FROM_NODE_2

    VifAttachment2: *VIF_ATTACHMENT_02--ESX

    VifDetachment2: *VIF_DETACHMENT_02--ESX

    CreateLogicalSwitch02FromNode1: *CREATE_LOGICAL_SWITCH_02

    VerifyClusterMembers2MP2CCPFromNode1: *VERIFY_CLUSTER_MEMBERS_2MP_2CCP_FROM_NODE_1

    CreateTransportNodesFromNode1: *CREATE_TRANSPORT_NODE_01--ESX

    CreateLogicalSwitch01FromNode1: *CREATE_LOGICAL_SWITCH_01

    DeleteLogicalSwitchFromNode1: *DELETE_LOGICAL_SWITCH_01

    DeleteTransportNodesFromNode1: *DELETE_TRANSPORT_NODE_01

    VifAttachment1: *VIF_ATTACHMENT_01--ESX

    VifDetachment1: *VIF_DETACHMENT_01--ESX

    VifDetachmentForVM1Ignore:
      Type: NetAdapter
      TestAdapter: 'vm.[1].vnic.[1]'
      reconfigure: true
      connected: 0
      startconnected: 0
      expectedResult: ignore

    VifDetachmentForVM2Ignore:
      Type: NetAdapter
      TestAdapter: 'vm.[2].vnic.[1]'
      reconfigure: true
      connected: 0
      startconnected: 0
      expectedResult: ignore

    DeleteVnic1FromVM1Ignore:
      <<: *DELETE_VNIC_1_FROM_VM1
      expectedResult: ignore

    DeleteVnic1FromVM2Ignore:
      <<: *DELETE_VNIC_1_FROM_VM2
      expectedResult: ignore

    RemoveMP_Node2Ignore:
      <<: *REMOVE_MP_NODE_2
      expectedResult: ignore

    RemoveMP_Node3Ignore:
      <<: *REMOVE_MP_NODE_3
      expectedResult: ignore

MPCluster3Nodes_StartStopProtonService:
  Product: 'NSXTransformers'
  Component: 'Clustering'
  Category: 'ManagementPlatform'
  TestName: 'MPCluster3Nodes_StartStopProtonService'
  Version: "2"
  TCMSId: ''
  Priority: 'P0'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'Join nodes to form a cluster of three nodes, verify proton down and up
            makes the cluster stable and have quorum'
  Procedure: '1. Form MP cluster of 3 nodes
              2. stop proton service of node 2 and check for cluster memebers and status
              3. stop proton service of node 3 and check for cluster memebers and status
              4. start proton service of node 3 and check for cluster memebers and status
              5. start proton service of node 2 and check for cluster memebers and status'
  ExpectedResult: 'After step 2 - 3 members and status STABLE
                   After step 3 - 3 members and status UNSTABLE
                   After step 4 - 3 members and status STABLE
                   After step 5 - 3 members and status STABLE'
  Duration: '300'
  Tags: 'nsx,management,clustering,cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'N'
  TestbedSpec: *3MP_1ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    <<: *CCPClusteringConfigurationWorkloads
    Sequence:
      - ["InitialVerificationOf3MPNodeCluster"]
      - ["SetProtonServiceIdForNode2"]
      - ["StopProtonServiceOnNode2"]
      - ["VerifyStopProtonServiceStatusForNode2"]
      - ["VerifyClusterMembers_3MP"]
      - ["Wait_For_Cluster_Status_Stable_On_Node1"]
      - ["Wait_For_Cluster_Status_Stable_On_Node3"]
      - ["SetProtonServiceIdForNode3"]
      - ["StopProtonServiceOnNode3"]
      - ["VerifyStopProtonServiceStatusForNode3"]
      - ["VerifyClusterMembers_3MP"]
      - ["Wait_For_Cluster_Status_Unstable_On_Node1"]
      - ["StartProtonServiceOnNode3"]
      - ["VerifyStartProtonServiceStatusForNode3"]
      - ["VerifyClusterMembers_3MP"]
      - ["Wait_For_Cluster_Status_Stable_On_Node1"]
      - ["Wait_For_Cluster_Status_Stable_On_Node3"]
      - ["StartProtonServiceOnNode2"]
      - ["VerifyStartProtonServiceStatusForNode2"]
      - ["VerifyClusterMembers_3MP"]
      - ["VerifyClusterStatusFromAllNodes"]
    ExitSequence:
      - ["ClusterStabilityFor3MPClusterAfterTestCaseExecution"]

    SetProtonServiceIdForNode2: *SET_PROTON_SERVICE_ID_FOR_NODE_2

    StopProtonServiceOnNode2: *STOP_PROTON_SERVICE_ON_NODE_2

    VerifyStopProtonServiceStatusForNode2: *VERFIY_STOP_PROTON_SERVICE_STATUS_FOR_NODE_2

    SetProtonServiceIdForNode3: *SET_PROTON_SERVICE_ID_FOR_NODE_3

    StopProtonServiceOnNode3: *STOP_PROTON_SERVICE_ON_NODE_3

    VerifyStopProtonServiceStatusForNode3: *VERFIY_STOP_PROTON_SERVICE_STATUS_FOR_NODE_3

    StartProtonServiceOnNode3: *START_PROTON_SERVICE_ON_NODE_3

    VerifyStartProtonServiceStatusForNode3: *VERFIY_START_PROTON_SERVICE_STATUS_FOR_NODE_3

    StartProtonServiceOnNode2: *START_PROTON_SERVICE_ON_NODE_2

    VerifyStartProtonServiceStatusForNode2: *VERFIY_START_PROTON_SERVICE_STATUS_FOR_NODE_2

MPCluster3NodesCacadedFailure_activities_failover:
  Product: 'NSXTransformers'
  Component: 'Clustering'
  Category: 'ManagementPlatform'
  TestName: 'MPCluster3NodesCacadedFailure_activities_failover'
  Version: "2"
  TCMSId: ''
  Priority: 'P0'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'Join nodes to form a cluster of three nodes Check activities are getting fired
            even after there is no write quorum'
  Procedure: '1. Login to the NSX managers
              2. Using REST call - /cluster/nodes - join second node to cluster
              3. Using REST call - /cluster/nodes - join third node to cluster
              4. Configure 1k Logical switches on node 1
              5. Shutdown node 3
              6. Configure 1k Logical switches on node 2
              7. Shutdown node 2'
  ExpectedResult: 'All 2k Logical switch configuration should go through'
  Duration: '300'
  Tags: 'nsx,management,clustering,3mp_3ccp_4esx_with_cluster_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'N'
  TestbedSpec: *3MP_3CCP_4ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    <<: *CCPClusteringConfigurationWorkloads
    Sequence:
      - ["InitialVerificationOf3MP3CCPNodeCluster"]
      - ["CreateLogicalSwitch1To100FromNode1","CreateLogicalSwitch101To200FromNode1",
         "CreateLogicalSwitch201To300FromNode1","CreateLogicalSwitch301To400FromNode1",
         "CreateLogicalSwitch401To500FromNode1","CreateLogicalSwitch501To600FromNode1",
         "CreateLogicalSwitch601To700FromNode1","CreateLogicalSwitch701To800FromNode1",
         "CreateLogicalSwitch801To900FromNode1","CreateLogicalSwitch901To1000FromNode1",]
      - ["Verify100thLogicalSwitchInfoOnControllers"]
      - ["Verify500thLogicalSwitchInfoOnControllers"]
      - ["Verify1000thLogicalSwitchInfoOnControllers"]
      - ["VifAttachment1"]
      - ["VerifyLSReplicationModeOnHostForLS1"]
      - ["ShutdownNode3"]
      - ["VerifyClusterStatusFromNodes1And2ForMPAndCCP"]
      - ["CreateLogicalSwitch1001To1100FromNode2","CreateLogicalSwitch1101To1200FromNode2",
         "CreateLogicalSwitch1201To1300FromNode2","CreateLogicalSwitch1301To1400FromNode2",
         "CreateLogicalSwitch1401To1500FromNode2","CreateLogicalSwitch1501To1600FromNode2",
         "CreateLogicalSwitch1601To1700FromNode2","CreateLogicalSwitch1701To1800FromNode2",
         "CreateLogicalSwitch1801To1900FromNode2","CreateLogicalSwitch1901To2000FromNode2",]
      - ["Verify1001thLogicalSwitchInfoOnControllers"]
      - ["Verify1501thLogicalSwitchInfoOnControllers"]
      - ["Verify2000thLogicalSwitchInfoOnControllers"]
      - ["VifAttachment2"]
      - ["VerifyLSReplicationModeOnHostForLS2"]
      - ["VerifyLSReplicationModeOnHostForLS1"]
      - ["ShutdownNode2"]
      - ["Wait_For_Cluster_Status_Unstable_On_Node1_With_Sleep"]
    ExitSequence:
      - ["ClusterStabilityFor3MP3CCPClusterAfterTestCaseExecution"]
      - ["VifDetachment1"]
      - ["VifDetachment2"]
      - ["DeleteVnic1FromVM1"]
      - ["DeleteVnic1FromVM2"]
      - ["DeleteVnic1FromVM3"]
      - ["DeleteVnic1FromVM4"]
      - ["DeleteLogicalSwitchFromNode1"]

    Wait_For_Cluster_Status_Unstable_On_Node1_With_Sleep:
      <<: *WAIT_UNSTABLE_NODE_1
      sleepbetweenworkloads: 300

    CreateLogicalSwitch1To100FromNode1:
      Type: NSX
      TestNSX: nsxmanager.[1]
      logicalswitch:
        '[1-100]':
          name: 'autogenerate'  # display_name in product schema
          summary: '1-100 logical Switch'  # description in product schema
          transport_zone_id: nsxmanager.[1].transportzone.[1]
          admin_state: UP  # switch_admin_state in product schema
          # replication_mode's value is case sensitive
          replication_mode: MTEP # source

    CreateLogicalSwitch101To200FromNode1:
      Type: NSX
      TestNSX: nsxmanager.[1]
      logicalswitch:
        '[101-200]':
          name: 'autogenerate'  # display_name in product schema
          summary: '101-200 logical Switch'  # description in product schema
          transport_zone_id: nsxmanager.[1].transportzone.[1]
          admin_state: UP  # switch_admin_state in product schema
          # replication_mode's value is case sensitive
          replication_mode: MTEP # source

    CreateLogicalSwitch201To300FromNode1:
      Type: NSX
      TestNSX: nsxmanager.[1]
      logicalswitch:
        '[201-300]':
          name: 'autogenerate'  # display_name in product schema
          summary: '201-300 logical Switch'  # description in product schema
          transport_zone_id: nsxmanager.[1].transportzone.[1]
          admin_state: UP  # switch_admin_state in product schema
          # replication_mode's value is case sensitive
          replication_mode: MTEP # source

    CreateLogicalSwitch301To400FromNode1:
      Type: NSX
      TestNSX: nsxmanager.[1]
      logicalswitch:
        '[301-400]':
          name: 'autogenerate'  # display_name in product schema
          summary: '301-400 logical Switch'  # description in product schema
          transport_zone_id: nsxmanager.[1].transportzone.[1]
          admin_state: UP  # switch_admin_state in product schema
          # replication_mode's value is case sensitive
          replication_mode: MTEP # source

    CreateLogicalSwitch401To500FromNode1:
      Type: NSX
      TestNSX: nsxmanager.[1]
      logicalswitch:
        '[401-500]':
          name: 'autogenerate'  # display_name in product schema
          summary: '401-500 logical Switch'  # description in product schema
          transport_zone_id: nsxmanager.[1].transportzone.[1]
          admin_state: UP  # switch_admin_state in product schema
          # replication_mode's value is case sensitive
          replication_mode: MTEP # source

    CreateLogicalSwitch501To600FromNode1:
      Type: NSX
      TestNSX: nsxmanager.[1]
      logicalswitch:
        '[501-600]':
          name: 'autogenerate'  # display_name in product schema
          summary: '501-600 logical Switch'  # description in product schema
          transport_zone_id: nsxmanager.[1].transportzone.[1]
          admin_state: UP  # switch_admin_state in product schema
          # replication_mode's value is case sensitive
          replication_mode: MTEP # source

    CreateLogicalSwitch601To700FromNode1:
      Type: NSX
      TestNSX: nsxmanager.[1]
      logicalswitch:
        '[601-700]':
          name: 'autogenerate'  # display_name in product schema
          summary: '601-700 logical Switch'  # description in product schema
          transport_zone_id: nsxmanager.[1].transportzone.[1]
          admin_state: UP  # switch_admin_state in product schema
          # replication_mode's value is case sensitive
          replication_mode: MTEP # source

    CreateLogicalSwitch701To800FromNode1:
      Type: NSX
      TestNSX: nsxmanager.[1]
      logicalswitch:
        '[701-800]':
          name: 'autogenerate'  # display_name in product schema
          summary: '701-800 logical Switch'  # description in product schema
          transport_zone_id: nsxmanager.[1].transportzone.[1]
          admin_state: UP  # switch_admin_state in product schema
          # replication_mode's value is case sensitive
          replication_mode: MTEP # source

    CreateLogicalSwitch801To900FromNode1:
      Type: NSX
      TestNSX: nsxmanager.[1]
      logicalswitch:
        '[801-900]':
          name: 'autogenerate'  # display_name in product schema
          summary: '801-900 logical Switch'  # description in product schema
          transport_zone_id: nsxmanager.[1].transportzone.[1]
          admin_state: UP  # switch_admin_state in product schema
          # replication_mode's value is case sensitive
          replication_mode: MTEP # source

    CreateLogicalSwitch901To1000FromNode1:
      Type: NSX
      TestNSX: nsxmanager.[1]
      logicalswitch:
        '[901-1000]':
          name: 'autogenerate'  # display_name in product schema
          summary: '901-1000 logical Switch'  # description in product schema
          transport_zone_id: nsxmanager.[1].transportzone.[1]
          admin_state: UP  # switch_admin_state in product schema
          # replication_mode's value is case sensitive
          replication_mode: MTEP # source

    CreateLogicalSwitch1001To1100FromNode2:
      Type: NSX
      TestNSX: nsxmanager.[2]
      logicalswitch:
        '[1001-1100]':
          name: 'autogenerate'  # display_name in product schema
          summary: '1001-1100 logical Switch'  # description in product schema
          transport_zone_id: nsxmanager.[1].transportzone.[1]
          admin_state: UP  # switch_admin_state in product schema
          # replication_mode's value is case sensitive
          replication_mode: MTEP # source

    CreateLogicalSwitch1101To1200FromNode2:
      Type: NSX
      TestNSX: nsxmanager.[2]
      logicalswitch:
        '[1101-1200]':
          name: 'autogenerate'  # display_name in product schema
          summary: '1101-1200 logical Switch'  # description in product schema
          transport_zone_id: nsxmanager.[1].transportzone.[1]
          admin_state: UP  # switch_admin_state in product schema
          # replication_mode's value is case sensitive
          replication_mode: MTEP # source

    CreateLogicalSwitch1201To1300FromNode2:
      Type: NSX
      TestNSX: nsxmanager.[2]
      logicalswitch:
        '[1201-1300]':
          name: 'autogenerate'  # display_name in product schema
          summary: '1201-1300 logical Switch'  # description in product schema
          transport_zone_id: nsxmanager.[1].transportzone.[1]
          admin_state: UP  # switch_admin_state in product schema
          # replication_mode's value is case sensitive
          replication_mode: MTEP # source

    CreateLogicalSwitch1301To1400FromNode2:
      Type: NSX
      TestNSX: nsxmanager.[2]
      logicalswitch:
        '[1301-1400]':
          name: 'autogenerate'  # display_name in product schema
          summary: '1301-1400 logical Switch'  # description in product schema
          transport_zone_id: nsxmanager.[1].transportzone.[1]
          admin_state: UP  # switch_admin_state in product schema
          # replication_mode's value is case sensitive
          replication_mode: MTEP # source

    CreateLogicalSwitch1401To1500FromNode2:
      Type: NSX
      TestNSX: nsxmanager.[2]
      logicalswitch:
        '[1401-1500]':
          name: 'autogenerate'  # display_name in product schema
          summary: '1401-1500 logical Switch'  # description in product schema
          transport_zone_id: nsxmanager.[1].transportzone.[1]
          admin_state: UP  # switch_admin_state in product schema
          # replication_mode's value is case sensitive
          replication_mode: MTEP # source

    CreateLogicalSwitch1501To1600FromNode2:
      Type: NSX
      TestNSX: nsxmanager.[2]
      logicalswitch:
        '[1501-1600]':
          name: 'autogenerate'  # display_name in product schema
          summary: '1501-1600 logical Switch'  # description in product schema
          transport_zone_id: nsxmanager.[1].transportzone.[1]
          admin_state: UP  # switch_admin_state in product schema
          # replication_mode's value is case sensitive
          replication_mode: MTEP # source

    CreateLogicalSwitch1601To1700FromNode2:
      Type: NSX
      TestNSX: nsxmanager.[2]
      logicalswitch:
        '[1601-1700]':
          name: 'autogenerate'  # display_name in product schema
          summary: '1601-1700 logical Switch'  # description in product schema
          transport_zone_id: nsxmanager.[1].transportzone.[1]
          admin_state: UP  # switch_admin_state in product schema
          # replication_mode's value is case sensitive
          replication_mode: MTEP # source

    CreateLogicalSwitch1701To1800FromNode2:
      Type: NSX
      TestNSX: nsxmanager.[2]
      logicalswitch:
        '[1701-1800]':
          name: 'autogenerate'  # display_name in product schema
          summary: '1701-1800 logical Switch'  # description in product schema
          transport_zone_id: nsxmanager.[1].transportzone.[1]
          admin_state: UP  # switch_admin_state in product schema
          # replication_mode's value is case sensitive
          replication_mode: MTEP # source

    CreateLogicalSwitch1801To1900FromNode2:
      Type: NSX
      TestNSX: nsxmanager.[2]
      logicalswitch:
        '[1801-1900]':
          name: 'autogenerate'  # display_name in product schema
          summary: '1801-1900 logical Switch'  # description in product schema
          transport_zone_id: nsxmanager.[1].transportzone.[1]
          admin_state: UP  # switch_admin_state in product schema
          # replication_mode's value is case sensitive
          replication_mode: MTEP # source

    CreateLogicalSwitch1901To2000FromNode2:
      Type: NSX
      TestNSX: nsxmanager.[2]
      logicalswitch:
        '[1901-2000]':
          name: 'autogenerate'  # display_name in product schema
          summary: '1901-2000 logical Switch'  # description in product schema
          transport_zone_id: nsxmanager.[1].transportzone.[1]
          admin_state: UP  # switch_admin_state in product schema
          # replication_mode's value is case sensitive
          replication_mode: MTEP # source

    Verify100thLogicalSwitchInfoOnControllers:
      Type: "Controller"
      TestController: 'nsxcontroller.[-1]'
      execution_type: *CONTROLLER_EXECUTION_TYPE
      switches: 'nsxmanager.[1].logicalswitch.[100]'
      'get_logical_switches[?]contain_once':
        table:
          - switch_vni: "nsxmanager.[1].logicalswitch.[100]"
            replication_mode: 'mtep'
            binding_type: 'vxstt'

    Verify500thLogicalSwitchInfoOnControllers:
      Type: "Controller"
      TestController: 'nsxcontroller.[-1]'
      execution_type: *CONTROLLER_EXECUTION_TYPE
      switches: 'nsxmanager.[1].logicalswitch.[500]'
      'get_logical_switches[?]contain_once':
        table:
          - switch_vni: "nsxmanager.[1].logicalswitch.[500]"
            replication_mode: 'mtep'
            binding_type: 'vxstt'

    Verify1000thLogicalSwitchInfoOnControllers:
      Type: "Controller"
      TestController: 'nsxcontroller.[-1]'
      execution_type: *CONTROLLER_EXECUTION_TYPE
      switches: 'nsxmanager.[1].logicalswitch.[1000]'
      'get_logical_switches[?]contain_once':
        table:
          - switch_vni: "nsxmanager.[1].logicalswitch.[1000]"
            replication_mode: 'mtep'
            binding_type: 'vxstt'

    Verify1001thLogicalSwitchInfoOnControllers:
      Type: "Controller"
      TestController: 'nsxcontroller.[-1]'
      execution_type: *CONTROLLER_EXECUTION_TYPE
      switches: 'nsxmanager.[1].logicalswitch.[1001]'
      'get_logical_switches[?]contain_once':
        table:
          - switch_vni: "nsxmanager.[1].logicalswitch.[1001]"
            replication_mode: 'mtep'
            binding_type: 'vxstt'

    Verify1501thLogicalSwitchInfoOnControllers:
      Type: "Controller"
      TestController: 'nsxcontroller.[-1]'
      execution_type: *CONTROLLER_EXECUTION_TYPE
      switches: 'nsxmanager.[1].logicalswitch.[1501]'
      'get_logical_switches[?]contain_once':
        table:
          - switch_vni: "nsxmanager.[1].logicalswitch.[1501]"
            replication_mode: 'mtep'
            binding_type: 'vxstt'

    Verify2000thLogicalSwitchInfoOnControllers:
      Type: "Controller"
      TestController: 'nsxcontroller.[-1]'
      execution_type: *CONTROLLER_EXECUTION_TYPE
      switches: 'nsxmanager.[1].logicalswitch.[2000]'
      'get_logical_switches[?]contain_once':
        table:
          - switch_vni: "nsxmanager.[1].logicalswitch.[2000]"
            replication_mode: 'mtep'
            binding_type: 'vxstt'

    ShutdownNode2: *SHUTDOWN_NODE_2

    ShutdownNode3: *SHUTDOWN_NODE_3

    DeleteLogicalSwitchFromNode1: *DELETE_LOGICAL_SWITCH_01

    DeleteTransportNodesFromNode1: *DELETE_TRANSPORT_NODE_01

    CreateLogicalSwitchNext1KFromNode2:
      Type: NSX
      TestNSX: nsxmanager.[2]
      logicalswitch:
        '[1001-2000]':
          name: 'ls-demo-1'  # display_name in product schema
          summary: '1st logical Switch'  # description in product schema
          transport_zone_id: nsxmanager.[1].transportzone.[1]
          admin_state: UP  # switch_admin_state in product schema
          # replication_mode's value is case sensitive
          replication_mode: MTEP # source

    VifAttachment1: *VIF_ATTACHMENT_01--ESX

    VifAttachment2: *VIF_ATTACHMENT_02--ESX

    VifDetachment1: *VIF_DETACHMENT_01--ESX

    VifDetachment2: *VIF_DETACHMENT_02--ESX

MPClusterConsistency3Nodes_Delete:
  Product: 'NSXTransformers'
  Component: 'Clustering'
  Category: 'ManagementPlatform'
  TestName: 'MPClusterConsistency3Nodes_Delete'
  Version: "2"
  TCMSId: ''
  Priority: 'P0'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  TestbedSpec: *3MP_1ESX
  Summary: 'Join nodes to form a cluster of three nodes, verify desired state is replicated
            across clusters Logical switch delete,ip pool delete used for consistency
            verifications. Deletion is done on different node than create.
            After deletion read is performed on same object on different node'
  Procedure: '1. Form cluster of 3 MP nodes
              2. Create 10 ip pool on node1
              3. Delete all configured objects from node 2'
  ExpectedResult: 'Verify these objects are deleted from all nodes by doing get operation on
                   them on other two nodes'
  Duration: '300'
  Tags: 'nsx,management,clustering,cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'N'
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    <<: *IPPoolWorkloads
    Sequence:
      - ["InitialVerificationOf3MPNodeCluster"]
      - ["CreateIPPoolFrom_Node1_1_to_10"]
      - ["ReadIPPoolFrom_Node1_For_1_10"]
      - ["ReadIPPoolFrom_Node2_For_1_10"]
      - ["ReadIPPoolFrom_Node3_For_1_10"]
      - ["DeleteAllIPPoolsFrom_Node2"]
    ExitSequence:
      - ["DeleteAllIPPoolsFrom_Node1"]

    DeleteAllIPPoolsFrom_Node1:
      <<: *DELETE_ALL_IPPOOLS
      expectedResult: ignore

MPClusterConsistency3Nodes_Update:
  Product: 'NSXTransformers'
  Component: 'Clustering'
  Category: 'ManagementPlatform'
  TestName: 'MPClusterConsistency3Nodes_Update'
  Version: "2"
  TCMSId: ''
  Priority: 'P0'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  TestbedSpec: *3MP_1ESX
  Summary: 'Join nodes to form a cluster of three nodes, verify desired state is replicated across clusters
            Logical switch update, ip ppol update used for consistency verifications updates
            done on multiple nodes'
  Procedure: '1. Form cluster of 3 MP nodes
              2. Create 10 ip pool on node 1
              3. Update all objects configured in above two steps'
  ExpectedResult: 'Verify these objects are updated on remaining two nodes'
  Duration: '300'
  Tags: 'nsx,management,clustering,cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'N'
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    <<: *IPPoolWorkloads
    Sequence:
      - ["InitialVerificationOf3MPNodeCluster"]
      - ["CreateIPPoolFrom_Node1_1_to_10"]
      - ["ReadIPPoolFrom_Node1_For_1_10"]
      - ["ReadIPPoolFrom_Node2_For_1_10"]
      - ["ReadIPPoolFrom_Node3_For_1_10"]
      - ["UpdateIPPool1From_Node2_For_1_10"]
      - ["UpdateIPPool2From_Node3_For_1_10"]
      - ["ReadPPool1From_Node2_For_1_10_After_Update"]
      - ["ReadPPool2From_Node3_For_1_10_After_Update"]
      - ["ReadIPPool4From_Node3_For_1_10"]
    ExitSequence:
      - ["DeleteAllIPPoolsFrom_Node1"]

MPClusterWithoutControlCluster:
  Product: 'NSXTransformers'
  Component: 'Clustering'
  Category: 'ManagementPlatform'
  TestName: 'MPClusterWithoutControlCluster'
  Version: "2"
  TCMSId: ''
  Priority: 'P1'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  TestbedSpec: *3MP_1ESX
  Summary: 'Join nodes to form a cluster of three nodes
            Configure LS and grouping objects on MP cluster and verify REST returns OK
            Also verify all nodes are synced with these configurations'
  Procedure: '1. Configure management plane cluster with 2 node
              2. Do not configure any control plane nodes
              3. Configure Logical switch
              4. Read configured objects from all MP nodes'
  ExpectedResult: 'All configurations should return OK All read operations from all nodes should go through'
  Duration: '300'
  Tags: 'nsx,management,clustering,cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'N'
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    <<: *IPPoolWorkloads
    <<: *CCPClusteringConfigurationWorkloads
    Sequence:
      - ["InitialVerificationOf3MPNodeCluster"]
      - ["CreateTransportZone1From_Node1"]
      - ["ReadTransportZone1FromAllNodes"]
      - ["CreateLogicalSwitch01From_Node1"]
      - ["ReadLS1FromMPNode1"]
    ExitSequence:
      - ["DeleteLogicalSwitchFrom_Node1"]
      - ["DeleteTransportZonesFrom_Node1"]

MPCluster3NodesConcurrentCrash_1:
  Product: 'NSXTransformers'
  Component: 'Clustering'
  Category: 'ManagementPlatform'
  TestName: 'MPCluster3NodesConcurrentCrash_1'
  Version: "2"
  TCMSId: ''
  Priority: 'P0'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'Join nodes to form a cluster of three nodes, simultaneously crash 2 nodes'
  Procedure: '1. Login to the NSX managers
              2. Using REST call - /cluster/nodes - join second node to cluster
              3. Using REST call - /cluster/nodes - join third node to cluster
              4. Configure LS 1 on node 1
              5. Kill nodes 2 and 3 in quick succession
              6. Configure LS 2 on node 1
              7. Start nodes 2 and 3
              8. Verify cluster status
              9. Now create LS 2 on node 1'
  ExpectedResult: 'After step 5 - LS 2 configuration should not go through after 15-30 sec
                   After step 8 - Cluster status should be STABLE
                   After step 9 - LS 2 should be created'
  Duration: '300'
  Tags: 'nsx,management,clustering,3mp_3ccp_4esx_with_cluster_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'N'
  TestbedSpec: *3MP_3CCP_4ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    <<: *CCPClusteringConfigurationWorkloads
    Sequence:
      - ["InitialVerificationOf3MP3CCPNodeCluster"]
      - ["CreateLogicalSwitch01FromNode1"]
      - ["VerifyLogicalSwitchInfo01OnControllers"]
      - ["ShutdownNode2","ShutdownNode3"]
      - ["SetProtonServiceIdFor_Node1_Sleep"]
      - ["CreateLogicalSwitch02FromNode1VerifyError"]
      - ["PowerOnMP_Node2","PowerOnMP_Node3"]
      - ["SetProtonServiceIdFor_Node1"]
      - ["RestartProtonServiceOn_Node1_WithSleep"]
      - ["Wait_For_Cluster_Status_Stable_On_Node1"]
      - ["VerifyClusterStatusFromAllNodesForMPAndCCP"]
      - ["CreateLogicalSwitch02FromNode1"]
      - ["VerifyLogicalSwitchInfo02OnControllers"]
    ExitSequence:
      - ["ClusterStabilityFor3MP3CCPClusterAfterTestCaseExecution"]
      - ["DeleteLogicalSwitchFromNode1"]

    SetProtonServiceIdFor_Node1_Sleep:
      <<: *SET_PROTON_SERVICE_ID_FOR_NODE_1
      noofretries: 10
      sleepbetweenretry: 60

    CreateLogicalSwitch01FromNode1: *CREATE_LOGICAL_SWITCH_01

    CreateLogicalSwitch02FromNode1: *CREATE_LOGICAL_SWITCH_02

    CreateLogicalSwitch02FromNode1VerifyError:
      <<: *CREATE_LOGICAL_SWITCH_02
      ExpectedResult:
        status_code: SERVICE_UNAVAILABLE

    CreateTransportNodesFromNode1: *CREATE_TRANSPORT_NODE_01--ESX

    VerifyLogicalSwitchInfo01OnControllers:
      Type: "Controller"
      TestController: 'nsxcontroller.[-1]'
      execution_type: *CONTROLLER_EXECUTION_TYPE
      switches: 'nsxmanager.[1].logicalswitch.[1]'
      'get_logical_switches[?]contain_once':
        table:
          - switch_vni: "nsxmanager.[1].logicalswitch.[1]"
            replication_mode: 'mtep'
            binding_type: 'vxstt'

    VerifyLogicalSwitchInfo02OnControllers:
      Type: "Controller"
      TestController: 'nsxcontroller.[-1]'
      execution_type: *CONTROLLER_EXECUTION_TYPE
      switches: 'nsxmanager.[1].logicalswitch.[2]'
      'get_logical_switches[?]contain_once':
        table:
          - switch_vni: "nsxmanager.[1].logicalswitch.[2]"
            replication_mode: 'mtep'
            binding_type: 'vxstt'

    ShutdownNode2: *SHUTDOWN_NODE_2

    ShutdownNode3: *SHUTDOWN_NODE_3

    PowerOnMPNode1: *POWER_ON_MP_NODE_1

    PowerOnMPNode2: *POWER_ON_MP_NODE_2

    PowerOnMPNode3: *POWER_ON_MP_NODE_3

    WaitForClusterStatusStableOnNode1: *WAIT_STABLE_NODE_1

    DeleteLogicalSwitchFromNode1: *DELETE_LOGICAL_SWITCH_01

    DeleteTransportNodesFromNode1: *DELETE_TRANSPORT_NODE_01

    RestartProtonServiceOn_Node1_WithSleep:
      <<: *RESTART_PROTON_SERVICE_ON_NODE_1
      sleepbetweenworkloads: 600

    WaitForClusterStatusUnknownOnNode1:
      <<: *WAIT_UNKNOWN_NODE_1
      sleepbetweenworkloads: 300

MPCluster_RBAC_Tests:
  Product: 'NSXTransformers'
  Component: 'Clustering'
  Category: 'ManagementPlatform'
  TestName: 'MPCluster_RBAC_Tests'
  Version: "2"
  TCMSId: ''
  Priority: 'P0'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'All clustering endpoints to be tested with all roles'
  Procedure: '1) install TACACS server
              2) Add user to TACACS configuration file
              3) configure TACACS IP in nsxmanager
              4) now form 3 cluster nodes with the user added at step 2'
  ExpectedResult: 'With user in TACACS server, CRUD on cluster should work.'
  Duration: '300'
  Tags: 'nsx,management,clustering,corner_cases_for_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'N'
  TestbedSpec: *4MP_2CCP_1AuthServer_2ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    <<: *CCPClusteringConfigurationWorkloads
    <<: *AAA_WORKLOADS
    Sequence:
      - ["SetAAAIdForNode1"]
      - ["SetAAAIdForNode2"]
      - ["SetAAAIdForNode3"]
      - ["UpdateProviderListForNode1"]
      - ["UpdateProviderListForNode2"]
      - ["UpdateProviderListForNode3"]
      - ["VerifyProviderList"]
      - ["StopAuthServer"]
      - ["BackupDefaultConfig"]
      - ["AddUser"]
      - ["StartAuthServer"]
      - ["GetMPNode1Id_With_New_User"]
      - ["GetMPNode2Id_With_New_User"]
      - ["GetMPNode3Id_With_New_User"]
      - ["ReadCluster_Node1_With_New_User"]
      - ["ReadCluster_Node2_With_New_User"]
      - ["ReadCluster_Node3_With_New_User"]
      - ["MapNSXManager1ToCluster_With_New_User"]
      - ["MapNSXManager2ToCluster_With_New_User"]
      - ["MapNSXManager3ToCluster_With_New_User"]
      - ["Wait_For_Cluster_Status_Stable_On_Node1_With_New_User"]
      - ["Wait_For_Cluster_Status_Stable_On_Node2_With_New_User"]
      - ["Wait_For_Cluster_Status_Stable_On_Node3_With_New_User"]
      - ["AddMPNode2ToClusterNode1_With_New_User"]
      - ["Wait_For_Cluster_Status_Stable_On_Node1_With_New_User"]
      - ["Wait_For_Cluster_Status_Stable_On_Node2_With_New_User"]
      - ["VerifyClusterMembers_2MP_From_Node1_With_New_User"]
      - ["AddMPNode3ToClusterNode1_With_New_User"]
      - ["Wait_For_Cluster_Status_Stable_On_Node1_With_New_User"]
      - ["Wait_For_Cluster_Status_Stable_On_Node2_With_New_User"]
      - ["Wait_For_Cluster_Status_Stable_On_Node3_With_New_User"]
      - ["VerifyClusterMembers_3MP_From_Node1_With_New_User"]
      - ["SetProtonServiceIdFor_Node2_With_New_User","SetProtonServiceIdFor_Node3_With_New_User"]
      - ["StopProtonServiceOn_Node2_With_New_User","StopProtonServiceOn_Node3_With_New_User"]
      - ["VerifyStopProtonServiceStatusFor_Node2_With_New_User","VerifyStopProtonServiceStatusFor_Node3_With_New_User"]
      - ["RemoveMP_Node2_From_Node1_With_New_User"]
      - ["RemoveMP_Node3_From_Node1_With_New_User"]
    ExitSequence:
      - ["StopAuthServer"]
      - ["RestoreDefaultConfig"]
      - ["StartAuthServer"]
      - ["Cleanup3NodesMPCluster"]

    SetAAAIdForNode1:
      Type: "NSX"
      TestNSX: "nsxmanager.[1]"
      aaa_provider:
          '[1]':
              map_object: true
              id_: 1

    SetAAAIdForNode2:
      Type: "NSX"
      TestNSX: "nsxmanager.[2]"
      aaa_provider:
          '[1]':
              map_object: true
              id_: 1

    SetAAAIdForNode3:
      Type: "NSX"
      TestNSX: "nsxmanager.[3]"
      aaa_provider:
          '[1]':
              map_object: true
              id_: 1

    GetMPNode1Id_With_New_User:
      <<: *GET_MP_NODE1_ID
      runtime_params:
        username: "sysadmin"
        password: "admin123"

    GetMPNode2Id_With_New_User:
      <<: *GET_MP_NODE2_ID
      runtime_params:
        username: "sysadmin"
        password: "admin123"

    GetMPNode3Id_With_New_User:
      <<: *GET_MP_NODE3_ID
      runtime_params:
        username: "sysadmin"
        password: "admin123"

    ReadCluster_Node1_With_New_User:
      <<: *READ_CLUSTER_NODE_1
      runtime_params:
        username: "sysadmin"
        password: "admin123"

    ReadCluster_Node2_With_New_User:
      <<: *READ_CLUSTER_NODE_2
      runtime_params:
        username: "sysadmin"
        password: "admin123"

    ReadCluster_Node3_With_New_User:
      <<: *READ_CLUSTER_NODE_3
      runtime_params:
        username: "sysadmin"
        password: "admin123"

    MapNSXManager1ToCluster_With_New_User:
      <<: *MAP_NSX_MANAGER_1_TO_CLUSTER
      runtime_params:
        username: "sysadmin"
        password: "admin123"

    MapNSXManager2ToCluster_With_New_User:
      <<: *MAP_NSX_MANAGER_2_TO_CLUSTER
      runtime_params:
        username: "sysadmin"
        password: "admin123"

    MapNSXManager3ToCluster_With_New_User:
      <<: *MAP_NSX_MANAGER_3_TO_CLUSTER
      runtime_params:
        username: "sysadmin"
        password: "admin123"

    Wait_For_Cluster_Status_Stable_On_Node1_With_New_User:
      <<: *WAIT_STABLE_NODE_1
      runtime_params:
        username: "sysadmin"
        password: "admin123"

    Wait_For_Cluster_Status_Stable_On_Node2_With_New_User:
      <<: *WAIT_STABLE_NODE_2
      runtime_params:
        username: "sysadmin"
        password: "admin123"

    Wait_For_Cluster_Status_Stable_On_Node3_With_New_User:
      <<: *WAIT_STABLE_NODE_3
      runtime_params:
        username: "sysadmin"
        password: "admin123"

    AddMPNode2ToClusterNode1_With_New_User:
      <<: *ADD_MP_NODE_2_TO_CLUSTER
      runtime_params:
        username: "sysadmin"
        password: "admin123"

    VerifyClusterMembers_2MP_From_Node1_With_New_User:
      <<: *VERIFY_CLUSTER_MEMBERS_2MP_FROM_NODE_1
      runtime_params:
        username: "sysadmin"
        password: "admin123"

    AddMPNode3ToClusterNode1_With_New_User:
      <<: *ADD_MP_NODE_3_TO_CLUSTER
      runtime_params:
        username: "sysadmin"
        password: "admin123"

    VerifyClusterMembers_3MP_From_Node1_With_New_User:
      <<: *VERIFY_CLUSTER_MEMBERS_3MP_FROM_NODE_1
      runtime_params:
        username: "sysadmin"
        password: "admin123"

    SetProtonServiceIdFor_Node2_With_New_User:
      <<: *SET_PROTON_SERVICE_ID_FOR_NODE_2
      runtime_params:
        username: "sysadmin"
        password: "admin123"

    SetProtonServiceIdFor_Node3_With_New_User:
      <<: *SET_PROTON_SERVICE_ID_FOR_NODE_3
      runtime_params:
        username: "sysadmin"
        password: "admin123"

    StopProtonServiceOn_Node2_With_New_User:
      <<: *STOP_PROTON_SERVICE_ON_NODE_2
      runtime_params:
        username: "sysadmin"
        password: "admin123"

    StopProtonServiceOn_Node3_With_New_User:
      <<: *STOP_PROTON_SERVICE_ON_NODE_3
      runtime_params:
        username: "sysadmin"
        password: "admin123"

    VerifyStopProtonServiceStatusFor_Node2_With_New_User:
      <<: *VERFIY_STOP_PROTON_SERVICE_STATUS_FOR_NODE_2
      runtime_params:
        username: "sysadmin"
        password: "admin123"

    VerifyStopProtonServiceStatusFor_Node3_With_New_User:
      <<: *VERFIY_STOP_PROTON_SERVICE_STATUS_FOR_NODE_3
      runtime_params:
        username: "sysadmin"
        password: "admin123"

    RemoveMP_Node2_From_Node1_With_New_User:
      <<: *REMOVE_MP_NODE_2
      runtime_params:
        username: "sysadmin"
        password: "admin123"

    RemoveMP_Node3_From_Node1_With_New_User:
      <<: *REMOVE_MP_NODE_3
      runtime_params:
        username: "sysadmin"
        password: "admin123"

MPClusterAddInvalidIP:
  Product: 'NSXTransformers'
  Component: 'Clustering'
  Category: 'ManagementPlatform'
  TestName: 'MPClusterAddInvalidIP'
  Version: "2"
  TCMSId: ''
  Priority: 'P2'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'Verify adding invalid ip to a cluster'
  Procedure: '1) add invalid ip while adding a node to cluster'
  ExpectedResult: 'STATUS 240 Bad Request'
  Duration: '300'
  Tags: 'nsx,management,clustering,corner_cases_for_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  TestbedSpec: *4MP_2CCP_1AuthServer_2ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    Sequence:
      - ["GetMPNode4Id"]
      - ["ReadCluster_Node4"]
      - ["MapNSXManager4ToCluster"]
      - ["Wait_For_Cluster_Status_Stable_On_Node4"]
      - ["VerifyClusterMembers_1MP_From_Node4"]
      - ["AddMPNode4WithInvalidIPToClusterNode1"]
    ExitSequence:
      - ["Wait_For_Cluster_Status_Stable_On_Node4"]
      - ["VerifyClusterMembers_1MP_From_Node4"]

    AddMPNode4WithInvalidIPToClusterNode1:
      Type: NSX
      TestNSX: "nsxmanager.[1]"
      clusternode:
        '[4]':
          mgr_role_config:
            username: 'admin'
            password: 'default'
            node_type: 'AddManagementNodeSpec'
            manager_ip: "1.2.3"
            manager_thumbprint: "nsxmanager.[4]"
      ExpectedResult:
        status_code: BAD_REQUEST

MPClusterAddInvalidCertificate:
  Product: 'NSXTransformers'
  Component: 'Clustering'
  Category: 'ManagementPlatform'
  TestName: 'MPClusterAddInvalidCertificate'
  Version: "2"
  TCMSId: ''
  Priority: 'P2'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'Verify adding invalid Certificate to a cluster'
  Procedure: '1) add invalid Certificate while adding a node to cluster'
  ExpectedResult: 'STATUS 240 Bad Request'
  Duration: '300'
  Tags: 'nsx,management,clustering,corner_cases_for_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  TestbedSpec: *4MP_2CCP_1AuthServer_2ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    Sequence:
      - ["GetMPNode4Id"]
      - ["ReadCluster_Node4"]
      - ["MapNSXManager4ToCluster"]
      - ["Wait_For_Cluster_Status_Stable_On_Node4"]
      - ["VerifyClusterMembers_1MP_From_Node4"]
      - ["AddMPNode4WithInvalidCertToClusterNode1"]
    ExitSequence:
      - ["Wait_For_Cluster_Status_Stable_On_Node4"]
      - ["VerifyClusterMembers_1MP_From_Node4"]

    AddMPNode4WithInvalidCertToClusterNode1:
      Type: NSX
      TestNSX: "nsxmanager.[1]"
      clusternode:
        '[4]':
          mgr_role_config:
            username: 'admin'
            password: 'default'
            node_type: 'AddManagementNodeSpec'
            manager_ip: "nsxmanager.[4]"
            manager_thumbprint: "invalidcertificate"
      ExpectedResult:
        status_code: BAD_REQUEST

MPClusterAddInvalidUserName:
  Product: 'NSXTransformers'
  Component: 'Clustering'
  Category: 'ManagementPlatform'
  TestName: 'MPClusterAddInvalidUserName'
  Version: "2"
  TCMSId: ''
  Priority: 'P2'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'Verify adding invalid username to a cluster'
  Procedure: '1) add invalid UserName while adding a node to cluster'
  ExpectedResult: 'STATUS 240 Bad Request'
  Duration: '300'
  Tags: 'nsx,management,clustering,corner_cases_for_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  TestbedSpec: *4MP_2CCP_1AuthServer_2ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    Sequence:
      - ["GetMPNode4Id"]
      - ["ReadCluster_Node4"]
      - ["MapNSXManager4ToCluster"]
      - ["Wait_For_Cluster_Status_Stable_On_Node4"]
      - ["VerifyClusterMembers_1MP_From_Node4"]
      - ["AddMPNode4WithInvalidUsernameToClusterNode1"]
    ExitSequence:
      - ["Wait_For_Cluster_Status_Stable_On_Node4"]
      - ["VerifyClusterMembers_1MP_From_Node4"]

    AddMPNode4WithInvalidUsernameToClusterNode1:
      Type: NSX
      TestNSX: "nsxmanager.[1]"
      clusternode:
        '[4]':
          mgr_role_config:
            username: 'invalidusername'
            password: 'default'
            node_type: 'AddManagementNodeSpec'
            manager_ip: "nsxmanager.[4]"
            manager_thumbprint: "nsxmanager.[4]"
      ExpectedResult:
        status_code: BAD_REQUEST

MPClusterAddInvalidPassword:
  Product: 'NSXTransformers'
  Component: 'Clustering'
  Category: 'ManagementPlatform'
  TestName: 'MPClusterAddInvalidPassword'
  Version: "2"
  TCMSId: ''
  Priority: 'P2'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'Verify adding invalid password to a cluster'
  Procedure: '1) add invalid Password while adding a node to cluster'
  ExpectedResult: 'STATUS 240 Bad Request'
  Duration: '300'
  Tags: 'nsx,management,clustering,corner_cases_for_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  TestbedSpec: *4MP_2CCP_1AuthServer_2ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    Sequence:
      - ["GetMPNode4Id"]
      - ["ReadCluster_Node4"]
      - ["MapNSXManager4ToCluster"]
      - ["Wait_For_Cluster_Status_Stable_On_Node4"]
      - ["VerifyClusterMembers_1MP_From_Node4"]
      - ["AddMPNode4WithInvalidPasswordToClusterNode1"]
    ExitSequence:
      - ["Wait_For_Cluster_Status_Stable_On_Node4"]
      - ["VerifyClusterMembers_1MP_From_Node4"]

    AddMPNode4WithInvalidPasswordToClusterNode1:
      Type: NSX
      TestNSX: "nsxmanager.[1]"
      clusternode:
        '[4]':
          mgr_role_config:
            username: 'admin'
            password: 'invalidpassword'
            node_type: 'AddManagementNodeSpec'
            manager_ip: "nsxmanager.[4]"
            manager_thumbprint: "nsxmanager.[4]"
      ExpectedResult:
        status_code: BAD_REQUEST

MPClusterAddInvalidType:
  Product: 'NSXTransformers'
  Component: 'Clustering'
  Category: 'ManagementPlatform'
  TestName: 'MPClusterAddInvalidType'
  Version: "2"
  TCMSId: ''
  Priority: 'P2'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'Verify adding invalid type to a cluster'
  Procedure: '1) add invalid Type while adding a node to cluster'
  ExpectedResult: 'STATUS 240 Bad Request'
  Duration: '300'
  Tags: 'nsx,management,clustering,corner_cases_for_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  TestbedSpec: *4MP_2CCP_1AuthServer_2ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringVerificationWorkloads
    Sequence:
      - ["GetMPNode1Id"]
      - ["GetMPNode2Id"]
      - ["ReadCluster_Node1"]
      - ["ReadCluster_Node2"]
      - ["MapNSXManager1ToCluster"]
      - ["MapNSXManager2ToCluster"]
      - ["Wait_For_Cluster_Status_Stable_On_Node1"]
      - ["Wait_For_Cluster_Status_Stable_On_Node2"]
      - ["VerifyClusterMembers_1MP_From_Node1"]
      - ["VerifyClusterMembers_1MP_From_Node2"]
      - ["AddMPNode2WithInvalidNodeTypeToClusterNode1"]
    ExitSequence:
      - ["Cleanup2NodesMPCluster"]

    AddMPNode2WithInvalidNodeTypeToClusterNode1:
      Type: NSX
      TestNSX: "nsxmanager.[1]"
      clusternode:
        '[2]':
          mgr_role_config:
            username: 'admin'
            password: 'default'
            node_type: 'invalidtype'
            manager_ip: "nsxmanager.[2]"
            manager_thumbprint: "nsxmanager.[2]"
      ExpectedResult:
        status_code: BAD_REQUEST

AddRemoveAndAddNode_1:
  Product: 'NSXTransformers'
  Component: 'Clustering'
  Category: 'ManagementPlatform'
  TestName: 'AddRemoveAndAddNode_1'
  Version: "2"
  TCMSId: ''
  Priority: 'P1'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'To verify when the cluster status is STABLE, user can add/remove node,if applicable, from a cluster.'
  Procedure: ' 1) form a cluster of 3 nodes n1,n2,n3
               2) verify cluster status and member list
               3) remove node n3
               4) verify cluster status and member list
               5) add node n4
               6) verify cluster status and member list'
  ExpectedResult: '1. After step 3, node n3 is removed from the cluster successfully
                   2. After step 5, node n4 is added to the cluster successfully'
  Duration: '300'
  Tags: 'nsx,management,clustering,corner_cases_for_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  TestbedSpec: *4MP_2CCP_1AuthServer_2ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    Sequence:
      - ["GetMPNodeIdForAllNodes"]
      - ["GetMPNode4Id"]
      - ["MapNSXManagerForAllNodes"]
      - ["MapNSXManager4ToCluster"]
      - ["ReadClusterForAllNodes"]
      - ["ReadCluster_Node4"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["Wait_For_Cluster_Status_Stable_On_Node4"]
      - ["Add2MPNodesToCluster"]
      - ["VerifyClusterMembers3MPFromNode1"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["DeleteNode3FromClusterNode1"]
      - ["VerifyClusterStatusFromNodes1And2"]
      - ["VerifyClusterMembers2MPFromNode1"]
      - ["AddMPNode4ToCluster"]
      - ["VerifyClusterStatusFromNodes1And2"]
      - ["WaitForClusterStatusStableOnNode4"]
      - ["ReVerifyClusterMembers3MPFromNode1"]
    ExitSequence:
      - ["CleanupMPNode4ForReuse"]
      - ["Cleanup3NodesMPCluster"]

    ReVerifyClusterMembers3MPFromNode1: *VERIFY_CLUSTER_MEMBERS_3MP_FROM_NODE_1_HAVING_NODES_1_2_4

    WaitForClusterStatusStableOnNode4: *WAIT_STABLE_NODE_4

    VerifyClusterMembers3MPFromNode1: *VERIFY_CLUSTER_MEMBERS_3MP_FROM_NODE_1

    VerifyClusterMembers2MPFromNode1: *VERIFY_CLUSTER_MEMBERS_2MP_FROM_NODE_1

RemoveNonMemberOfCluster:
  Product: 'NSXTransformers'
  Component: 'Clustering'
  Category: 'ManagementPlatform'
  TestName: 'RemoveNonMemberOfCluster'
  Version: "2"
  TCMSId: ''
  Priority: 'P2'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'Here we need to check cluster is as it was before after firing remove call for non cluster member'
  Procedure: ' 1. create  a cluster of 3 node n1,n2,n3
               2. remove n4 from n1/n2'
  ExpectedResult: 'verify cluster is as it was before'
  Duration: '300'
  Tags: 'nsx,management,clustering,corner_cases_for_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  TestbedSpec: *4MP_2CCP_1AuthServer_2ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    Sequence:
      - ["GetMPNode4Id"]
      - ["ReadCluster_Node4"]
      - ["MapNSXManager4ToCluster"]
      - ["Wait_For_Cluster_Status_Stable_On_Node4"]
      - ["VerifyClusterMembers_1MP_From_Node4"]
      - ["RemoveNonMemberNode4FromClusterNode1"]

    RemoveNonMemberNode4FromClusterNode1:
      Type : "NSX"
      TestNSX : "nsxmanager.[1]"
      deleteclusternode:  "nsxmanager.[4].clusternode.[4]"

MPClusterPartialPartition_3nodes_1:
  Product: 'NSXTransformers'
  Category: 'ManagementPlatform'
  Component: 'Clustering'
  TestName: 'MPClusterPartialPartition_3nodes_1'
  Version: "2"
  TCMSId: ''
  Priority: 'P0'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'to verify write quorum with 3 node cluster using iptable rule'
  Procedure: '1. create  a cluster of 3 node n1,n2 and n3
              2. create ip pool ippool-1
              3. read ippool-1 from all nodes
              4. add iptable rule to block communciation between node 1 and  node n2, n3.
              5. check cluster status from all nodes
              6. create ippool-2 from node 1
              7. create ippool-2 from node 2
              8. create ippool-3 from node 3
              9. now remove the iptable rules from node 1 and restart its proton service
              10. read ippool from all nodes'
  ExpectedResult: '1. After step 5 - node 1 status as UNKNOWN and node 2 and 3 as STABLE
                   2. After step 6 - verify ippool-2 not created from node 1
                   3. After step 7 - verify ippool-2 created from node 2
                   4. After step 8 - verify ippool-3 created from node 3
                   5. After step 10 - node 1,2 and 3 should show 3 ippools namely ippool-1,ippool-2 and ippool-3'
  Duration: '300'
  Tags: 'nsx,management,clustering,cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  TestbedSpec: *3MP_1ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringVerificationWorkloads
    <<: *IPPoolWorkloads
    Sequence:
      - ["InitialVerificationOf3MPNodeCluster"]
      - ["CreateIPPool1FromNode1"]
      - ["ReadIPPool1FromNode1"]
      - ["ReadIPPoolFromNode2"]
      - ["ReadIPPoolFromNode3"]
      - ["BlockMPNode2Traffic_On_Node1","BlockMPNode3Traffic_On_Node1"]
      - ["Wait_For_Cluster_Status_Unknown_On_Node1"]
      - ["VerifyClusterStatusFrom_Node2"]
      - ["VerifyClusterStatusFrom_Node3"]
      - ["CreateIPPool2From_Node1_Verify_Error"]
      - ["CreateIPPool2FromNode2"]
      - ["ReadIPPool2FromNode2"]
      - ["CreateIPPool3FromNode3"]
      - ["ReadIPPool3FromNode3"]
      - ["UnBlockMPNode2Traffic_On_Node1","UnBlockMPNode3Traffic_On_Node1"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["ReadIPPoolsFromAllNodes"]
    ExitSequence:
      - ["UnBlockMPNode2Traffic_On_Node1","UnBlockMPNode3Traffic_On_Node1"]
      - ["ClusterStabilityFor3MPClusterAfterTestCaseExecution"]
      - ["DeleteAllIPPoolsFrom_Node1"]

    CreateIPPool1FromNode1: *CREATE_IPPOOL_1

    ReadIPPool1FromNode1: *READ_IP_POOL_1_FROM_NODE_1

    CreateIPPool2FromNode2:
      <<: *CREATE_IPPOOL_2_FROM_NODE2
      sleepbetweenworkloads: 300

    ReadIPPoolFromNode2:  *READ_IP_POOL_FROM_NODE_2

    ReadIPPoolFromNode3:  *READ_IP_POOL_FROM_NODE_3

    ReadIPPool2FromNode2: *READ_IP_POOL_2_FROM_NODE_2

    CreateIPPool3FromNode3: *CREATE_IPPOOL_3_FROM_NODE3

    ReadIPPool3FromNode3: *READ_IP_POOL_3_FROM_NODE_3

    VerifyStartProtonServiceStatusFor_Node1_With_Sleep:
      <<: *VERFIY_START_PROTON_SERVICE_STATUS_FOR_NODE_1
      sleepbetweenworkloads: 120

    CreateIPPool2From_Node1_Verify_Error:
      <<: *CREATE_IPPOOL_2
      ExpectedResult:
        status_code: SERVICE_UNAVAILABLE

MPClusterConsistency3Nodes_Create:
  Product: 'NSXTransformers'
  Component: 'Clustering'
  Category: 'ManagementPlatform'
  TestName: 'MPClusterConsistency3Nodes_Create'
  Version: "2"
  TCMSId: ''
  Priority: 'P0'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  TestbedSpec: *3MP_1ESX
  Summary: 'Join nodes to form a cluster of three nodes, verify desired state is replicated
            across clusters Logical switch configuration, ip pools creations used for
            consistency verifications'
  Procedure: '1. Form cluster of 3 MP nodes
              2. Create 10 ip pools on node 1'
  ExpectedResult: 'Verify all configurations are replicated on other nodes by doing get operation'
  Duration: '300'
  Tags: 'nsx,management,clustering,cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'N'
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    <<: *IPPoolWorkloads
    Sequence:
      - ["InitialVerificationOf3MPNodeCluster"]
      - ["CreateIPPoolFrom_Node1_1_to_10"]
      - ["ReadIPPoolFrom_Node1_For_1_10"]
      - ["ReadIPPoolFrom_Node2_For_1_10"]
      - ["ReadIPPoolFrom_Node3_For_1_10"]
    ExitSequence:
      - ["ClusterStabilityFor3MPClusterAfterTestCaseExecution"]
      - ["DeleteAllIPPoolsFrom_Node1"]

MPCluster3NodesNoWriteQuorum:
  Product: 'NSXTransformers'
  Category: 'ManagementPlatform'
  Component: 'Clustering'
  TestName: 'MPCluster3NodesNoWriteQuorum'
  Version: "2"
  TCMSId: ''
  Priority: 'P0'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'Join nodes to form a cluster of three nodes. Check cluster
            is in read only mode after multiple nodes crash.'
  Procedure: '1. Login to the NSX managers
              2. Using REST call - /cluster/nodes - join second node to cluster
              3. Using REST call - /cluster/nodes - join third node to cluster
              4. Shutdown node 3
              5. Configure transport zone on node 1
              6. Shutdown node 2
              7. Configure transport zone on node 1
              8. Read configured transport zone on node 1'
  ExpectedResult: 'After step 5 - Transport zone configuration is present on
                   node 1 and node 2
                   After step 7 - Transport zone configuration does not go
                   through. API returns 503'
  Duration: '300'
  Tags: 'nsx,management,clustering,cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'N'
  TestbedSpec: *3MP_1ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    Sequence:
      - ["InitialVerificationOf3MPNodeCluster"]
      - ["CreateTransportZone1From_Node1"]
      - ["ReadTransportZone1FromAllNodes"]
      - ["Shutdown_Node3"]
      - ["CreateTransportZone2From_Node1"]
      - ["ReadTransportZone2From_Node1"]
      - ["ReadTransportZone2From_Node2"]
      - ["Shutdown_Node2_With_Sleep"]
      - ["Wait_For_Cluster_Status_Unstable_On_Node1_With_Sleep"]
      - ["CreateTransportZone3From_Node1_Verify_Error"]
    ExitSequence:
      - ["PowerOnMP_Node1"]
      - ["PowerOnMP_Node2"]
      - ["PowerOnMP_Node3"]
      - ["ClusterStabilityFor3MPClusterAfterTestCaseExecution"]
      - ["DeleteTransportZonesFrom_Node1"]

    Shutdown_Node2_With_Sleep:
      <<: *SHUTDOWN_NODE_2
      sleepbetweenworkloads: 180

    CreateTransportZone3From_Node1_Verify_Error:
      <<: *CREATE_TRANSPORT_ZONE_3
      ExpectedResult:
        status_code: SERVICE_UNAVAILABLE

    Wait_For_Cluster_Status_Unstable_On_Node1_With_Sleep:
      <<: *WAIT_UNSTABLE_NODE_1
      sleepbetweenworkloads: 300

MPCluster2NodesNoWriteQuorum_With_Graceful_Shutdown:
  Product: 'NSXTransformers'
  Category: 'ManagementPlatform'
  Component: 'Clustering'
  TestName: 'MPCluster2NodesNoWriteQuorum_With_Graceful_Shutdown'
  Version: "2"
  TCMSId: ''
  Priority: 'P1'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'Join nodes to form a cluster of two nodes check cluster is in
            read only mode after node crash'
  Procedure: '1. Login to the NSX managers
              2. Using REST call - /cluster/nodes - join second node to cluster
              3. Configure ip pool 1 on node 1
              4. Shutdown node 2 from cli gracefully and check status from node 1
              5. Configure ip pool2 on node 1'
  ExpectedResult: 'Step 4 - cluster status of node 1 should be UNSTABLE
                   Step 5 - ip pool configuration does not go through. API returns 503'
  Duration: '300'
  Tags: 'nsx,management,clustering,corner_cases_for_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'N'
  TestbedSpec: *4MP_2CCP_1AuthServer_2ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringVerificationWorkloads
    <<: *CCPClusteringConfigurationWorkloads
    Sequence:
      - ["GetMPNode1Id"]
      - ["GetMPNode2Id"]
      - ["MapNSXManager1ToCluster"]
      - ["MapNSXManager2ToCluster"]
      - ["ReadClusterNode1"]
      - ["ReadClusterNode2"]
      - ["Wait_For_Cluster_Status_Stable_On_Node1"]
      - ["Wait_For_Cluster_Status_Stable_On_Node2"]
      - ["AddMPNode2ToCluster"]
      - ["VerifyClusterMembers2MP"]
      - ["CreateIPPool1FromNode1"]
      - ["ReadIPPool1FromNode1"]
      - ["ReadIPPool1FromNode2"]
      - ["Shutdown_Node2"]
      - ["Wait_For_Cluster_Status_Unstable_On_Node1_With_Sleep"]
      - ["CreateIPPool2FromNode1VerifyError"]
    ExitSequence:
      - ["PowerOnMP_Node1","PowerOnMP_Node2"]
      - ["SetProtonServiceIdFor_Node1_Sleep"]
      - ["RestartProtonServiceOn_Node1","RestartProtonServiceOn_Node2"]
      - ["VerifyStartProtonServiceStatusFor_Node1","VerifyStartProtonServiceStatusFor_Node2"]
      - ["Wait_For_Cluster_Status_Stable_On_Node1","Wait_For_Cluster_Status_Stable_On_Node2"]
      - ["DeleteAllIPPoolsFromNode1"]
      - ["Cleanup2NodesMPCluster"]

    DeleteAllIPPoolsFromNode1: *DELETE_ALL_IPPOOLS

    CreateIPPool1FromNode1: *CREATE_IPPOOL_1

    ReadIPPool1FromNode1: *READ_IP_POOL_1_FROM_NODE_1

    ReadIPPool1FromNode2: *READ_IP_POOL_FROM_NODE_2

    CreateIPPool2FromNode1VerifyError:
      <<: *CREATE_IPPOOL_2
      ExpectedResult:
        status_code: SERVICE_UNAVAILABLE

    VerifyClusterMembers2MP: *VERIFY_CLUSTER_MEMBERS_2MP

    ReadClusterNode1: *READ_CLUSTER_NODE_1

    ReadClusterNode2: *READ_CLUSTER_NODE_2

    SetProtonServiceIdFor_Node1_Sleep:
      <<: *SET_PROTON_SERVICE_ID_FOR_NODE_1
      sleepbetweenworkloads: 300

    Wait_For_Cluster_Status_Unstable_On_Node1_With_Sleep:
      <<: *WAIT_UNSTABLE_NODE_1
      sleepbetweenworkloads: 300

MPClusterNegativeJoinTwoClusters:
  Product: 'NSXTransformers'
  Category: 'ManagementPlatform'
  Component: 'Clustering'
  TestName: 'MPClusterNegativeJoinTwoClusters'
  Version: "2"
  TCMSId: ''
  Priority: 'P1'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'Join two MP clusters'
  Procedure: '1. Using REST call - /cluster/nodes - join node from one cluster to another
              2. User should be notified that two cluster cannot be joined'
  ExpectedResult: 'Joining operation fails with user notification'
  Duration: '300'
  Tags: 'nsx,management,clustering,corner_cases_for_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'N'
  TestbedSpec: *4MP_2CCP_1AuthServer_2ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringVerificationWorkloads
    Sequence:
      - ["GetMPNodeIdForAllNodes"]
      - ["GetMPNode4Id"]
      - ["MapNSXManagerForAllNodes"]
      - ["MapNSXManager4ToCluster"]
      - ["ReadClusterForAllNodes"]
      - ["ReadCluster_Node4"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["Wait_For_Cluster_Status_Stable_On_Node4"]
      - ["VerifyClusterMembersFromAllNodes"]
      - ["VerifyClusterMembers_1MP_From_Node4"]
      - ["AddMPNode2ToCluster"]
      - ["VerifyClusterMembers_2MP"]
      - ["AddMPNode4ToClusterNode3"]
      - ["VerifyClusterMembers_2MP_From_Node3_Having_Node4"]
      - ["AddMPNode4ToClusterNode1VerifyError"]
    ExitSequence:
      - ["Cleanup3NodesMPCluster"]
      - ["CleanupMPNode4ForReuse"]

    AddMPNode4ToClusterNode1VerifyError:
      <<: *ADD_MP_NODE_4_TO_CLUSTER_MP_NODE_1
      ExpectedResult:
        status_code: BAD_REQUEST

MPClusterResidentConfigurationOnNodeToBeAdded:
  Product: 'NSXTransformers'
  Category: 'ManagementPlatform'
  Component: 'Clustering'
  TestName: 'MPClusterResidentConfigurationOnNodeToBeAdded'
  Version: "2"
  TCMSId: ''
  Priority: 'P0'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'Verify when 2 different clusters with single nodes added,
            there configurations are reflected as per the clusters configuration.'
  Procedure: '1. Install NSX Manager node n1 and Controller node c1 and configure ip pool 1 on it
              2. Install NSX Manager node n2 and Controller node c2 and  and configure ip pool 2 on it
              3. Now add node n2 to node n1'
  ExpectedResult: 'Verify node n2 wont have any congifurations like ip pool 2 and Controller node c2
                   It should have ip pool 1 and Controller node c1'
  Duration: '300'
  Tags: 'nsx,management,clustering,corner_cases_for_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  TestbedSpec: *4MP_2CCP_1AuthServer_2ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringVerificationWorkloads
    <<: *IPPoolWorkloads
    <<: *CCPClusteringConfigurationWorkloads
    Sequence:
      - ["GetMPNode1Id"]
      - ["GetMPNode2Id"]
      - ["MapNSXManager1ToCluster"]
      - ["MapNSXManager2ToCluster"]
      - ["ReadCluster_Node1"]
      - ["ReadCluster_Node2"]
      - ["Wait_For_Cluster_Status_Stable_On_Node1"]
      - ["Wait_For_Cluster_Status_Stable_On_Node2"]
      - ["VerifyClusterMembers_1MP_From_Node1"]
      - ["VerifyClusterMembers_1MP_From_Node2"]
      - ["CreateIPPool1FromNode1"]
      - ["ReadIPPool1FromNode1"]
      - ["CreateIPPool2FromNode2"]
      - ["ReadIPPool2FromNode2"]
      - ["RegisterController1"]
      - ['SetSecurityOnController1']
      - ['InitializeController1']
      - ["JoinController1ToCCPCluster"]
      - ["VerifyClusterMembers_1MP_1CCP_From_Node1"]
      - ["VerifyOneNSXManagersOnController1"]
      - ["RegisterController2WithMPNode2"]
      - ['SetSecurityOnController2']
      - ['InitializeController2']
      - ["JoinController2ToCCPClusterNode2"]
      - ["VerifyClusterMembers_1MP_1CCP_From_Node2"]
      - ["VerifyOneNSXManagersOnController2"]
      - ["AddMPNode2ToCluster"]
      - ["VerifyClusterMembers_2MP_1CCP_From_Node1"]
      - ["VerifyTwoNSXManagersOnController1"]
      - ["ReadIPPool1FromNode1"]
      - ["SetIPPool1IdForNode2"]
      - ["ReadIPPool1FromNode2"]
      - ["ReadIPPool2FromNode2VerifyError"]
    ExitSequence:
      - ["DeleteAllIPPoolsFromNode1"]
      - ["CleanupCCPNode1ForReuse"]
      - ['StopController2']
      - ['RemoveCCPNode2FromClusterNode2']
      - ["StopMootOnController2"]
      - ["ClearController2"]
      - ["DeleteMootServerOnController2"]
      - ['DeleteBootStrapOnController2']
      - ['DeleteControllerNodeUUIDFileOnController2']
      - ["StartMootOnController2"]
      - ["Cleanup2NodesMPCluster"]

    CreateIPPool1FromNode1: *CREATE_IPPOOL_1

    ReadIPPool1FromNode1: *READ_IP_POOL_1_FROM_NODE_1

    CreateIPPool2FromNode2: *CREATE_IPPOOL_2_FROM_NODE2

    ReadIPPool1FromNode2: *READ_IP_POOL_FROM_NODE_2

    ReadIPPool2FromNode2: *READ_IP_POOL_2_FROM_NODE_2

    DeleteAllIPPoolsFromNode1: *DELETE_ALL_IPPOOLS

    ReadIPPool2FromNode2VerifyError:
      <<: *READ_IP_POOL_2_FROM_NODE_2
      ExpectedResult:
        status_code: NOT_FOUND

MPClusterResidentConfigurationOnNode:
  Product: 'NSXTransformers'
  Category: 'ManagementPlatform'
  Component: 'Clustering'
  TestName: 'MPClusterResidentConfigurationOnNode'
  Version: "2"
  TCMSId: ''
  Priority: 'P0'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'Verify when a node is added to  cluster, its configurations are reflected as per the clusters configuration.'
  Procedure: '1. Install NSX Manager node n1 and Controller node c1 and configure ip pool 1 on it
              2. Install NSX Manager node n2
              3. Now add node n2 to node n1'
  ExpectedResult: 'Verify node n2 should show congifurations like ip pool 1 and Controller node c1'
  Duration: '300'
  Tags: 'nsx,management,clustering,corner_cases_for_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  TestbedSpec: *4MP_2CCP_1AuthServer_2ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringVerificationWorkloads
    <<: *IPPoolWorkloads
    <<: *CCPClusteringConfigurationWorkloads
    Sequence:
      - ["GetMPNode1Id"]
      - ["GetMPNode2Id"]
      - ["MapNSXManager1ToCluster"]
      - ["MapNSXManager2ToCluster"]
      - ["ReadCluster_Node1"]
      - ["ReadCluster_Node2"]
      - ["Wait_For_Cluster_Status_Stable_On_Node1"]
      - ["Wait_For_Cluster_Status_Stable_On_Node2"]
      - ["VerifyClusterMembers_1MP_From_Node1"]
      - ["VerifyClusterMembers_1MP_From_Node2"]
      - ["CreateIPPool1FromNode1"]
      - ["ReadIPPool1FromNode1"]
      - ["CreateDummyObjectIPPool"]
      - ["RegisterController1"]
      - ['SetSecurityOnController1']
      - ['InitializeController1']
      - ["JoinController1ToCCPCluster"]
      - ["VerifyOneNSXManagersOnController1"]
      - ["VerifyClusterMembers_1MP_1CCP_From_Node1"]
      - ["AddMPNode2ToCluster"]
      - ["VerifyClusterMembers_2MP_1CCP_From_Node1"]
      - ["VerifyTwoNSXManagersOnController1"]
      - ["ReadIPPool1FromNode1"]
      - ["SetIPPool1IdForNode2"]
      - ["ReadIPPool1FromNode2"]
    ExitSequence:
      - ["DeleteAllIPPoolsFromNode1"]
      - ["CleanupCCPNode1ForReuse"]
      - ["Cleanup2NodesMPCluster"]

    CreateIPPool1FromNode1: *CREATE_IPPOOL_1

    ReadIPPool1FromNode1: *READ_IP_POOL_1_FROM_NODE_1

    ReadIPPool1FromNode2: *READ_IP_POOL_FROM_NODE_2

    DeleteAllIPPoolsFromNode1: *DELETE_ALL_IPPOOLS

    CreateDummyObjectIPPool:
      Type: "NSX"
      TestNSX: "nsxmanager.[2]"
      ippool:
        '[1]':
          map_object: true
          id_: "nsxmanager.[1].ippool.[1]"

MPClusterDownUpAllNodesVerifyLastKnownState:
  Product: 'NSXTransformers'
  Category: 'ManagementPlatform'
  Component: 'Clustering'
  TestName: 'MPClusterDownUpAllNodesVerifyLastKnownState'
  Version: "2"
  TCMSId: ''
  Priority: 'P0'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'Verify rebooting all the nodes of a cluster does not loose the configuration objects.'
  Procedure: '1. Login to the NSX managers
              2. Using REST call - /cluster/nodes - join second node to cluster
              3. Using REST call - /cluster/nodes - join third node to cluster
              4. Verify cluster status and membership
              5. Create 10 transport zones  on node 1 and verify from other nodes
              6. Create 10 ip pools on node 1 and verify from other nodes
              7. Now reboot all the cluster nodes'
  ExpectedResult: 'Verify that all the nodes should have 10 transport zones and 10 ip pools'
  Duration: '300'
  Tags: 'nsx,management,clustering,cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'N'
  TestbedSpec: *3MP_1ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    <<: *IPPoolWorkloads
    Sequence:
      - ["InitialVerificationOf3MPNodeCluster"]
      - ["CreateTransportZone1To10From_Node1"]
      - ["ReadTZFrom_Node1_For_1_10"]
      - ["ReadTZFrom_Node2_For_1_10"]
      - ["ReadTZFrom_Node3_For_1_10"]
      - ["CreateIPPoolFrom_Node1_1_to_10"]
      - ["ReadIPPoolFrom_Node1_For_1_10"]
      - ["ReadIPPoolFrom_Node2_For_1_10"]
      - ["ReadIPPoolFrom_Node3_For_1_10"]
      - ["Restart_Node1","Restart_Node2","Restart_Node3"]
      - ["SetProtonServiceIdFor_Node1_After_Restart"]
      - ["VerifyStartProtonServiceStatusFor_Node1",
        "VerifyStartProtonServiceStatusFor_Node2",
        "VerifyStartProtonServiceStatusFor_Node3"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["VerifyClusterMembers_3MP"]
      - ["ReadTZFrom_Node1_For_1_10"]
      - ["ReadTZFrom_Node2_For_1_10"]
      - ["ReadTZFrom_Node3_For_1_10"]
      - ["ReadIPPoolFrom_Node1_For_1_10"]
      - ["ReadIPPoolFrom_Node2_For_1_10"]
      - ["ReadIPPoolFrom_Node3_For_1_10"]
    ExitSequence:
      - ["ClusterStabilityFor3MPClusterAfterTestCaseExecution"]
      - ["DeleteTransportZonesFrom_Node1"]
      - ["DeleteAllIPPoolsFrom_Node1"]

    SetProtonServiceIdFor_Node1_After_Restart:
      <<: *SET_PROTON_SERVICE_ID_FOR_NODE_1
      sleepbetweenworkloads: 600

MPClusterConcurrentReadOn3ClusterNode:
  Product: 'NSXTransformers'
  Category: 'ManagementPlatform'
  Component: 'Clustering'
  TestName: 'MPClusterConcurrentReadOn3ClusterNode'
  Version: "2"
  TCMSId: ''
  Priority: 'P0'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'Verify concurrent read on all nodes'
  Procedure: '1.Form cluster of 3 MP nodes
              2.Create 10 IPPool objects
              3.Read the created ip pool concurrently from all nodes 1,2 and 3'
  ExpectedResult: ' Verify all configurations are available on other nodes by doing get operation'
  Duration: '300'
  Tags: 'nsx,management,clustering,cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'N'
  TestbedSpec: *3MP_1ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    <<: *IPPoolWorkloads
    Sequence:
      - ["InitialVerificationOf3MPNodeCluster"]
      - ["CreateIPPoolFrom_Node1_1_to_10"]
      - ["ReadIPPoolFrom_Node1_For_1_10"]
      - ["ReadIPPoolFrom_Node2_For_1_10"]
      - ["ReadIPPoolFrom_Node3_For_1_10"]
      - ["ReadIPPool8From_Node1_For_1_10"
        ,"ReadIPPool1From_Node2_For_1_10","ReadIPPool2From_Node3_For_1_10"]
      - ["ReadIPPool1From_Node1_For_1_10",
        "ReadIPPool1From_Node2_For_1_10","ReadIPPool1From_Node3_For_1_10"]
    ExitSequence:
      - ["ClusterStabilityFor3MPClusterAfterTestCaseExecution"]
      - ["DeleteAllIPPoolsFrom_Node1"]

AddRemoveAndAddNode_2:
  Product: 'NSXTransformers'
  Component: 'Clustering'
  Category: 'ManagementPlatform'
  TestName: 'AddRemoveAndAddNode_2'
  Version: "2"
  TCMSId: ''
  Priority: 'P1'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'To verify when 2nd node shutdown able to add 3rd node to form a 3 node cluster and then remove 3rd node'
  Procedure: ' 1) form a cluster of 2 nodes n1,n2
               2) verify cluster status and member list
               3) create and verify ip pool-1 from node 1 and node 2
               4) shutdown node n2
               5) verify cluster status and member list
               6) add node n3
               7) verify cluster status and member list.create and verify ip pool-2
               8) bring up node 2
               9) verify cluster status and member list
               10) verify ip pool-1 and 2 from node 1,2 and node 3
               11) remove node 3
               12) verify cluster status and member list'
  ExpectedResult: '1. After step 3, ip pool-1 present on node n1 and n2
                   2. After step 5, cluster status UNSTABLE from node 1 and 2 members
                   3. After step 6, cluster status STABLE from node 1 and node 3 and 3 members
                   4. After step 10, ip pool-1 and 2 present on all nodes n1,n2,n3
                   5. After step 12, cluster status STABLE and 2 members'
  Duration: '300'
  Tags: 'nsx,management,clustering,corner_cases_for_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'N'
  TestbedSpec: *4MP_2CCP_1AuthServer_2ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *IPPoolWorkloads
    <<: *MPClusteringVerificationWorkloads
    Sequence:
      - ["GetMPNodeIdForAllNodes"]
      - ["MapNSXManagerForAllNodes"]
      - ["ReadClusterForAllNodes"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["AddMPNode2ToCluster"]
      - ["VerifyClusterStatusFrom_Node1"]
      - ["VerifyClusterStatusFrom_Node2"]
      - ["VerifyClusterMembers_2MP_From_Node1"]
      - ["CreateIPPool1FromNode1"]
      - ["ReadIPPool1FromNode1"]
      - ["ReadIPPool1FromNode2"]
      - ["Shutdown_Node2"]
      - ["Wait_For_Cluster_Status_Unstable_On_Node1_With_Sleep"]
      - ["VerifyClusterMembers_2MP_From_Node1"]
      - ["AddMPNode3ToCluster"]
      - ["Wait_For_Cluster_Status_Stable_On_Node1"]
      - ["Wait_For_Cluster_Status_Stable_On_Node3"]
      - ["VerifyClusterMembers_3MP_From_Node_1"]
      - ["ReadIPPool1FromNode1"]
      - ["ReadIPPool1FromNode3"]
      - ["CreateIPPool2FromNode1"]
      - ["ReadIPPool2FromNode1"]
      - ["ReadIPPool2FromNode3"]
      - ["PowerOnMP_Node2"]
      - ["Wait_For_Cluster_Status_Stable_On_Node1"]
      - ["Wait_For_Cluster_Status_Stable_On_Node3"]
      - ["Wait_For_Cluster_Status_Stable_On_Node2_With_Sleep"]
      - ["ReadIPPool1FromNode1"]
      - ["ReadIPPool1FromNode2"]
      - ["ReadIPPool1FromNode3"]
      - ["ReadIPPool2FromNode1"]
      - ["ReadIPPool2FromNode2"]
      - ["ReadIPPool2FromNode3"]
    ExitSequence:
      - ["DeleteAllIPPoolsFromNode1"]
      - ["PowerOnMP_Node2"]
      - ["Cleanup3NodesMPCluster"]

    CreateIPPool1FromNode1: *CREATE_IPPOOL_1

    ReadIPPool1FromNode1: *READ_IP_POOL_1_FROM_NODE_1

    CreateIPPool2FromNode1: *CREATE_IPPOOL_2

    ReadIPPool1FromNode2: *READ_IP_POOL_FROM_NODE_2

    ReadIPPool2FromNode1: *READ_IP_POOL_2_FROM_NODE_1

    DeleteAllIPPoolsFromNode1: *DELETE_ALL_IPPOOLS

    ReadIPPool1FromNode3: *READ_IP_POOL_FROM_NODE_3

    ReadIPPool2FromNode2: *READ_IP_POOL_2_FROM_NODE_2

    ReadIPPool2FromNode3: *READ_IP_POOL_2_FROM_NODE_3

    Wait_For_Cluster_Status_Stable_On_Node2_With_Sleep:
      <<: *WAIT_STABLE_NODE_2
      sleepbetweenworkloads: 600

    Wait_For_Cluster_Status_Unstable_On_Node1_With_Sleep:
      <<: *WAIT_UNSTABLE_NODE_1
      sleepbetweenworkloads: 300

APIUnavailableDueToRestartOrdering:
  Product: 'NSXTransformers'
  Category: 'ManagementPlatform'
  Component: 'Clustering'
  TestName: 'APIUnavailableDueToRestartOrdering'
  Version: "2"
  TCMSId: ''
  Priority: 'P1'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'Here whenever the cluster status is in "INITIALIZING", no NSX API (other than clustering endpoint)
            is available. In this case, Proton would automatically return HTTP 503 error with the cluster
            status as the payload.'
  Procedure: '1) install 3 nsx manager
              2) Form a 3 node cluster
              3) Shutdown node 1
              4) check the cluster status ==> STABLE [Since 2 nodes are up i.e majority]
              5) Shutdown node 2
              6) check the cluster status ==> UNSTABLE
              7) Shutdown node 3
              8) now start the node 1
              9) check the cluster status
              10) Now fire REST API for adding a new ip pool'
  ExpectedResult: ' 1. After step 9, cluster status should be INITIALISING and waiting for other 2 nodes(n2 and n3)
                    2. After step 10, REST API should fail with STATUS 244 Not Found'
  Duration: '300'
  Tags: 'nsx,management,clustering,cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'N'
  TestbedSpec: *3MP_1ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringVerificationWorkloads
    <<: *IPPoolWorkloads
    Sequence:
      - ["InitialVerificationOf3MPNodeCluster"]
      - ["Shutdown_Node1"]
      - ["Wait_For_Cluster_Status_Stable_On_Node2"]
      - ["Wait_For_Cluster_Status_Stable_On_Node3"]
      - ["Shutdown_Node2_With_Sleep"]
      - ["Shutdown_Node3_With_Sleep"]
      - ["PowerOnMP_Node1_With_Sleep"]
      - ["Wait_For_Cluster_Status_INIT_On_Node1_With_Sleep"]
      - ["CreateIPPool1From_Node1_Verify_Error"]
    ExitSequence:
      - ["PowerOnMP_Node2","PowerOnMP_Node1","PowerOnMP_Node3"]
      - ["ClusterStabilityFor3MPClusterAfterTestCaseExecution"]

    Shutdown_Node3_With_Sleep:
      <<: *SHUTDOWN_NODE_3
      sleepbetweenworkloads: 300

    Shutdown_Node2_With_Sleep:
      <<: *SHUTDOWN_NODE_2
      sleepbetweenworkloads: 300

    VerifyClusterStatusFrom_Node2_With_Sleep:
      <<: *VERIFY_CLUSTER_STATUS_FROM_NODE_2
      sleepbetweenworkloads: 200

    Wait_For_Cluster_Status_Unstable_On_Node3_With_Sleep:
      <<: *WAIT_UNSTABLE_NODE_3
      sleepbetweenworkloads: 120

    PowerOnMP_Node1_With_Sleep:
      <<: *POWER_ON_MP_NODE_1
      sleepbetweenworkloads: 300

    Wait_For_Cluster_Status_INIT_On_Node1_With_Sleep:
      <<: *WAIT_INIT_NODE_1
      sleepbetweenworkloads: 300

    CreateIPPool1From_Node1_Verify_Error:
      <<: *CREATE_IPPOOL_1
      ExpectedResult:
        status_code: SERVICE_UNAVAILABLE

AddRemoveNodeDuringBringUpOrder_2:
  Product: 'NSXTransformers'
  Category: 'ManagementPlatform'
  Component: 'Clustering'
  TestName: 'AddRemoveNodeDuringBringUpOrder_2'
  Version: "2"
  TCMSId: ''
  Priority: 'P2'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'To verify when the cluster status is INITIALIZING after bring up order,
            user cant add or remove node from a cluster'
  Procedure: '1) install 3 nsx manager
              2) Form a 3 node cluster
              3) Shutdown node 1
              4) check the cluster status ==> STABLE [Since 2 nodes are up i.e majority]
              5) Shutdown node 2
              6) check the cluster status ==> UNSTABLE
              7) Shutdown node 3
              8) now start the node 1
              9) check the cluster status ==> INITIALIZING
              10) Now try to delete node 3 from cluster
              11) Now try to add node 4 from cluster'
  ExpectedResult: ' After step 10 and 11 - This is not going to work as Gemfire is not yet initialized.
                   The expected workflow is to first bring this node to a STABLE or UNSTABLE state
                   (by bringing up other nodes or by revoking the nodes that cannot be brought up)
                   and then invoking the API.'
  Duration: '300'
  Tags: 'nsx,management,clustering,corner_cases_for_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'N'
  TestbedSpec: *4MP_2CCP_1AuthServer_2ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringVerificationWorkloads
    Sequence:
      - ["GetMPNodeIdForAllNodes"]
      - ["MapNSXManagerForAllNodes"]
      - ["ReadClusterForAllNodes"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["VerifyClusterMembersFromAllNodes"]
      - ["Add2MPNodesToCluster"]
      - ["VerifyClusterMembers_3MP"]
      - ["GetMPNode4Id"]
      - ["MapNSXManager4ToCluster"]
      - ["VerifyClusterMembers_1MP_From_Node4"]
      - ["Shutdown_Node1"]
      - ["ShutdownNode2WithSleep"]
      - ["ShutdownNode3WithSleep"]
      - ["PowerOnMP_Node1"]
      - ["Wait_For_Cluster_Status_INIT_On_Node1_With_Sleep"]
      - ["RemoveMPNode3VerifyError"]
      - ["AddMPNode4ToClusterNode1VerifyError"]
    ExitSequence:
      - ["PowerOnMP_Node1","PowerOnMP_Node2","PowerOnMP_Node3"]
      - ["Cleanup3NodesMPCluster"]
      - ["CleanupMPNode4ForReuse"]

    ShutdownNode2WithSleep:
      <<: *SHUTDOWN_NODE_2
      sleepbetweenworkloads: 120

    ShutdownNode3WithSleep:
      <<: *SHUTDOWN_NODE_3
      sleepbetweenworkloads: 120

    Wait_For_Cluster_Status_INIT_On_Node1_With_Sleep:
      <<: *WAIT_INIT_NODE_1
      sleepbetweenworkloads: 120

    RemoveMPNode3VerifyError:
      <<: *REMOVE_MP_NODE_3
      ExpectedResult:
        status_code: SERVICE_UNAVAILABLE

    AddMPNode4ToClusterNode1VerifyError:
      <<: *ADD_MP_NODE_4_TO_CLUSTER_MP_NODE_1
      ExpectedResult:
        status_code: SERVICE_UNAVAILABLE

AddRemoveNodeDuringBringUpOrder_1:
  Product: 'NSXTransformers'
  Component: 'Clustering'
  Category: 'ManagementPlatform'
  TestName: 'AddRemoveNodeDuringBringUpOrder_1'
  Version: "2"
  TCMSId: ''
  Priority: 'P2'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'To verify when the cluster status is STABLE after bring up order,
            user can add/remove node from a cluster.'
  Procedure: ' 1) form a cluster of 3 nodes n1,n2,n3
              2) verify cluster status and member list
              3) shutdown n3
              4) verify cluster status and member list
              5) shutdown n2
              6) verify cluster status and member list
              7) shutdown n1
              8) start n3
              9) verify cluster status and member list
             10) start n2 and n1
             11) verify cluster status and member list
             12) remove node n3
             13) verify cluster status and member list
             14) deploy new node n4 add node n4 again
             15) verify cluster status and member list'
  ExpectedResult: '1. After step 12, node n3 is removed from the cluster successfully
                   2. After step 14, node n4 is added to the cluster successfully'
  Duration: '300'
  Tags: 'nsx,management,clustering,corner_cases_for_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  TestbedSpec: *4MP_2CCP_1AuthServer_2ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringVerificationWorkloads
    Sequence:
      - ["GetMPNodeIdForAllNodes"]
      - ["MapNSXManagerForAllNodes"]
      - ["ReadClusterForAllNodes"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["VerifyClusterMembersFromAllNodes"]
      - ["Add2MPNodesToCluster"]
      - ["VerifyClusterMembers_3MP"]
      - ["GetMPNode4Id"]
      - ["MapNSXManager4ToCluster"]
      - ["VerifyClusterMembers_1MP_From_Node4"]
      - ["Shutdown_Node1"]
      - ["ShutdownNode2WithSleep"]
      - ["ShutdownNode3WithSleep"]
      - ["PowerOnMPNode1WithSleep"]
      - ["PowerOnMP_Node2","PowerOnMP_Node3"]
      - ["Wait_For_Cluster_Status_STABLE_On_Node1_With_Sleep"]
      - ["Wait_For_Cluster_Status_Stable_On_Node2"]
      - ["Wait_For_Cluster_Status_Stable_On_Node3"]
      - ["SetProtonServiceIdFor_Node3"]
      - ["StopProtonServiceOn_Node3"]
      - ["VerifyStopProtonServiceStatusFor_Node3"]
      - ["RemoveMPNode3"]
      - ["VerifyClusterMembers_2MP"]
      - ["Wait_For_Cluster_Status_Stable_On_Node1"]
      - ["Wait_For_Cluster_Status_Stable_On_Node2"]
      - ["AddMPNode4ToClusterNode1"]
      - ["VerifyClusterMembers_3MP_After_Node4_Added"]
      - ["Wait_For_Cluster_Status_Stable_On_Node1"]
      - ["Wait_For_Cluster_Status_Stable_On_Node2"]
      - ["Wait_For_Cluster_Status_Stable_On_Node4"]
    ExitSequence:
      - ["PowerOnMP_Node1","PowerOnMP_Node2","PowerOnMP_Node3"]
      - ["Cleanup3NodesMPCluster"]
      - ["CleanupMPNode4ForReuse"]

    PowerOnMPNode1WithSleep:
      <<: *POWER_ON_MP_NODE_1
      sleepbetweenworkloads: 120

    ShutdownNode2WithSleep:
      <<: *SHUTDOWN_NODE_2
      sleepbetweenworkloads: 120

    ShutdownNode3WithSleep:
      <<: *SHUTDOWN_NODE_3
      sleepbetweenworkloads: 120

    Wait_For_Cluster_Status_STABLE_On_Node1_With_Sleep:
      <<: *WAIT_STABLE_NODE_1
      sleepbetweenworkloads: 150

    RemoveMPNode3: *REMOVE_MP_NODE_3

    AddMPNode4ToClusterNode1: *ADD_MP_NODE_4_TO_CLUSTER_MP_NODE_1

MPClusterRemoveSelfFromWhenStatusUNSTABLE:
  Product: 'NSXTransformers'
  Category: 'ManagementPlatform'
  Component: 'Clustering'
  TestName: 'MPClusterRemoveSelfFromWhenStatusUNSTABLE'
  Version: "2"
  TCMSId: ''
  Priority: 'P1'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'To verify when cluster status UNTABLE, self node remove is not allowed'
  Procedure: '1. Form a cluster of 2 nodes
              2. Now Shutdown node 2 [grace power failure]
              3. Check for cluster status from node 1
              4. now delete self node 1'
  ExpectedResult: 'after step 3,it Should be UNSTABLE
                   after step 4, it should fail'
  Duration: '300'
  Tags: 'nsx,management,clustering,corner_cases_for_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  TestbedSpec: *4MP_2CCP_1AuthServer_2ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringVerificationWorkloads
    Sequence:
      - ["GetMPNode1Id"]
      - ["GetMPNode2Id"]
      - ["MapNSXManager1ToCluster"]
      - ["MapNSXManager2ToCluster"]
      - ["ReadClusterNode1"]
      - ["ReadClusterNode2"]
      - ["VerifyClusterStatusFromNode1"]
      - ["VerifyClusterStatusFromNode2"]
      - ["AddMPNode2ToCluster"]
      - ["VerifyClusterMembers2MP"]
      - ["Shutdown_Node2"]
      - ["Wait_For_Cluster_Status_Unstable_On_Node1_With_Sleep"]
      - ["SetProtonServiceIdFor_Node1"]
      - ["StopProtonServiceOn_Node1"]
      - ["VerifyStopProtonServiceStatusFor_Node1"]
      - ["RemoveMPNode1FromSelfNode1_After_Proton_Service_Down"]
    ExitSequence:
      - ["PowerOnMP_Node1","PowerOnMP_Node2"]
      - ["Cleanup2NodesMPCluster"]

    VerifyClusterMembers2MP: *VERIFY_CLUSTER_MEMBERS_2MP

    ReadClusterNode1: *READ_CLUSTER_NODE_1

    ReadClusterNode2: *READ_CLUSTER_NODE_2

    VerifyClusterStatusFromNode1: *VERIFY_CLUSTER_STATUS_FROM_NODE_1

    VerifyClusterStatusFromNode2: *VERIFY_CLUSTER_STATUS_FROM_NODE_2

    Wait_For_Cluster_Status_Stable_On_Node1_With_Sleep:
      <<: *WAIT_STABLE_NODE_1
      sleepbetweenworkloads: 120

    RemoveMPNode1FromSelfNode1_After_Proton_Service_Down:
      Type : "NSX"
      TestNSX : "nsxmanager.[1]"
      ExpectedResult:
        status_code: INTERNAL_SERVER_ERROR
      deleteclusternode:  "nsxmanager.[1].clusternode.[1]"

    Wait_For_Cluster_Status_Unstable_On_Node1_With_Sleep:
      <<: *WAIT_UNSTABLE_NODE_1
      sleepbetweenworkloads: 180

MPCluster3Nodes_BringUpOrder_2:
  Product: 'NSXTransformers'
  Category: 'ManagementPlatform'
  Component: 'Clustering'
  TestName: 'MPCluster3Nodes_BringUpOrder_2'
  Version: "2"
  TCMSId: ''
  Priority: 'P1'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  TestbedSpec: *3MP_1ESX
  Summary: 'Join nodes to form a cluster of three nodes, check for bring up sequence'
  Procedure: '1. Login to the NSX managers
              2. Form cluster of three nodes
              3. Configure transport zone 1
              4. shutdown node 1
              5. configure transport zone 2 on node 2
              6. shutdown node 2
              7. shutdown node 3
              8. bring up node 1
              9. bring up node 3
              10. configure transport zone 3 on node 1
              11. bring up node 2'
  ExpectedResult: 'Nodes 2 and 1 need to wait till node 3 is up'
  Duration: '300'
  Tags: 'nsx,management,clustering,cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'N'
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    Sequence:
      - ["InitialVerificationOf3MPNodeCluster"]
      - ["CreateTransportZone1From_Node1"]
      - ["ReadTransportZone1FromAllNodes"]
      - ["PowerOffMP_Node1"]
      - ["CreateTransportZone2From_Node2"]
      - ["ReadTransportZone2From_Node2_WithSleep"]
      - ["ReadTransportZone2From_Node3"]
      - ["PowerOffMP_Node2","PowerOffMP_Node3"]
      - ["PowerOnMP_Node1_With_Sleep"]
      - ["PowerOnMP_Node3"]
      - ["PowerOnMP_Node2"]
      - ["SetProtonServiceIdFor_Node1_Sleep"]
      - ["SetProtonServiceIdFor_Node2","SetProtonServiceIdFor_Node3"]
      - ["RestartProtonServiceOn_Node1","RestartProtonServiceOn_Node2",
        "RestartProtonServiceOn_Node3"]
      - ["VerifyStartProtonServiceStatusFor_Node1",
        "VerifyStartProtonServiceStatusFor_Node2",
        "VerifyStartProtonServiceStatusFor_Node3"]
      - ["Wait_For_Cluster_Status_Stable_On_Node1_With_Sleep"]
      - ["Wait_For_Cluster_Status_Stable_On_Node3"]
      - ["Wait_For_Cluster_Status_Stable_On_Node2"]
      - ["CreateTransportZone3From_Node1"]
      - ["ReadTransportZone1FromAllNodes"]
      - ["ReadTransportZone2FromAllNodes"]
      - ["ReadTransportZone3FromAllNodes"]
    ExitSequence:
      - ["ClusterStabilityFor3MPClusterAfterTestCaseExecution"]
      - ["DeleteTransportZonesFrom_Node1"]

    Wait_For_Cluster_Status_Stable_On_Node1_With_Sleep:
      <<: *WAIT_STABLE_NODE_1
      sleepbetweenworkloads: 120

    PowerOnMP_Node1_With_Sleep:
      <<: *POWER_ON_MP_NODE_1
      sleepbetweenworkloads: 300

    ReadTransportZone2From_Node2_WithSleep:
      <<: *READ_TRANSPORT_ZONE_2_FROM_NODE_2
      sleepbetweenworkloads: 31

    SetProtonServiceIdFor_Node1_Sleep:
      <<: *SET_PROTON_SERVICE_ID_FOR_NODE_1
      sleepbetweenworkloads: 600

MPCluster3Nodes_BringUpOrder_3:
  Product: 'NSXTransformers'
  Category: 'ManagementPlatform'
  Component: 'Clustering'
  TestName: 'MPCluster3Nodes_BringUpOrder_3'
  Version: "2"
  TCMSId: ''
  Priority: 'P1'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  TestbedSpec: *3MP_1ESX
  Summary: 'Join nodes to form a cluster of three nodes, check for bring up sequence'
  Procedure: '1. Login to the NSX managers
              2. Form cluster of three nodes
              3. Configure transport zone 1
              4. shutdown node 1
              5. configure transport zone 2 on node 2
              6. shutdown node 2
              7. shutdown node 3
              8. bring up node 3
              9. bring up node 2
              10. configure transport zone 3 on node 3
              11. bring up node 1'
  ExpectedResult: 'All Nodes should be up and running'
  Duration: '300'
  Tags: 'nsx,management,clustering,cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'N'
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    Sequence:
      - ["InitialVerificationOf3MPNodeCluster"]
      - ["CreateTransportZone1From_Node1"]
      - ["ReadTransportZone1FromAllNodes"]
      - ["Shutdown_Node1"]
      - ["CreateTransportZone2From_Node2"]
      - ["ReadTransportZone2From_Node2"]
      - ["ReadTransportZone2From_Node3"]
      - ["ShutdownNode2WithSleep"]
      - ["ShutdownNode3WithSleep"]
      - ["PowerOnMP_Node3_With_Sleep"]
      - ["PowerOnMP_Node2"]
      - ["PowerOnMP_Node1"]
      - ["Wait_For_Cluster_Status_Stable_On_Node3"]
      - ["Wait_For_Cluster_Status_Stable_On_Node2"]
      - ["Wait_For_Cluster_Status_Stable_On_Node1_With_Sleep"]
      - ["CreateTransportZone3From_Node3"]
      - ["ReadTransportZone1FromAllNodes"]
      - ["ReadTransportZone2FromAllNodes"]
      - ["ReadTransportZone3FromAllNodes"]
    ExitSequence:
      - ["ClusterStabilityFor3MPClusterAfterTestCaseExecution"]
      - ["DeleteTransportZonesFrom_Node1"]

    Wait_For_Cluster_Status_Stable_On_Node2_With_Sleep:
      <<: *WAIT_STABLE_NODE_2
      sleepbetweenworkloads: 120

    PowerOnMP_Node3_With_Sleep:
      <<: *POWER_ON_MP_NODE_3
      sleepbetweenworkloads: 600

    ShutdownNode2WithSleep:
      <<: *SHUTDOWN_NODE_2
      sleepbetweenworkloads: 180

    ShutdownNode3WithSleep:
      <<: *SHUTDOWN_NODE_3
      sleepbetweenworkloads: 180

    CreateTransportZone3From_Node3: *CREATE_TRANSPORT_ZONE_3_FROM_NODE3

    Wait_For_Cluster_Status_Stable_On_Node1_With_Sleep:
      <<: *WAIT_STABLE_NODE_1
      sleepbetweenworkloads: 300

ReuseMPNodeTest:
  Product: 'NSXTransformers'
  Category: 'ManagementPlatform'
  Component: 'Clustering'
  TestName: 'ReuseMPNodeTest'
  Version: "2"
  TCMSId: ''
  Priority: 'P1'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'To verify re-using the mp nodes'
  Procedure: '1. Create a cluster of 3 node n1,n2 and n3
              2. Remove mp node 2 and mp node 3
              3. Do cleanup
              4. Again form a 3 node cluster'
  ExpectedResult: '1. After step 1 - 3 node cluster is formed
                   2. After step 2 - node 2 and 3 is removed
                   3. After step 3 - cleanup successfull and they are now independent nodes
                   4. After step 4 - 3 node cluster is formed'
  Duration: '300'
  Tags: 'nsx,management,clustering,corner_cases_for_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  TestbedSpec: *4MP_2CCP_1AuthServer_2ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringVerificationWorkloads
    Sequence:
      - ["GetMPNodeIdForAllNodes"]
      - ["MapNSXManagerForAllNodes"]
      - ["ReadClusterForAllNodes"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["Add2MPNodesToCluster"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["VerifyClusterMembers_3MP"]
      # Now doing the cleanup part
      - ["Cleanup3NodesMPCluster"]
      # Now forming again 3 node cluster i.e re-using the MP nodes
      - ["Add2MPNodesToCluster"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["VerifyClusterMembers_3MP"]
    ExitSequence:
      - ["Cleanup3NodesMPCluster"]

MPCluster3NodesConcurrentCrash_activities_failover:
  Product: 'NSXTransformers'
  Component: 'Clustering'
  Category: 'ManagementPlatform'
  TestName: 'MPCluster3NodesConcurrentCrash_activities_failover'
  Version: "2"
  TCMSId: ''
  Priority: 'P0'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'Join nodes to form a cluster of 3 nodes, simultaneously crash 2 nodes
            and verify all activities are getting executed by the active node'
  Procedure: ' 1. Login to the NSX managers
               2. Using REST call - /cluster/nodes - join second node to cluster
               3. Using REST call - /cluster/nodes - join third node to cluster
               4. Configure transport zone 1 on node 1
               5. Configure 1k logical switches on node 1
               6. Shutdown node 2 and 3 in quick succession
               7. Configure transport zone 2 on node 1'
  ExpectedResult: 'Transport zone configuration should not go through after 15-30 sec from step 7
                   Verify all logical switches are configured after 2 nodes have gone down'
  Duration: '300'
  Tags: 'nsx,management,clustering,3mp_3ccp_4esx_with_cluster_cat'
  AutomationLevel: 'Automated'
  Developer: 'vkarra'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'N'
  TestbedSpec: *3MP_3CCP_4ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    <<: *CCPClusteringConfigurationWorkloads
    Sequence:
      - ["InitialVerificationOf3MP3CCPNodeCluster"]
      - ["CreateLogicalSwitch1To300FromNode1",
         "CreateLogicalSwitch301To600FromNode1",
         "CreateLogicalSwitch601To1000FromNode1"]
      - ["Verify100thLogicalSwitchInfoOnControllers"]
      - ["Verify500thLogicalSwitchInfoOnControllers"]
      - ["Verify1000thLogicalSwitchInfoOnControllers"]
      - ["ShutdownNode2","ShutdownNode3"]
      - ["WaitForClusterStatusUnstableOnNode1"]
      - ["CreateTransportZone2FromNode1VerifyError"]
    ExitSequence:
      - ["ClusterStabilityFor3MP3CCPClusterAfterTestCaseExecution"]
      - ["DeleteLogicalSwitchFromNode1"]

    CreateLogicalSwitch1To300FromNode1:
      Type: NSX
      TestNSX: nsxmanager.[1]
      logicalswitch:
        '[1-300]':
          name: 'autogenerate'  # display_name in product schema
          summary: '1st logical Switch'  # description in product schema
          transport_zone_id: nsxmanager.[1].transportzone.[1]
          admin_state: UP  # switch_admin_state in product schema
          # replication_mode's value is case sensitive
          replication_mode: MTEP # source

    CreateLogicalSwitch301To600FromNode1:
      Type: NSX
      TestNSX: nsxmanager.[1]
      logicalswitch:
        '[301-600]':
          name: 'autogenerate'
          summary: '1st logical Switch'
          transport_zone_id: nsxmanager.[1].transportzone.[1]
          admin_state: UP
          replication_mode: MTEP

    CreateLogicalSwitch601To1000FromNode1:
      Type: NSX
      TestNSX: nsxmanager.[1]
      logicalswitch:
        '[601-1000]':
          name: 'autogenerate'
          summary: '1st logical Switch'
          transport_zone_id: nsxmanager.[1].transportzone.[1]
          admin_state: UP
          replication_mode: MTEP

    Verify100thLogicalSwitchInfoOnControllers:
        Type: "Controller"
        TestController: 'nsxcontroller.[-1]'
        execution_type: *CONTROLLER_EXECUTION_TYPE
        switches: 'nsxmanager.[1].logicalswitch.[100]'
        'get_logical_switches[?]contain_once':
          table:
            - switch_vni: "nsxmanager.[1].logicalswitch.[100]"
              replication_mode: 'mtep'
              binding_type: 'vxstt'

    Verify500thLogicalSwitchInfoOnControllers:
      Type: "Controller"
      TestController: 'nsxcontroller.[-1]'
      execution_type: *CONTROLLER_EXECUTION_TYPE
      switches: 'nsxmanager.[1].logicalswitch.[500]'
      'get_logical_switches[?]contain_once':
        table:
          - switch_vni: "nsxmanager.[1].logicalswitch.[500]"
            replication_mode: 'mtep'
            binding_type: 'vxstt'

    Verify1000thLogicalSwitchInfoOnControllers:
      Type: "Controller"
      TestController: 'nsxcontroller.[-1]'
      execution_type: *CONTROLLER_EXECUTION_TYPE
      switches: 'nsxmanager.[1].logicalswitch.[1000]'
      'get_logical_switches[?]contain_once':
        table:
          - switch_vni: "nsxmanager.[1].logicalswitch.[1000]"
            replication_mode: 'mtep'
            binding_type: 'vxstt'

    CreateTransportZone2FromNode1VerifyError:
      <<: *CREATE_TRANSPORT_ZONE_2_FROM_NODE1
      ExpectedResult:
        status_code: SERVICE_UNAVAILABLE

    ShutdownNode2: *SHUTDOWN_NODE_2

    ShutdownNode3: *SHUTDOWN_NODE_3

    PowerOnMPNode1: *POWER_ON_MP_NODE_1

    PowerOnMPNode2: *POWER_ON_MP_NODE_2

    PowerOnMPNode3: *POWER_ON_MP_NODE_3

    WaitForClusterStatusStableOnNode1: *WAIT_STABLE_NODE_1

    DeleteLogicalSwitchFromNode1: *DELETE_LOGICAL_SWITCH_01

    DeleteTransportNodesFromNode1: *DELETE_TRANSPORT_NODE_01

    WaitForClusterStatusUnstableOnNode1:
      <<: *WAIT_UNSTABLE_NODE_1
      sleepbetweenworkloads: 180

MPClusterConcurrentUpdateOn3ClusterNode:
  Product: 'NSXTransformers'
  Component: 'Clustering'
  Category: 'ManagementPlatform'
  TestName: 'MPClusterConcurrentUpdateOn3ClusterNode'
  Version: "2"
  TCMSId: ''
  Priority: 'P0'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  TestbedSpec: *3MP_1ESX
  Summary: 'cluster with 3 mp nodes and concurrently update ip pool from node 1,2 and node 3'
  Procedure: '1. Form cluster of 3 MP nodes
              2. Create ip pool
              3. Update the created ip pool concurrently from all nodes 1,2,3 with same data'
  ExpectedResult: 'Verify ippool is updated successfully and cluster is stable'
  Duration: '300'
  Tags: 'nsx,management,clustering,cat'
  AutomationLevel: 'Automated'
  Developer: 'kchougule'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  WORKLOADS:
    <<: *MPClusteringVerificationWorkloads
    <<: *MPClusteringConfigurationWorkloads
    <<: *IPPoolWorkloads
    Sequence:
      - - InitialVerificationOf3MPNodeCluster
      - - CreateIPPool1From_Node1
      - - ReadIPPool1From_Node1
      - - ReadIPPool1From_Node2
      - - ReadIPPool1From_Node3
      - - UpdateIPPool1From_Node1
        - UpdateIPPool1From_Node2
        - UpdateIPPool1From_Node3
      - - ReadIPPoolFrom_Node1_After_Update
      - - ReadIPPoolFrom_Node2_After_Update
      - - ReadIPPoolFrom_Node3_After_Update
      - - VerifyClusterStatusFrom_Node1
      - - VerifyClusterStatusFrom_Node2
      - - VerifyClusterStatusFrom_Node3
    ExitSequence:
      - - ClusterStabilityFor3MPClusterAfterTestCaseExecution
      - - DeleteAllIPPoolsFromNode1

    ReadIPPool1From_Node2:
      <<: *READ_IP_POOL_1_FROM_NODE_1
      Testgroupingobject: "nsxmanager.[2].ippool.[1]"

    ReadIPPool1From_Node3:
      <<: *READ_IP_POOL_1_FROM_NODE_1
      Testgroupingobject: "nsxmanager.[3].ippool.[1]"

    UpdateIPPool1From_Node1:
      <<: *UPDATE_IP_POOL_FROM_NODE_1
      expectedResult: ignore

    UpdateIPPool1From_Node2:
      <<: *UPDATE_IP_POOL_FROM_NODE_2
      expectedResult: ignore

    UpdateIPPool1From_Node3:
      <<: *UPDATE_IP_POOL_FROM_NODE_3
      expectedResult: ignore

    DeleteAllIPPoolsFromNode1: *DELETE_ALL_IPPOOLS

MPClusterConcurrentDeleteOn3ClusterNode:
  Product: 'NSXTransformers'
  Component: 'Clustering'
  Category: 'ManagementPlatform'
  TestName: 'MPClusterConcurrentDeleteOn3ClusterNode'
  Version: "2"
  TCMSId: ''
  Priority: 'P0'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  TestbedSpec: *3MP_1ESX
  Summary: 'cluster with 3 mp nodes and concurrently delete ip pool from node 1,2 and node 3'
  Procedure: '1. Form cluster of 3 MP nodes
              2. Create ip pool
              3. Delete the created ip pool concurrently from all nodes 1,2,3'
  ExpectedResult: 'Verify all delete operations success and cluster status is stable'
  Duration: '300'
  Tags: 'nsx,management,clustering,cat'
  AutomationLevel: 'Automated'
  Developer: 'kchougule'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  WORKLOADS:
    <<: *MPClusteringVerificationWorkloads
    <<: *MPClusteringConfigurationWorkloads
    <<: *IPPoolWorkloads
    Sequence:
      - - InitialVerificationOf3MPNodeCluster
      - - CreateIPPool1From_Node1
      - - CreateDummyObjectIPPool
      - - ReadIPPool1From_Node1
      - - ReadIPPool1From_Node2
      - - ReadIPPool1From_Node3
      - - DeleteIPPool1From_Node1
        - DeleteIPPool1From_Node2
        - DeleteIPPool1From_Node3
      - - ReadIPPool1From_Node1_AfterDelete
      - - VerifyClusterStatusFrom_Node1
      - - VerifyClusterStatusFrom_Node2
      - - VerifyClusterStatusFrom_Node3
    ExitSequence:
      - - ClusterStabilityFor3MPClusterAfterTestCaseExecution
      - - DeleteAllIPPoolsFromNode1

    ReadIPPool1From_Node2:
      <<: *READ_IP_POOL_1_FROM_NODE_1
      Testgroupingobject: "nsxmanager.[2].ippool.[1]"

    ReadIPPool1From_Node3:
      <<: *READ_IP_POOL_1_FROM_NODE_1
      Testgroupingobject: "nsxmanager.[3].ippool.[1]"

    DeleteIPPool1From_Node1:
      <<: *DELETE_IPPOOL_1_FROM_NODE_1
      expectedResult: ignore

    DeleteIPPool1From_Node2:
      <<: *DELETE_IPPOOL_1_FROM_NODE_2
      expectedResult: ignore

    DeleteIPPool1From_Node3:
      <<: *DELETE_IPPOOL_1_FROM_NODE_1
      TestNSX: "nsxmanager.[3]"
      expectedResult: ignore

    CreateDummyObjectIPPool:
      Type: "NSX"
      TestNSX: "nsxmanager.[1]"
      ippool:
        '[2]':
          map_object: true
          id_: "nsxmanager.[1].ippool.[1]"

    ReadIPPool1From_Node1_AfterDelete:
      <<: *READ_IP_POOL_1_FROM_NODE_1
      TestGroupingObject: "nsxmanager.[1].ippool.[2]"
      ExpectedResult:
        status_code: NOT_FOUND

    DeleteAllIPPoolsFromNode1:
      <<: *DELETE_ALL_IPPOOLS
      expectedResult: ignore

RestartMPNodeMultipleTimes:
  Product: 'NSXTransformers'
  Component: 'Clustering'
  Category: 'ManagementPlatform'
  TestName: 'RestartMPNodeMultipleTimes'
  Version: "2"
  TCMSId: ''
  Priority: 'P2'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'To verify cluster status is stable and REST api is working after restarting
            MP node multiple times'
  Procedure: ' 1. Reboot node 1
               2. Wait till node 1 comes UP and then check the cluster status from all nodes
               3. Create IPPool from MP node 1
               4. Read IPPool from MP node 1
               5. Delete IPPool from MP node 1
               6. repeate these steps for multiple times'
  ExpectedResult: 'Cluster status should be stable and REST api should succeed'
  Duration: '300'
  Tags: 'nsx,management,clustering,cat'
  AutomationLevel: 'Automated'
  Developer: 'kchougule'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  TestbedSpec: *3MP_1ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringVerificationWorkloads
    <<: *IPPoolWorkloads
    Sequence:
      - - InitialVerificationOf3MPNodeCluster
      - - MapNSXManager1ToCluster
      - - SetProtonServiceIdFor_Node1
      - - Restart_Node1
      - - Wait_for_cluster_status_STABLE_on_Node1_After_Restart
      - - CreateIPPool1From_Node1
      - - ReadIPPool1From_Node1
      - - DeleteIPPool1From_Node1
      - - Restart_Node1
      - - Wait_for_cluster_status_STABLE_on_Node1_After_Restart
      - - CreateIPPool1From_Node1
      - - ReadIPPool1From_Node1
      - - DeleteIPPool1From_Node1
      - - Restart_Node1
      - - Wait_for_cluster_status_STABLE_on_Node1_After_Restart
      - - CreateIPPool1From_Node1
      - - ReadIPPool1From_Node1
      - - DeleteIPPool1From_Node1
      - - Restart_Node1
      - - Wait_for_cluster_status_STABLE_on_Node1_After_Restart
      - - CreateIPPool1From_Node1
      - - ReadIPPool1From_Node1
      - - DeleteIPPool1From_Node1
      - - VerifyClusterStatusFrom_Node1
      - - VerifyClusterStatusFrom_Node2
      - - VerifyClusterStatusFrom_Node3
    ExitSequence:
      - - ClusterStabilityFor3MPClusterAfterTestCaseExecution
      - - DeleteAllIPPoolsFromNode1

    Wait_for_cluster_status_STABLE_on_Node1_After_Restart:
        <<: *WAIT_STABLE_NODE_1
        sleepbetweenworkloads: 180

    DeleteAllIPPoolsFromNode1:
      <<: *DELETE_ALL_IPPOOLS
      expectedResult: ignore

RestartMPClusterMultipleTimes:
  Product: 'NSXTransformers'
  Component: 'Clustering'
  Category: 'ManagementPlatform'
  TestName: 'RestartMPClusterMultipleTimes'
  Version: "2"
  TCMSId: ''
  Priority: 'P2'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'To verify cluster status is stable and REST api is working after restarting
            MP cluster multiple times'
  Procedure: ' 1. Reboot MP cluster
               2. Wait till MP cluster up and then check the cluster status from all nodes
               3. Create IPPool from MP node 1
               4. Read IPPool from MP node 1
               5. Delete IPPool from MP node 1
               6. Repeate these steps for multiple times'
  ExpectedResult: 'Cluster status should be stable and REST api should succeed'
  Duration: '300'
  Tags: 'nsx,management,clustering,cat'
  AutomationLevel: 'Automated'
  Developer: 'kchougule'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'Y'
  TestbedSpec: *3MP_1ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringVerificationWorkloads
    <<: *IPPoolWorkloads
    Sequence:
      - - InitialVerificationOf3MPNodeCluster
      - - MapNSXManager1ToCluster
      - - PowerOffMP_Node1
        - PowerOffMP_Node2
        - PowerOffMP_Node3
      - - PowerOnMP_Node3_With_Delay
      - - PowerOnMP_Node2
      - - PowerOnMP_Node1
      - - Wait_for_cluster_status_STABLE_on_Node1_After_Restart
      - - Wait_For_Cluster_Status_Stable_On_Node2
      - - Wait_For_Cluster_Status_Stable_On_Node3
      - - CreateIPPool1From_Node1
      - - ReadIPPool1From_Node1
      - - DeleteIPPool1From_Node1
      - - PowerOffMP_Node1
        - PowerOffMP_Node2
        - PowerOffMP_Node3
      - - PowerOnMP_Node3_With_Delay
      - - PowerOnMP_Node2
      - - PowerOnMP_Node1
      - - Wait_for_cluster_status_STABLE_on_Node1_After_Restart
      - - Wait_For_Cluster_Status_Stable_On_Node2
      - - Wait_For_Cluster_Status_Stable_On_Node3
      - - CreateIPPool1From_Node1
      - - ReadIPPool1From_Node1
      - - DeleteIPPool1From_Node1
      - - PowerOffMP_Node1
        - PowerOffMP_Node2
        - PowerOffMP_Node3
      - - PowerOnMP_Node3_With_Delay
      - - PowerOnMP_Node2
      - - PowerOnMP_Node1
      - - Wait_for_cluster_status_STABLE_on_Node1_After_Restart
      - - Wait_For_Cluster_Status_Stable_On_Node2
      - - Wait_For_Cluster_Status_Stable_On_Node3
      - - CreateIPPool1From_Node1
      - - ReadIPPool1From_Node1
      - - DeleteIPPool1From_Node1
      - - PowerOffMP_Node1
        - PowerOffMP_Node2
        - PowerOffMP_Node3
      - - PowerOnMP_Node3_With_Delay
      - - PowerOnMP_Node2
      - - PowerOnMP_Node1
      - - Wait_for_cluster_status_STABLE_on_Node1_After_Restart
      - - Wait_For_Cluster_Status_Stable_On_Node2
      - - Wait_For_Cluster_Status_Stable_On_Node3
      - - CreateIPPool1From_Node1
      - - ReadIPPool1From_Node1
      - - DeleteIPPool1From_Node1
      - - VerifyClusterStatusFrom_Node1
      - - VerifyClusterStatusFrom_Node2
      - - VerifyClusterStatusFrom_Node3
    ExitSequence:
      - - ClusterStabilityFor3MPClusterAfterTestCaseExecution
      - - DeleteAllIPPoolsFromNode1

    PowerOnMP_Node3_With_Delay:
      <<: *POWER_ON_MP_NODE_3
      sleepbetweenworkloads: 300

    Wait_for_cluster_status_STABLE_on_Node1_After_Restart:
        <<: *WAIT_STABLE_NODE_1
        sleepbetweenworkloads: 300

    DeleteAllIPPoolsFromNode1:
      <<: *DELETE_ALL_IPPOOLS
      expectedResult: ignore

MPCluster3Nodes_BringUpOrder_7:
  Product: 'NSXTransformers'
  Category: 'ManagementPlatform'
  Component: 'Clustering'
  TestName: 'MPCluster3Nodes_BringUpOrder_7'
  Version: "2"
  TCMSId: ''
  Priority: 'P1'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  TestbedSpec: *3MP_3CCP_4ESX
  Summary: 'Join nodes to form a cluster of three nodes, check for bring up sequence'
  Procedure: '1. Create logical switch 1 from MP node 1
              2. Create logical router 1 from MP node 1
              3. Shutdown MP node 1
              4. Create logical switch 2 from MP node 2
              5. Create logical router 2 from MP node 2
              6. Shutdown MP node 2
              7. Shutdown MP node 3
              8. Bring up MP node 2
              9. Bring up MP node 1
              10. Bring up MP node 3
              11. Verify that both logical switches and both logical routers
                  are present all MP nodes'
  ExpectedResult: 'Data should be available on all MP nodes after
                   complete cluster is up and stable'
  Duration: '300'
  Tags: 'nsx,management,clustering,3mp_3ccp_4esx_with_cluster_cat'
  AutomationLevel: 'Automated'
  Developer: 'kchougule'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'N'
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    <<: *MPClusteringL3Workloads
    Sequence:
      - - MapNSXManager1ToCluster
      - - CreateLogicalSwitch01From_Node1
      - - ReadLogicalSwitch01FromNode1
      - - VerifyLogicalSwitch01InfoOnControllers
      - - CreateLogicalRouter01FromNode1
      - - ReadLogicalRouter01FromNode1
      - - Shutdown_Node1
      - - CreateLogicalSwitch02From_Node2
      - - ReadLogicalSwitch02FromNode2
      - - VerifyLogicalSwitch02InfoOnControllers
      - - CreateLogicalRouter02FromNode2
      - - ReadLogicalRouter02FromNode2
      - - ShutdownNode2WithSleep
      - - ShutdownNode3WithSleep
      - - PowerOnMP_Node2
      - - PowerOnMP_Node1
      - - PowerOnMP_Node3
      - - Wait_For_Cluster_Status_Stable_On_Node2_With_Sleep
      - - Wait_For_Cluster_Status_Stable_On_Node1
      - - Wait_For_Cluster_Status_Stable_On_Node3
      - - ReadLogicalSwitch01FromNode1
      - - ReadLogicalSwitch01FromNode2
      - - ReadLogicalSwitch01FromNode3
      - - ReadLogicalRouter01FromNode1
      - - ReadLogicalRouter01FromNode2
      - - ReadLogicalRouter01FromNode3
      - - ReadLogicalSwitch02FromNode1
      - - ReadLogicalSwitch02FromNode2
      - - ReadLogicalSwitch02FromNode3
      - - ReadLogicalRouter02FromNode1
      - - ReadLogicalRouter02FromNode2
      - - ReadLogicalRouter02FromNode3
      - - VerifyLogicalSwitch01InfoOnControllers
      - - VerifyLogicalSwitch02InfoOnControllers
    ExitSequence:
      - - ClusterStabilityFor3MP3CCPClusterAfterTestCaseExecution
      - - CleanupLogicalSwitches
      - - CleanupLogicalRouter

    Wait_For_Cluster_Status_Stable_On_Node2_With_Sleep:
      <<: *WAIT_STABLE_NODE_2
      sleepbetweenworkloads: 120

    ShutdownNode2WithSleep:
      <<: *SHUTDOWN_NODE_2
      sleepbetweenworkloads: 120

    ShutdownNode3WithSleep:
      <<: *SHUTDOWN_NODE_3
      sleepbetweenworkloads: 120

AddRemoveNodeWhenClusterStatusUNSTABLE:
  Product: 'NSXTransformers'
  Category: 'ManagementPlatform'
  Component: 'Clustering'
  TestName: 'AddRemoveNodeWhenClusterStatusUNSTABLE'
  Version: "2"
  TCMSId: ''
  Priority: 'P2'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'To verify when the cluster status is UNSTABLE,
            user can add or remove node from a cluster'
  Procedure: '1) Form a 2 node cluster
              2) verify cluster status and member list ==> STABLE and members n1& n2
              3) Shutdown node 2
              4) check the cluster status ==> UNSTABLE
              5) Add node 3
              6) Verify cluster status = > STABLE and member list contains n1, n2, n3
              7) Shutdown node 3
              8) check the cluster status ==> UNSTABLE and members n1, n2 and n3
              9) Remove node 3
              10) Check the cluster status ==> STABLE and members n1 and n2'
  ExpectedResult: 'After step4, cluster status should be UNSTABLE
                   After step5, node n3 is added to the cluster successfully
                   After step9, node n3 is removed from the cluster successfully.
                   After step 10, cluster status is STABLE.'
  Duration: '300'
  Tags: 'nsx,management,clustering,corner_cases_for_cat'
  AutomationLevel: 'Automated'
  Developer: 'yvhora'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'N'
  TestbedSpec: *4MP_2CCP_1AuthServer_2ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringVerificationWorkloads
    <<: *IPPoolWorkloads
    Sequence:
      - ["GetMPNodeIdForAllNodes"]
      - ["MapNSXManagerForAllNodes"]
      - ["ReadClusterForAllNodes"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["VerifyClusterMembersFromAllNodes"]
      - ["AddMPNode2ToCluster"]
      - ["VerifyClusterStatusFromNodes1And2"]
      - ["VerifyClusterMembers_2MP_From_Node1"]
      - ["Shutdown_Node2"]
      - ["Wait_For_Cluster_Status_Unstable_On_Node1_With_Sleep"]
      - ["AddMPNode3ToCluster"]
      - ["Wait_For_Cluster_Status_Stable_On_Node1"]
      - ["Wait_For_Cluster_Status_Stable_On_Node3"]
      - ["VerifyClusterMembers_3MP_From_Node_1"]
      - ["VerifyClusterMembers_3MP_From_Node_3"]
      - ["Shutdown_Node3_With_Sleep"]
      - ["Wait_For_Cluster_Status_Unstable_On_Node1_With_Sleep"]
      - ["VerifyClusterMembers_3MP"]
      - ["RemoveMP_Node3"]
      - ["Wait_For_Cluster_Status_Unstable_On_Node1_With_Sleep"]
      - ["VerifyClusterMembers_2MP_From_Node1"]
    ExitSequence:
      - ["Cleanup3NodesMPCluster"]

    Shutdown_Node3_With_Sleep:
      <<: *SHUTDOWN_NODE_3
      sleepbetweenworkloads: 120

    Wait_For_Cluster_Status_Unstable_On_Node1_With_Sleep:
      <<: *WAIT_UNSTABLE_NODE_1
      sleepbetweenworkloads: 180

MPCluster2NodesNoWriteQuorum_With_UnGraceful_Shutdown:
  Product: 'NSXTransformers'
  Category: 'ManagementPlatform'
  Component: 'Clustering'
  TestName: 'MPCluster2NodesNoWriteQuorum_With_UnGraceful_Shutdown'
  Version: "2"
  TCMSId: ''
  Priority: 'P1'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'Join nodes to form a cluster of two nodes check cluster is in
            read only mode after node crash'
  Procedure: '1. Login to the NSX managers
              2. Using REST call - /cluster/nodes - join second node to cluster
              3. Configure transport zone 1 on node 1
              4. Power Off Node 2 ungracefully and check status from node 1
              5. Configure transport zone 2 on node 1'
  ExpectedResult: 'Step 4 - cluster status of node 1 should be UNKNOWN
                   Step 5 - Transport zone configuration does not go through. API returns 503'
  Duration: '300'
  Tags: 'nsx,management,clustering,corner_cases_for_cat'
  AutomationLevel: 'Automated'
  Developer: 'yvhora'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'N'
  TestbedSpec: *4MP_2CCP_1AuthServer_2ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    Sequence:
      - ["GetMPNodeIdForAllNodes"]
      - ["MapNSXManagerForAllNodes"]
      - ["ReadClusterForAllNodes"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["VerifyClusterMembersFromAllNodes"]
      - ["AddMPNode2ToCluster"]
      - ["VerifyClusterStatusFromNodes1And2"]
      - ["VerifyClusterMembers_2MP_From_Node1"]
      - ["CreateTransportZone1From_Node1"]
      - ["ReadTransportZone1From_Node1"]
      - ["ReadTransportZone1From_Node2"]
      - ["PowerOffMP_Node1"]
      - ["Wait_For_Cluster_Status_Unknown_On_Node2_With_Sleep"]
      - ["CreateTransportZone2From_Node2_Verify_Error"]
    ExitSequence:
      - ["Cleanup2NodesMPCluster"]

    Wait_For_Cluster_Status_Unknown_On_Node2_With_Sleep:
      <<: *WAIT_UNKNOWN_NODE_2
      sleepbetweenworkloads: 120

    CreateTransportZone2From_Node2_Verify_Error:
      <<: *CREATE_TRANSPORT_ZONE_2_FROM_NODE2
      ExpectedResult:
        status_code: SERVICE_UNAVAILABLE

MPClusterForm3MPNodeCluster_Using_CLI:
  Product: 'NSXTransformers'
  Category: 'ManagementPlatform'
  Component: 'Clustering'
  TestName: 'MPClusterForm3MPNodeCluster_Using_CLI'
  Version: "2"
  TCMSId: ''
  Priority: 'P0'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  Summary: 'To verify formation of 3mp cluster using CLI'
  Procedure: '1) Login to NSX Managers
              2) Using CLI, join second node to cluster
              3) Using CLI, join third node to cluster
              4) Using CLI, Remove third node from cluster
              5) Using CLI, Remove second node from cluster'
  ExpectedResult: 'After step2, node 2 is successfully registered to cluster
                   After step3, node 3 is successfully registered to cluster
                   After step4, node 3 is successfully removed from cluster'
  Duration: '300'
  Tags: 'nsx,management,clustering,corner_cases_for_cat'
  AutomationLevel: 'Automated'
  Developer: 'yvhora'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'N'
  TestbedSpec: *4MP_2CCP_1AuthServer_2ESX
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringVerificationWorkloads
    Sequence:
      - ["GetMPNodeIdForAllNodes"]
      - ["MapNSXManagerForAllNodes"]
      - ["ReadClusterForAllNodes"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["AddMPNode2ToClusterNode1_Using_CLI"]
      - ["VerifyClusterStatusFromNodes1And2"]
      - ["VerifyClusterStatus_Stable_On_Node1_Using_CLI"]
      - ["VerifyClusterStatus_Stable_On_Node2_Using_CLI"]
      - ["VerifyClusterMembers_2MP_From_Node1"]
      - ["AddMPNode3ToClusterNode1_Using_CLI"]
      - ["VerifyClusterStatusFromAllNodes"]
      - ["VerifyClusterStatusFromAllNodesCLI"]
      - ["VerifyClusterMembers_3MP"]
      - ["Shutdown_Node3"]
      - ["RemoveMPNode3FromCluster_Using_CLI_With_Sleep"]
      - ["VerifyClusterStatus_Stable_On_Node1_Using_CLI"]
      - ["VerifyClusterStatus_Stable_On_Node2_Using_CLI"]
      - ["VerifyClusterMembers_2MP_From_Node1"]
      - ["Shutdown_Node2"]
      - ["RemoveMPNode2FromCluster_Using_CLI_With_Sleep"]
      - ["VerifyClusterStatus_Stable_On_Node1_Using_CLI"]
      - ["VerifyClusterMembers_1MP_From_Node1"]
    ExitSequence:
      - ["PowerOnMP_Node2","PowerOnMP_Node3"]
      - ["SetProtonServiceIdFor_Node2_With_Sleep"]
      - ["StopProtonServiceOn_Node2"]
      - ["VerifyStopProtonServiceStatusFor_Node2"]
      - ["RemoveMP_Node2Ignore"]
      - ["SetProtonServiceIdFor_Node3"]
      - ["StopProtonServiceOn_Node3"]
      - ["VerifyStopProtonServiceStatusFor_Node3"]
      - ["RemoveMP_Node3Ignore"]
      - ["CleanupMPNode3ForReuse"]
      - ["CleanupMPNode2ForReuse"]

    RemoveMP_Node2Ignore:
      <<: *REMOVE_MP_NODE_2
      expectedResult: ignore

    RemoveMP_Node3Ignore:
      <<: *REMOVE_MP_NODE_3
      expectedResult: ignore

    RemoveMPNode3FromCluster_Using_CLI_With_Sleep:
      <<: *REMOVE_MP_NODE_3_FROM_NODE_1_CLI
      sleepbetweenworkloads: 300

    RemoveMPNode2FromCluster_Using_CLI_With_Sleep:
      <<: *REMOVE_MP_NODE_2_FROM_NODE_1_CLI
      sleepbetweenworkloads: 300

MPCluster3Nodes_BringUpOrder_4_CrashNodes:
  Product: 'NSXTransformers'
  Category: 'ManagementPlatform'
  Component: 'Clustering'
  TestName: 'MPCluster3Nodes_BringUpOrder_4_CrashNodes'
  Version: "2"
  TCMSId: ''
  Priority: 'P1'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  TestbedSpec: *3MP_3CCP_4ESX_WITHOUT_CLUSTER
  Summary: 'Join nodes to form a cluster of three nodes, check for bring up sequence,
            Revoke and remove crashed nodes'
  Procedure: '1. Create logical switch 1 from MP node 1
              2. Create logical router 1 from MP node 1
              3. Shutdown MP node 1
              4. Create logical switch 2 from MP node 2
              5. Create logical router 2 from MP node 2
              6. Shutdown MP node 2
              7. Shutdown MP node 3
              8. Bring up MP node 1
              9. Revoke and remove MP node 2 and 3 from cluster
              10. Create logical switch 3 from MP node 1
              11. Create logical router 3 from MP node 1
              12. Verify that step 10 and step 11 is successful after revoke'
  ExpectedResult: 'Logical switch 1,3 and logical router 1,3 should be there
                   on MP node 1'
  Duration: '300'
  Tags: 'nsx,management,clustering,3mp_3ccp_4esx_without_cluster_cat'
  AutomationLevel: 'Automated'
  Developer: 'kchougule'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'N'
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    <<: *MPClusteringL3Workloads
    <<: *CCPClusteringConfigurationWorkloads
    Sequence:
      - - MapNSXManagerForAllNodes
      - - GetMPNodeIdForAllNodes
      - - Add2MPNodesToCluster
      - - RegisterController1
      - - RegisterController2
      - - RegisterController3
      - - SetSecurityOnController1
      - - InitializeController1
      - - JoinController1ToCCPCluster
      - - SetSecurityOnController2
      - - JoinController2ToCCPCluster
      - - SetSecurityOnController3
      - - JoinController3ToCCPCluster
      - - ActivateController2
      - - ActivateController3
      - - VerifyClusterMembers_3MP_3CCP
      - - VerifyClusterStatusFromAllNodes
      - - CreateLogicalSwitch01From_Node1
      - - ReadLogicalSwitch01FromNode1
      - - VerifyLogicalSwitch01InfoOnControllers
      - - CreateLogicalRouter01FromNode1
      - - ReadLogicalRouter01FromNode1
      - - Shutdown_Node1
      - - Wait_For_Cluster_Status_Stable_On_Node2
      - - Wait_For_Cluster_Status_Stable_On_Node3
      - - CreateLogicalSwitch02From_Node2WithSleep
      - - ReadLogicalSwitch02FromNode2
      - - VerifyLogicalSwitch02InfoOnControllers
      - - CreateLogicalRouter02FromNode2
      - - ReadLogicalRouter02FromNode2
      - - ShutdownNode2WithSleep
      - - ShutdownNode3WithSleep
      - - PowerOnMP_Node1
      - - WaitForClusterStatusInitializingOnNode1WithSleep
      - - RevokeNode2AndNode3FromNode1
      - - RemoveMP_Node2
      - - RemoveMP_Node3
      - - Wait_For_Cluster_Status_Stable_On_Node1
      - - CreateLogicalSwitch03From_Node1
      - - CreateLogicalRouter03FromNode1
      - - ReadLogicalSwitch01FromNode1
      - - ReadLogicalRouter01FromNode1
      - - ReadLogicalSwitch03FromNode1
      - - ReadLogicalRouter03FromNode1
      - - VerifyLogicalRouter02NotFoundFromNode1
    ExitSequence:
      - - PowerOnMP_Node1
        - PowerOnMP_Node2
        - PowerOnMP_Node3
      - - SetProtonServiceIdFor_Node2_WithSleep
      - - CleanupLogicalSwitches
      - - DeleteLogicalRouter01FromNode1
      - - DeleteLogicalRouter03FromNode1
      - - Cleanup3NodesCCPCluster
      - - StopProtonServiceOn_Node2Ignore
      - - VerifyStopProtonServiceStatusFor_Node2Ignore
      - - RemoveMP_Node2Ignore
      - - SetProtonServiceIdFor_Node3Ignore
      - - StopProtonServiceOn_Node3Igore
      - - VerifyStopProtonServiceStatusFor_Node3Ignore
      - - RemoveMP_Node3Ignore
      - - CleanupMPNode3ForReuse
      - - CleanupMPNode2ForReuse

    StopProtonServiceOn_Node2Ignore:
      <<: *STOP_PROTON_SERVICE_ON_NODE_2
      expectedResult: ignore

    VerifyStopProtonServiceStatusFor_Node2Ignore:
      <<: *VERFIY_STOP_PROTON_SERVICE_STATUS_FOR_NODE_2
      expectedResult: ignore

    SetProtonServiceIdFor_Node3Ignore:
      <<: *SET_PROTON_SERVICE_ID_FOR_NODE_3
      expectedResult: ignore

    StopProtonServiceOn_Node3Igore:
      <<: *STOP_PROTON_SERVICE_ON_NODE_3
      expectedResult: ignore

    VerifyStopProtonServiceStatusFor_Node3Ignore:
      <<: *VERFIY_STOP_PROTON_SERVICE_STATUS_FOR_NODE_3
      expectedResult: ignore

    SetProtonServiceIdFor_Node2_WithSleep:
      <<: *SET_PROTON_SERVICE_ID_FOR_NODE_2
      sleepbetweenworkloads: 300
      expectedResult: ignore

    RemoveMP_Node2Ignore:
      <<: *REMOVE_MP_NODE_2
      expectedResult: ignore

    RemoveMP_Node3Ignore:
      <<: *REMOVE_MP_NODE_3
      expectedResult: ignore

    CreateLogicalSwitch02From_Node2WithSleep:
      <<: *CREATE_LOGICAL_SWITCH_02_FROM_NODE2
      sleepbetweenworkloads: 300

    ShutdownNode2WithSleep:
      <<: *SHUTDOWN_NODE_2
      sleepbetweenworkloads: 180

    ShutdownNode3WithSleep:
      <<: *SHUTDOWN_NODE_3
      sleepbetweenworkloads: 180

    WaitForClusterStatusInitializingOnNode1WithSleep:
      Type: 'Cluster'
      TestCluster: 'nsxmanager.[1].cluster.[1]'
      sleepbetweenworkloads: 300
      wait_for_required_cluster_status:
        'required_status': 'INITIALIZING'
        'time_to_monitor': '300'

    RevokeNode2AndNode3FromNode1:
      Type : "Cluster"
      TestCluster : "nsxmanager.[1].clusternode.[2]"
      revoke_cluster_node:
        hosts:
          - 'host2'
          - 'host3'

    RemoveMP_Node2_WithIgnore:
      <<: *REMOVE_MP_NODE_2
      expectedResult: ignore

    RemoveMP_Node3_WithIgnore:
      <<: *REMOVE_MP_NODE_3
      expectedResult: ignore

    VerifyLogicalRouter02NotFoundFromNode1:
      <<: *READ_LOGICALROUTER_02_FROM_NODE1
      ExpectedResult:
        status_code: NOT_FOUND

MPCluster3Nodes_BringUpOrder_5_CrashNodes:
  Product: 'NSXTransformers'
  Category: 'ManagementPlatform'
  Component: 'Clustering'
  TestName: 'MPCluster3Nodes_BringUpOrder_5_CrashNodes'
  Version: "2"
  TCMSId: ''
  Priority: 'P1'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  TestbedSpec: *3MP_3CCP_4ESX_WITHOUT_CLUSTER
  Summary: 'Join nodes to form a cluster of three nodes, check for bring up
            sequence, Revoke and remove crashed nodes'
  Procedure: '1. Create logical switch 1 from MP node 1
              2. Create logical router 1 from MP node 1
              3. Shutdown MP node 1
              4. Create logical switch 2 from MP node 2
              5. Create logical router 2 from MP node 2
              6. Shutdown MP node 2
              7. Shutdown MP node 3
              8. Bring up MP node 1
              9. Bring up MP node 2
              10. Revoke and remove MP node 3 from cluster
              11. Create logical switch 3 from MP node 1
              12. Create logical router 3 from MP node 1
              13. Verify that step 11 and step 12 is successful after revoke'
  ExpectedResult: 'Logical switch 1,2,3 and logical router 1,2,3 should be
                   there on MP node 1'
  Duration: '300'
  Tags: 'nsx,management,clustering,3mp_3ccp_4esx_without_cluster_cat'
  AutomationLevel: 'Automated'
  Developer: 'kchougule'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'N'
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    <<: *MPClusteringL3Workloads
    <<: *CCPClusteringConfigurationWorkloads
    Sequence:
      - - Add2MPNodesToCluster
      - - RegisterController1
      - - RegisterController2
      - - RegisterController3
      - - SetSecurityOnController1
      - - InitializeController1
      - - JoinController1ToCCPCluster
      - - SetSecurityOnController2
      - - JoinController2ToCCPCluster
      - - SetSecurityOnController3
      - - JoinController3ToCCPCluster
      - - ActivateController2
      - - ActivateController3
      - - GetMPNode1Id
      - - GetMPNode2Id
      - - GetMPNode3Id
      - - VerifyClusterMembers_3MP_3CCP
      - - MapNSXManager1ToCluster
      - - VerifyClusterStatusFromAllNodes
      - - CreateLogicalSwitch01From_Node1
      - - ReadLogicalSwitch01FromNode1
      - - VerifyLogicalSwitch01InfoOnControllers
      - - CreateLogicalRouter01FromNode1
      - - ReadLogicalRouter01FromNode1
      - - Shutdown_Node1WithSleep
      - - CreateLogicalSwitch02From_Node2
      - - ReadLogicalSwitch02FromNode2
      - - VerifyLogicalSwitch02InfoOnControllers
      - - CreateLogicalRouter02FromNode2
      - - ReadLogicalRouter02FromNode2
      - - ShutdownNode2WithSleep
      - - ShutdownNode3WithSleep
      - - PowerOnMP_Node1
      - - PowerOnMP_Node2
      - - WaitForClusterStatusInitializingOnNode1WithSleep
      - - RevokeNode3FromNode2
      - - RemoveMP_Node3
      - - Wait_For_Cluster_Status_Stable_On_Node1
      - - Wait_For_Cluster_Status_Stable_On_Node2
      - - CreateLogicalSwitch03From_Node1
      - - CreateLogicalRouter03FromNode1
      - - ReadLogicalSwitch01FromNode1
      - - ReadLogicalRouter01FromNode1
      - - ReadLogicalSwitch02FromNode1
      - - ReadLogicalRouter02FromNode1
      - - ReadLogicalSwitch03FromNode1
      - - ReadLogicalRouter03FromNode1
      - - VerifyLogicalSwitch01InfoOnControllers
      - - VerifyLogicalSwitch02InfoOnControllers
      - - VerifyLogicalSwitch03InfoOnControllers
    ExitSequence:
      - - CleanupLogicalSwitches
      - - CleanupLogicalRouter
      - - Cleanup3NodesCCPCluster
      - - SetProtonServiceIdFor_Node2_With_Sleep
      - - StopProtonServiceOn_Node2
      - - VerifyStopProtonServiceStatusFor_Node2
      - - RemoveMP_Node2
      - - PowerOnMP_Node3
      - - Cleanup3NodesCCPCluster
      - - SetProtonServiceIdFor_Node3_With_Sleep
      - - StopProtonServiceOn_Node3
      - - VerifyStopProtonServiceStatusFor_Node3
      - - RemoveMP_Node3_WithIgnore
      - - CleanupMPNode2ForReuse
      - - CleanupMPNode3ForReuse

    Shutdown_Node1WithSleep:
      <<: *SHUTDOWN_NODE_1
      sleepbetweenworkloads: 120

    ShutdownNode2WithSleep:
      <<: *SHUTDOWN_NODE_2
      sleepbetweenworkloads: 120

    ShutdownNode3WithSleep:
      <<: *SHUTDOWN_NODE_3
      sleepbetweenworkloads: 120

    WaitForClusterStatusInitializingOnNode1WithSleep:
      Type: 'Cluster'
      TestCluster: 'nsxmanager.[1].cluster.[1]'
      sleepbetweenworkloads: 180
      wait_for_required_cluster_status:
        'required_status': 'INITIALIZING'
        'time_to_monitor': '300'

    RevokeNode3FromNode2:
      Type : "Cluster"
      TestCluster : "nsxmanager.[2].clusternode.[2]"
      revoke_cluster_node:
        hosts:
          - 'host3'

    VerifyLogicalSwitch03InfoOnControllers:
      Type: "Controller"
      TestController: 'nsxcontroller.[-1]'
      execution_type: *CONTROLLER_EXECUTION_TYPE1
      switches: 'nsxmanager.[1].logicalswitch.[3]'
      'get_logical_switches[?]contain_once':
        table:
          - switch_vni: "nsxmanager.[1].logicalswitch.[3]"
            replication_mode: 'mtep'
            binding_type: 'vxstt'

    RemoveMP_Node3_WithIgnore:
      <<: *REMOVE_MP_NODE_3
      expectedResult: ignore

MPCluster3Nodes_BringUpOrder_6_CrashNodes:
  Product: 'NSXTransformers'
  Category: 'ManagementPlatform'
  Component: 'Clustering'
  TestName: 'MPCluster3Nodes_BringUpOrder_6_CrashNodes'
  Version: "2"
  TCMSId: ''
  Priority: 'P1'
  PMT: ''
  Testcaselevel: 'Functional'
  Testcasetype: 'Functional'
  QCPath:  ''
  TestbedSpec: *3MP_3CCP_4ESX_WITHOUT_CLUSTER
  Summary: 'Join nodes to form a cluster of three nodes, check for bring up
            sequence, Revoke and remove crashed nodes'
  Procedure: '1. Create logical switch 1 from MP node 1
              2. Create logical router 1 from MP node 1
              3. Poweroff MP node 1 and node 2 simultaneously
              4. Shutdown MP node 3
              5. Bring up MP node 3
              6. Revoke and remove MP node 1 and 2 from cluster
              7. Create logical switch 2 from MP node 3
              8. Create logical router 2 from MP node 3
              9. Verify that step 7 and step 8 is successful after revoke'
  ExpectedResult: 'Logical switch 1,2 and logical router 1,2 should be
                   there on MP node 3'
  Duration: '300'
  Tags: 'nsx,management,clustering,3mp_3ccp_4esx_without_cluster_cat'
  AutomationLevel: 'Automated'
  Developer: 'kchougule'
  FullyAutomatable: 'Y'
  Status: 'Execution Ready'
  PartnerFacing: 'N'
  WORKLOADS:
    <<: *MPClusteringConfigurationWorkloads
    <<: *MPClusteringL2Workloads
    <<: *MPClusteringVerificationWorkloads
    <<: *MPClusteringL3Workloads
    <<: *CCPClusteringConfigurationWorkloads
    Sequence:
      - - Add2MPNodesToCluster
      - - RegisterController1
      - - RegisterController2
      - - RegisterController3
      - - SetSecurityOnController1
      - - InitializeController1
      - - JoinController1ToCCPCluster
      - - SetSecurityOnController2
      - - JoinController2ToCCPCluster
      - - SetSecurityOnController3
      - - JoinController3ToCCPCluster
      - - ActivateController2
      - - ActivateController3
      - - GetMPNode1Id
      - - GetMPNode2Id
      - - GetMPNode3Id
      - - VerifyClusterMembers_3MP_3CCP
      - - MapNSXManager1ToCluster
      - - VerifyClusterStatusFromAllNodes
      - - CreateLogicalSwitch01From_Node1
      - - ReadLogicalSwitch01FromNode1
      - - VerifyLogicalSwitch01InfoOnControllers
      - - CreateLogicalRouter01FromNode1
      - - ReadLogicalRouter01FromNode1
      - - PowerOffMP_Node1
        - PowerOffMP_Node2
      - - ShutdownNode3WithSleep
      - - PowerOnMP_Node3
      - - WaitForClusterStatusInitializingOnNode3WithSleep
      - - RevokeNode1AndNode2FromNode3
      - - RemoveMP_Node1FromNode3
      - - RemoveMP_Node2FromNode3
      - - Wait_For_Cluster_Status_Stable_On_Node3
      - - CreateLogicalSwitch02From_Node3
      - - CreateLogicalRouter02FromNode3
      - - ReadLogicalSwitch01FromNode3
      - - ReadLogicalRouter01FromNode3
      - - ReadLogicalSwitch02FromNode3
      - - ReadLogicalRouter02FromNode3
      - - VerifyLogicalSwitch01InfoOnControllersUsingNode3
      - - VerifyLogicalSwitch02InfoOnControllersUsingNode3
    ExitSequence:
      - - CleanupLogicalSwitchesOnNode3
      - - CleanupLogicalRouterOnNode3
      - - PowerOnMP_Node1
      - - SetProtonServiceIdFor_Node1_WithSleep
      - - StopProtonServiceOn_Node1
      - - VerifyStopProtonServiceStatusFor_Node1
      - - RemoveMP_Node1_WithIgnore
      - - PowerOnMP_Node2
      - - SetProtonServiceIdFor_Node2_With_Sleep
      - - StopProtonServiceOn_Node2
      - - VerifyStopProtonServiceStatusFor_Node2
      - - RemoveMP_Node2_WithIgnore
      - - CleanupMPNode1ForReuse
      - - CleanupMPNode2ForReuse
      - - AddMPNode1ToClusterNode3 # To have the configuration back to MP node 1
      - - DeleteNode3FromClusterNode1
      - - CleanupMPNode3ForReuse
      - - Cleanup3NodesCCPCluster

    UnregisterController3FromNode3:
      <<: *UNREGISTER_CCP_NODE_3
      TestNSX: 'nsxmanager.[1]'

    UnregisterController1FromNode3:
      Type: NSX
      TestNSX: 'nsxmanager.[3]'
      deleteclusternode: *NSXCONTROLLER_CLUSTERNODE1
      SkipPostProcess: "1"

    UnregisterController2FromNode3:
      Type: NSX
      TestNSX: 'nsxmanager.[3]'
      deleteclusternode: *NSXCONTROLLER_CLUSTERNODE2
      SkipPostProcess: "1"

    ShutdownNode3WithSleep:
      <<: *SHUTDOWN_NODE_3
      sleepbetweenworkloads: 180

    WaitForClusterStatusInitializingOnNode3WithSleep:
      Type: 'Cluster'
      TestCluster: 'nsxmanager.[3].cluster.[1]'
      sleepbetweenworkloads: 180
      wait_for_required_cluster_status:
        'required_status': 'INITIALIZING'
        'time_to_monitor': '300'

    RevokeNode1AndNode2FromNode3:
      Type : "Cluster"
      TestCluster : "nsxmanager.[3].clusternode.[3]"
      revoke_cluster_node:
        hosts:
          - 'host1'
          - 'host2'

    CreateLogicalRouter02FromNode3:
      <<: *CREATE_LOGICALROUTER_02_ON_NODE2
      TestNSX: "nsxmanager.[3]"

    RemoveMP_Node1_WithIgnore:
      <<: *REMOVE_MP_NODE_1_FROM_NODE3
      expectedResult: ignore

    RemoveMP_Node2_WithIgnore:
      <<: *REMOVE_MP_NODE_2_FROM_NODE3
      expectedResult: ignore

    CleanupLogicalSwitchesOnNode3:
      Type: NSX
      TestNSX: nsxmanager.[3]
      deletelogicalswitch: 'nsxmanager.[3].logicalswitch.[-1]'

    CleanupLogicalRouterOnNode3:
      Type: NSX
      TestNSX: nsxmanager.[3]
      deletelogicalrouter: "nsxmanager.[3].logicalrouter.[-1]"
